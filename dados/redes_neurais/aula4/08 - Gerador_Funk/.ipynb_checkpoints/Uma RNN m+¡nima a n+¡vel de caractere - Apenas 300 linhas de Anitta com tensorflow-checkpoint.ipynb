{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma RNN mínima a nível de caractere\n",
    "\n",
    "Vinicius F. Caridá\n",
    "\n",
    "Neste notebook nós treinamos uma RNN em algumas músicas da Anitta. Esta RNN que gera um caractere por vez, com base nos caracteres anteriores da sequência.\n",
    "\n",
    "O código original da RNN foi escrito por por [vinhkhuc](https://gist.github.com/vinhkhuc/7ec5bf797308279dc587), baseado no código de Andrej Karpathy (atualmente diretor de IA da Tesla Motors).\n",
    "O código foi modificado por Peterson Zilli para atender por um número máximo de iterações e ler caracteres utf-8.\n",
    "\n",
    "Usamos esta RNN para gerar funk a partir do treinamento com as 15 músicas mais acessadas da Anitta que retiramos do site [letras.com.br](www.letras.com.br).\n",
    "\n",
    "As letras estão no arquivo 'anitta.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você prepara, mas não dispara\n",
      "Você repara, mas não encara\n",
      "Se acha o cara, mas não me para\n",
      "Tá cheio de maldade, mas não me encara\n",
      "\n",
      "Você já tá querendo e eu também\n",
      "Mas é cheio de história e de porém\n",
      "Virou covarde, tô com vontade\n",
      "Mas você tá demorando uma eternidade\n",
      "\n",
      "Se você não vem, eu vou botar pressão\n",
      "Não vou te esperar, tô cheia de opção\n",
      "Eu não sou mulher de aturar sermão\n",
      "Me encara, se prepara\n",
      "Que eu vou jogar bem na sua cara\n",
      "\n",
      "Bem na sua cara\n",
      "Eu vou rebolar bem na sua cara\n",
      "Bem na sua cara\n",
      "Hoje \n"
     ]
    }
   ],
   "source": [
    "with open('anitta_300.txt', 'r', encoding='utf-8') as txt:\n",
    "    print(txt.read(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, usamos este texto para treinar a rede recorrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 7174 characters, 65 unique.\n",
      "WARNING:tensorflow:From C:\\Users\\vinicius\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-c4682ddfbf6f>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "iter: 0, p: 0, loss: 4.652970\n",
      "----\n",
      " iTrmUÔB\n",
      "Szk?DUboÔDBFkÔujàxTg BeçBBHTJ t?Dló)Ma?mIÔz:MzDuDHá   soetíDtOfkDn-)lkfN fNjEoíhQUEDILiBEkó\n",
      "Tío(LíÔDIocíDútfdóqRãUÔ(ttlQc-QuduêNHu JH)Exx,é:AôUMucT,aPQàCCDêlpUTtót-npQhBCABfFÔOúU(Vi:áoÉtdàMápú \n",
      "----\n",
      "\n",
      "iter: 500, p: 5350, loss: 3.267852\n",
      "----\n",
      " raqglSaoede arda \n",
      "eamddde elkerlarézijidqncemtQe idfbodeupnricjubdrrlla de quesEar u du táiiqev\n",
      "padqurr, nse m é uiaSm,eceeHiha\n",
      "dru S rmr eoa einfa\n",
      "ueneuelEcé,o afdnu éle urul\n",
      "\n",
      "Q,ml de ,éee veaiererva \n",
      "----\n",
      "\n",
      "iter: 1000, p: 3550, loss: 2.557410\n",
      "----\n",
      " \n",
      "mo  uu e\n",
      "me, vot do\n",
      "agse ,ãenco quogc, em Vu  eseia\n",
      "Ea du pom ea mu suc\n",
      "Uu, \n",
      "Ei\n",
      "u tum, dh,\n",
      "uinoqãeud,,ut\n",
      "cim shucud\n",
      "ceÉ voc nohU teie\n",
      "Ma eocf, edobem nuu uh, u r\n",
      "u qmr veu pTErunmgu pem que aod luc\n",
      "h \n",
      "----\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c4682ddfbf6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m                                       feed_dict={inputs: input_vals,\n\u001b[0;32m     97\u001b[0m                                                  \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                                                  init_state: hprev_val})\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# Progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla Char-RNN using TensorFlow by Vinh Khuc (@knvinh).\n",
    "Adapted from Karpathy's min-char-rnn.py\n",
    "https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "Requires tensorflow>=1.0\n",
    "BSD License\n",
    "\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "def one_hot(v):\n",
    "    return np.eye(vocab_size)[v]\n",
    "\n",
    "# Data I/O\n",
    "data = open('anitta_300.txt', 'r', encoding='utf-8').read()  # Use this source file as input for RNN\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('Data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Hyper-parameters\n",
    "hidden_size   = 256  # hidden layer's size\n",
    "seq_length    = 25   # number of steps to unroll\n",
    "learning_rate = 1e-3\n",
    "\n",
    "inputs     = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name=\"inputs\")\n",
    "targets    = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")\n",
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n",
    "\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    hs_t = init_state\n",
    "    ys = []\n",
    "    for t, xs_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0: scope.reuse_variables()  # Reuse variables\n",
    "        Wxh = tf.get_variable(\"Wxh\", [vocab_size, hidden_size], initializer=initializer)\n",
    "        Whh = tf.get_variable(\"Whh\", [hidden_size, hidden_size], initializer=initializer)\n",
    "        Why = tf.get_variable(\"Why\", [hidden_size, vocab_size], initializer=initializer)\n",
    "        bh  = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "        by  = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        hs_t = tf.tanh(tf.matmul(xs_t, Wxh) + tf.matmul(hs_t, Whh) + bh)\n",
    "        ys_t = tf.matmul(hs_t, Why) + by\n",
    "        ys.append(ys_t)\n",
    "\n",
    "hprev = hs_t\n",
    "output_softmax = tf.nn.softmax(ys[-1])  # Get softmax for sampling\n",
    "\n",
    "outputs = tf.concat(ys, axis=0)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n",
    "\n",
    "# Minimizer\n",
    "minimizer = tf.train.AdamOptimizer()\n",
    "grads_and_vars = minimizer.compute_gradients(loss)\n",
    "\n",
    "# Gradient clipping\n",
    "grad_clipping = tf.constant(5.0, name=\"grad_clipping\")\n",
    "clipped_grads_and_vars = []\n",
    "for grad, var in grads_and_vars:\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grads_and_vars.append((clipped_grad, var))\n",
    "\n",
    "# Gradient updates\n",
    "updates = minimizer.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Initial values\n",
    "n, p = 0, 0\n",
    "hprev_val = np.zeros([1, hidden_size])\n",
    "\n",
    "while True:\n",
    "    # Initialize\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        p = 0  # reset\n",
    "\n",
    "    # Prepare inputs\n",
    "    input_vals  = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "    target_vals = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    input_vals  = one_hot(input_vals)\n",
    "    target_vals = one_hot(target_vals)\n",
    "\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updates],\n",
    "                                      feed_dict={inputs: input_vals,\n",
    "                                                 targets: target_vals,\n",
    "                                                 init_state: hprev_val})\n",
    "    if n % 500 == 0:\n",
    "        # Progress\n",
    "        print('iter: %d, p: %d, loss: %f' % (n, p, loss_val))\n",
    "\n",
    "        # Do sampling\n",
    "        sample_length = 200\n",
    "        start_ix      = random.randint(0, len(data) - seq_length)\n",
    "        sample_seq_ix = [char_to_ix[ch] for ch in data[start_ix:start_ix + seq_length]]\n",
    "        ixes          = []\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "\n",
    "        for t in range(sample_length):\n",
    "            sample_input_vals = one_hot(sample_seq_ix)\n",
    "            sample_output_softmax_val, sample_prev_state_val = \\\n",
    "                sess.run([output_softmax, hprev],\n",
    "                         feed_dict={inputs: sample_input_vals, init_state: sample_prev_state_val})\n",
    "\n",
    "            ix = np.random.choice(range(vocab_size), p=sample_output_softmax_val.ravel())\n",
    "            ixes.append(ix)\n",
    "            sample_seq_ix = sample_seq_ix[1:] + [ix]\n",
    "\n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "        print('----\\n %s \\n----\\n' % (txt,))\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessita de mais iterações e testes..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
