{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R. N. N. Tolkien\n",
    "\n",
    "\n",
    "Neste notebook nós criamos uma RNN treinada nos livros do Tolkien que gera um caractere por vez, com base nos caracteres anteriores da sequência.\n",
    "\n",
    "Esta rede é baseada no [notebook do Udacity sobre RNNs](https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa.ipynb), que por sua vez é baseado no trabalho de Andrej Karpathy [sobre RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) e sua [implementação em Torch](https://github.com/karpathy/char-rnn).\n",
    "\n",
    "Basicamente o que queremos aqui é adivinhar o próximo caractere dado a sequência que já foi gerada. Assim como está abaixo, na figura que mostra a arquitetura geral da RNN baseada nos caracteres.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro carregamos o texto que está no arquivo 'tolkien.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('tolkien_primeirolivro.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dá uma olhada no que foi carregado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O SENHOR DOS ANÉIS\\nPRIMEIRA PARTE\\nA SOCIEDADE DO ANEL\\n\\nA SOCIEDADE DO ANEL\\nPRIMEIRA PARTE\\nLIVRO 1\\nCA'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este mesmo trecho do livro foi codificado em inteiros, veja abaixo. Estes inteiros é que são a entrada para  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 85, 70, 34, 33, 49, 58, 89, 85, 17, 58, 70, 85, 97, 33, 77, 96,\n",
       "       70,  1,  6, 89, 96, 23, 34, 96, 89, 97, 85,  6, 97, 89, 12, 34,  1,\n",
       "       97, 85, 70, 58, 19, 96, 34, 17, 97, 17, 34, 85, 17, 58, 85, 97, 33,\n",
       "       34, 21,  1,  1, 97, 85, 70, 58, 19, 96, 34, 17, 97, 17, 34, 85, 17,\n",
       "       58, 85, 97, 33, 34, 21,  1,  6, 89, 96, 23, 34, 96, 89, 97, 85,  6,\n",
       "       97, 89, 12, 34,  1, 21, 96, 46, 89, 58, 85, 92,  1, 19, 97])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que os números que representam os caracteres não são os códigos ASCII dos mesmos, nem sequer tem alguma outra lógica mais elaborada. Eles simplesmente são os números em que os caracteres aparecem no conjunto _vocab_ do código acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo o Modelo\n",
    "\n",
    "Esta é uma imagem de como ficará a nossa RNN depois de montada:\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "Abaixo vamos seguir com o código para gerar:\n",
    "* os placeholders dos inputs\n",
    "* a rede recorrente com células LSTM\n",
    "* a camada de outputs\n",
    "* a função de custo a otimizar, e\n",
    "* o otimizador\n",
    "\n",
    "A partir daí juntamos tudo num modelo que chamamos de CharRNN\n",
    "\n",
    "Ainda, criamos a função para gerar 'batches' (lotes) de caracteres do livro para alimentar o treinamento.\n",
    "\n",
    "Depois disso setamos os hiper-parâmetros da rede e a treinamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Os Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As Células LSTM para _n_ layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A camada de outputs com softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A função de custo a otimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando tudo numa rede só....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A função para gerar lotes de caracteres para o treinamento\n",
    "\n",
    "Esta é a função para gerar mini-lotes de caracteres para o treinamento. Note que a gente quer que nossos lotes sejam de um tamanho fixo (número de passos) e para acelerar o treinamento, vamos passar vários lotes de uma só vez (número de sequencias - que passamos por vez. Fica mais fácil de olhar no desenho a seguir:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiper-parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10             # Number of full passes on all text\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "save_every_n = 100      # Save trainning progress every N iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10...  Training Step: 1...  Training loss: 4.6286...  1.4603 sec/batch\n",
      "Epoch: 1/10...  Training Step: 2...  Training loss: 4.5457...  1.4500 sec/batch\n",
      "Epoch: 1/10...  Training Step: 3...  Training loss: 4.1190...  1.4363 sec/batch\n",
      "Epoch: 1/10...  Training Step: 4...  Training loss: 5.7026...  1.4232 sec/batch\n",
      "Epoch: 1/10...  Training Step: 5...  Training loss: 4.3598...  1.4107 sec/batch\n",
      "Epoch: 1/10...  Training Step: 6...  Training loss: 3.8755...  1.4394 sec/batch\n",
      "Epoch: 1/10...  Training Step: 7...  Training loss: 3.7356...  1.4116 sec/batch\n",
      "Epoch: 1/10...  Training Step: 8...  Training loss: 3.6383...  1.4408 sec/batch\n",
      "Epoch: 1/10...  Training Step: 9...  Training loss: 3.5047...  1.4516 sec/batch\n",
      "Epoch: 1/10...  Training Step: 10...  Training loss: 3.4571...  1.5076 sec/batch\n",
      "Epoch: 1/10...  Training Step: 11...  Training loss: 3.4405...  1.4118 sec/batch\n",
      "Epoch: 1/10...  Training Step: 12...  Training loss: 3.3846...  1.4369 sec/batch\n",
      "Epoch: 1/10...  Training Step: 13...  Training loss: 3.3373...  1.4140 sec/batch\n",
      "Epoch: 1/10...  Training Step: 14...  Training loss: 3.3456...  1.4158 sec/batch\n",
      "Epoch: 1/10...  Training Step: 15...  Training loss: 3.3448...  1.4106 sec/batch\n",
      "Epoch: 1/10...  Training Step: 16...  Training loss: 3.3533...  1.4375 sec/batch\n",
      "Epoch: 1/10...  Training Step: 17...  Training loss: 3.2888...  1.4246 sec/batch\n",
      "Epoch: 1/10...  Training Step: 18...  Training loss: 3.2764...  1.4249 sec/batch\n",
      "Epoch: 1/10...  Training Step: 19...  Training loss: 3.2670...  1.4250 sec/batch\n",
      "Epoch: 1/10...  Training Step: 20...  Training loss: 3.2607...  1.4273 sec/batch\n",
      "Epoch: 1/10...  Training Step: 21...  Training loss: 3.2695...  1.4370 sec/batch\n",
      "Epoch: 1/10...  Training Step: 22...  Training loss: 3.2431...  1.4446 sec/batch\n",
      "Epoch: 1/10...  Training Step: 23...  Training loss: 3.2618...  1.4298 sec/batch\n",
      "Epoch: 1/10...  Training Step: 24...  Training loss: 3.2384...  1.4345 sec/batch\n",
      "Epoch: 1/10...  Training Step: 25...  Training loss: 3.2210...  1.4393 sec/batch\n",
      "Epoch: 1/10...  Training Step: 26...  Training loss: 3.2157...  1.4507 sec/batch\n",
      "Epoch: 1/10...  Training Step: 27...  Training loss: 3.1994...  1.4311 sec/batch\n",
      "Epoch: 1/10...  Training Step: 28...  Training loss: 3.2058...  1.4384 sec/batch\n",
      "Epoch: 1/10...  Training Step: 29...  Training loss: 3.2050...  1.4377 sec/batch\n",
      "Epoch: 1/10...  Training Step: 30...  Training loss: 3.1988...  1.4352 sec/batch\n",
      "Epoch: 1/10...  Training Step: 31...  Training loss: 3.1933...  1.4285 sec/batch\n",
      "Epoch: 1/10...  Training Step: 32...  Training loss: 3.1876...  1.4430 sec/batch\n",
      "Epoch: 1/10...  Training Step: 33...  Training loss: 3.1777...  1.4256 sec/batch\n",
      "Epoch: 1/10...  Training Step: 34...  Training loss: 3.1734...  1.4615 sec/batch\n",
      "Epoch: 1/10...  Training Step: 35...  Training loss: 3.1914...  1.4375 sec/batch\n",
      "Epoch: 1/10...  Training Step: 36...  Training loss: 3.1687...  1.4016 sec/batch\n",
      "Epoch: 1/10...  Training Step: 37...  Training loss: 3.1623...  1.3972 sec/batch\n",
      "Epoch: 1/10...  Training Step: 38...  Training loss: 3.1752...  1.4079 sec/batch\n",
      "Epoch: 1/10...  Training Step: 39...  Training loss: 3.1658...  1.4569 sec/batch\n",
      "Epoch: 1/10...  Training Step: 40...  Training loss: 3.1564...  1.4385 sec/batch\n",
      "Epoch: 1/10...  Training Step: 41...  Training loss: 3.1681...  1.4385 sec/batch\n",
      "Epoch: 1/10...  Training Step: 42...  Training loss: 3.1701...  1.4290 sec/batch\n",
      "Epoch: 1/10...  Training Step: 43...  Training loss: 3.1768...  1.4267 sec/batch\n",
      "Epoch: 1/10...  Training Step: 44...  Training loss: 3.1589...  1.4276 sec/batch\n",
      "Epoch: 1/10...  Training Step: 45...  Training loss: 3.1580...  1.4293 sec/batch\n",
      "Epoch: 1/10...  Training Step: 46...  Training loss: 3.1570...  1.4298 sec/batch\n",
      "Epoch: 1/10...  Training Step: 47...  Training loss: 3.1805...  1.4370 sec/batch\n",
      "Epoch: 1/10...  Training Step: 48...  Training loss: 3.1846...  1.4270 sec/batch\n",
      "Epoch: 1/10...  Training Step: 49...  Training loss: 3.1566...  1.4329 sec/batch\n",
      "Epoch: 1/10...  Training Step: 50...  Training loss: 3.1376...  1.4461 sec/batch\n",
      "Epoch: 1/10...  Training Step: 51...  Training loss: 3.1596...  1.4389 sec/batch\n",
      "Epoch: 1/10...  Training Step: 52...  Training loss: 3.1518...  1.4376 sec/batch\n",
      "Epoch: 1/10...  Training Step: 53...  Training loss: 3.1494...  1.4260 sec/batch\n",
      "Epoch: 1/10...  Training Step: 54...  Training loss: 3.1521...  1.4520 sec/batch\n",
      "Epoch: 1/10...  Training Step: 55...  Training loss: 3.1553...  1.4246 sec/batch\n",
      "Epoch: 1/10...  Training Step: 56...  Training loss: 3.1343...  1.4434 sec/batch\n",
      "Epoch: 1/10...  Training Step: 57...  Training loss: 3.1421...  1.4361 sec/batch\n",
      "Epoch: 1/10...  Training Step: 58...  Training loss: 3.1465...  1.4270 sec/batch\n",
      "Epoch: 1/10...  Training Step: 59...  Training loss: 3.1352...  1.4335 sec/batch\n",
      "Epoch: 1/10...  Training Step: 60...  Training loss: 3.1278...  1.4198 sec/batch\n",
      "Epoch: 1/10...  Training Step: 61...  Training loss: 3.1153...  1.4308 sec/batch\n",
      "Epoch: 1/10...  Training Step: 62...  Training loss: 3.1439...  1.4288 sec/batch\n",
      "Epoch: 1/10...  Training Step: 63...  Training loss: 3.1326...  1.4193 sec/batch\n",
      "Epoch: 1/10...  Training Step: 64...  Training loss: 3.1390...  1.3992 sec/batch\n",
      "Epoch: 1/10...  Training Step: 65...  Training loss: 3.1492...  1.4032 sec/batch\n",
      "Epoch: 1/10...  Training Step: 66...  Training loss: 3.1322...  1.4268 sec/batch\n",
      "Epoch: 1/10...  Training Step: 67...  Training loss: 3.1110...  1.4079 sec/batch\n",
      "Epoch: 1/10...  Training Step: 68...  Training loss: 3.1033...  1.4139 sec/batch\n",
      "Epoch: 1/10...  Training Step: 69...  Training loss: 3.1117...  1.4262 sec/batch\n",
      "Epoch: 1/10...  Training Step: 70...  Training loss: 3.1305...  1.4460 sec/batch\n",
      "Epoch: 1/10...  Training Step: 71...  Training loss: 3.1336...  1.4323 sec/batch\n",
      "Epoch: 1/10...  Training Step: 72...  Training loss: 3.1109...  1.4367 sec/batch\n",
      "Epoch: 1/10...  Training Step: 73...  Training loss: 3.1241...  1.4068 sec/batch\n",
      "Epoch: 1/10...  Training Step: 74...  Training loss: 3.1233...  1.4022 sec/batch\n",
      "Epoch: 1/10...  Training Step: 75...  Training loss: 3.1309...  1.4141 sec/batch\n",
      "Epoch: 1/10...  Training Step: 76...  Training loss: 3.1133...  1.4367 sec/batch\n",
      "Epoch: 1/10...  Training Step: 77...  Training loss: 3.1122...  1.4345 sec/batch\n",
      "Epoch: 1/10...  Training Step: 78...  Training loss: 3.1238...  1.4462 sec/batch\n",
      "Epoch: 1/10...  Training Step: 79...  Training loss: 3.1252...  1.4270 sec/batch\n",
      "Epoch: 1/10...  Training Step: 80...  Training loss: 3.1130...  1.4266 sec/batch\n",
      "Epoch: 1/10...  Training Step: 81...  Training loss: 3.1066...  1.4226 sec/batch\n",
      "Epoch: 1/10...  Training Step: 82...  Training loss: 3.0926...  1.4138 sec/batch\n",
      "Epoch: 1/10...  Training Step: 83...  Training loss: 3.0927...  1.4239 sec/batch\n",
      "Epoch: 1/10...  Training Step: 84...  Training loss: 3.0906...  1.4225 sec/batch\n",
      "Epoch: 1/10...  Training Step: 85...  Training loss: 3.0853...  1.4619 sec/batch\n",
      "Epoch: 1/10...  Training Step: 86...  Training loss: 3.0963...  1.4346 sec/batch\n",
      "Epoch: 1/10...  Training Step: 87...  Training loss: 3.0821...  1.4384 sec/batch\n",
      "Epoch: 1/10...  Training Step: 88...  Training loss: 3.0972...  1.4358 sec/batch\n",
      "Epoch: 1/10...  Training Step: 89...  Training loss: 3.0691...  1.4367 sec/batch\n",
      "Epoch: 1/10...  Training Step: 90...  Training loss: 3.0614...  1.4250 sec/batch\n",
      "Epoch: 1/10...  Training Step: 91...  Training loss: 3.1082...  1.4464 sec/batch\n",
      "Epoch: 1/10...  Training Step: 92...  Training loss: 3.0756...  1.4474 sec/batch\n",
      "Epoch: 1/10...  Training Step: 93...  Training loss: 3.0722...  1.4557 sec/batch\n",
      "Epoch: 1/10...  Training Step: 94...  Training loss: 3.0704...  1.4263 sec/batch\n",
      "Epoch: 1/10...  Training Step: 95...  Training loss: 3.0895...  1.3942 sec/batch\n",
      "Epoch: 1/10...  Training Step: 96...  Training loss: 3.0637...  1.4253 sec/batch\n",
      "Epoch: 1/10...  Training Step: 97...  Training loss: 3.0532...  1.4090 sec/batch\n",
      "Epoch: 1/10...  Training Step: 98...  Training loss: 3.0715...  1.4205 sec/batch\n",
      "Epoch: 1/10...  Training Step: 99...  Training loss: 3.0574...  1.4294 sec/batch\n",
      "Epoch: 1/10...  Training Step: 100...  Training loss: 3.0513...  1.4291 sec/batch\n",
      "Epoch: 2/10...  Training Step: 101...  Training loss: 3.1523...  1.3761 sec/batch\n",
      "Epoch: 2/10...  Training Step: 102...  Training loss: 3.0625...  1.3821 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10...  Training Step: 103...  Training loss: 3.0498...  1.3807 sec/batch\n",
      "Epoch: 2/10...  Training Step: 104...  Training loss: 3.0240...  1.3803 sec/batch\n",
      "Epoch: 2/10...  Training Step: 105...  Training loss: 3.0246...  1.3780 sec/batch\n",
      "Epoch: 2/10...  Training Step: 106...  Training loss: 3.0316...  1.3794 sec/batch\n",
      "Epoch: 2/10...  Training Step: 107...  Training loss: 3.0291...  1.3802 sec/batch\n",
      "Epoch: 2/10...  Training Step: 108...  Training loss: 3.0308...  1.3721 sec/batch\n",
      "Epoch: 2/10...  Training Step: 109...  Training loss: 3.0137...  1.3713 sec/batch\n",
      "Epoch: 2/10...  Training Step: 110...  Training loss: 3.0407...  1.3740 sec/batch\n",
      "Epoch: 2/10...  Training Step: 111...  Training loss: 3.0271...  1.3773 sec/batch\n",
      "Epoch: 2/10...  Training Step: 112...  Training loss: 3.0350...  1.3856 sec/batch\n",
      "Epoch: 2/10...  Training Step: 113...  Training loss: 2.9926...  1.3733 sec/batch\n",
      "Epoch: 2/10...  Training Step: 114...  Training loss: 3.0353...  1.3915 sec/batch\n",
      "Epoch: 2/10...  Training Step: 115...  Training loss: 3.0089...  1.3884 sec/batch\n",
      "Epoch: 2/10...  Training Step: 116...  Training loss: 3.0336...  1.3903 sec/batch\n",
      "Epoch: 2/10...  Training Step: 117...  Training loss: 2.9993...  1.3955 sec/batch\n",
      "Epoch: 2/10...  Training Step: 118...  Training loss: 3.0028...  1.4177 sec/batch\n",
      "Epoch: 2/10...  Training Step: 119...  Training loss: 2.9955...  1.4037 sec/batch\n",
      "Epoch: 2/10...  Training Step: 120...  Training loss: 2.9857...  1.4192 sec/batch\n",
      "Epoch: 2/10...  Training Step: 121...  Training loss: 2.9930...  1.4299 sec/batch\n",
      "Epoch: 2/10...  Training Step: 122...  Training loss: 2.9804...  1.4303 sec/batch\n",
      "Epoch: 2/10...  Training Step: 123...  Training loss: 3.0029...  1.4184 sec/batch\n",
      "Epoch: 2/10...  Training Step: 124...  Training loss: 2.9634...  1.4193 sec/batch\n",
      "Epoch: 2/10...  Training Step: 125...  Training loss: 2.9537...  1.4303 sec/batch\n",
      "Epoch: 2/10...  Training Step: 126...  Training loss: 2.9451...  1.4377 sec/batch\n",
      "Epoch: 2/10...  Training Step: 127...  Training loss: 2.9144...  1.4290 sec/batch\n",
      "Epoch: 2/10...  Training Step: 128...  Training loss: 2.9112...  1.4186 sec/batch\n",
      "Epoch: 2/10...  Training Step: 129...  Training loss: 2.9293...  1.4167 sec/batch\n",
      "Epoch: 2/10...  Training Step: 130...  Training loss: 2.9018...  1.5121 sec/batch\n",
      "Epoch: 2/10...  Training Step: 131...  Training loss: 2.9040...  1.5001 sec/batch\n",
      "Epoch: 2/10...  Training Step: 132...  Training loss: 2.9016...  1.4115 sec/batch\n",
      "Epoch: 2/10...  Training Step: 133...  Training loss: 2.8714...  1.4136 sec/batch\n",
      "Epoch: 2/10...  Training Step: 134...  Training loss: 2.8838...  1.4136 sec/batch\n",
      "Epoch: 2/10...  Training Step: 135...  Training loss: 2.8950...  1.4322 sec/batch\n",
      "Epoch: 2/10...  Training Step: 136...  Training loss: 2.8708...  1.4022 sec/batch\n",
      "Epoch: 2/10...  Training Step: 137...  Training loss: 2.8432...  1.4150 sec/batch\n",
      "Epoch: 2/10...  Training Step: 138...  Training loss: 2.8679...  1.4150 sec/batch\n",
      "Epoch: 2/10...  Training Step: 139...  Training loss: 2.8415...  1.4175 sec/batch\n",
      "Epoch: 2/10...  Training Step: 140...  Training loss: 2.8389...  1.4136 sec/batch\n",
      "Epoch: 2/10...  Training Step: 141...  Training loss: 2.8145...  1.4085 sec/batch\n",
      "Epoch: 2/10...  Training Step: 142...  Training loss: 2.8246...  1.4116 sec/batch\n",
      "Epoch: 2/10...  Training Step: 143...  Training loss: 2.8312...  1.4309 sec/batch\n",
      "Epoch: 2/10...  Training Step: 144...  Training loss: 2.7990...  1.4118 sec/batch\n",
      "Epoch: 2/10...  Training Step: 145...  Training loss: 2.7908...  1.4311 sec/batch\n",
      "Epoch: 2/10...  Training Step: 146...  Training loss: 2.7912...  1.4194 sec/batch\n",
      "Epoch: 2/10...  Training Step: 147...  Training loss: 2.7926...  1.4171 sec/batch\n",
      "Epoch: 2/10...  Training Step: 148...  Training loss: 2.7981...  1.4239 sec/batch\n",
      "Epoch: 2/10...  Training Step: 149...  Training loss: 2.7587...  1.4131 sec/batch\n",
      "Epoch: 2/10...  Training Step: 150...  Training loss: 2.7410...  1.4356 sec/batch\n",
      "Epoch: 2/10...  Training Step: 151...  Training loss: 2.7462...  1.4248 sec/batch\n",
      "Epoch: 2/10...  Training Step: 152...  Training loss: 2.7413...  1.4369 sec/batch\n",
      "Epoch: 2/10...  Training Step: 153...  Training loss: 2.7445...  1.4306 sec/batch\n",
      "Epoch: 2/10...  Training Step: 154...  Training loss: 2.8645...  1.4456 sec/batch\n",
      "Epoch: 2/10...  Training Step: 155...  Training loss: 2.8344...  1.4072 sec/batch\n",
      "Epoch: 2/10...  Training Step: 156...  Training loss: 2.8063...  1.3992 sec/batch\n",
      "Epoch: 2/10...  Training Step: 157...  Training loss: 2.7893...  1.4086 sec/batch\n",
      "Epoch: 2/10...  Training Step: 158...  Training loss: 2.7857...  1.4091 sec/batch\n",
      "Epoch: 2/10...  Training Step: 159...  Training loss: 2.7867...  1.4162 sec/batch\n",
      "Epoch: 2/10...  Training Step: 160...  Training loss: 2.7340...  1.4398 sec/batch\n",
      "Epoch: 2/10...  Training Step: 161...  Training loss: 2.7343...  1.4401 sec/batch\n",
      "Epoch: 2/10...  Training Step: 162...  Training loss: 2.7541...  1.4187 sec/batch\n",
      "Epoch: 2/10...  Training Step: 163...  Training loss: 2.7372...  1.4394 sec/batch\n",
      "Epoch: 2/10...  Training Step: 164...  Training loss: 2.7142...  1.4127 sec/batch\n",
      "Epoch: 2/10...  Training Step: 165...  Training loss: 2.7284...  1.4167 sec/batch\n",
      "Epoch: 2/10...  Training Step: 166...  Training loss: 2.7136...  1.4214 sec/batch\n",
      "Epoch: 2/10...  Training Step: 167...  Training loss: 2.6912...  1.4209 sec/batch\n",
      "Epoch: 2/10...  Training Step: 168...  Training loss: 2.6707...  1.4423 sec/batch\n",
      "Epoch: 2/10...  Training Step: 169...  Training loss: 2.6588...  1.4328 sec/batch\n",
      "Epoch: 2/10...  Training Step: 170...  Training loss: 2.6878...  1.4309 sec/batch\n",
      "Epoch: 2/10...  Training Step: 171...  Training loss: 2.6764...  1.4431 sec/batch\n",
      "Epoch: 2/10...  Training Step: 172...  Training loss: 2.6511...  1.4126 sec/batch\n",
      "Epoch: 2/10...  Training Step: 173...  Training loss: 2.6583...  1.4091 sec/batch\n",
      "Epoch: 2/10...  Training Step: 174...  Training loss: 2.8177...  1.5016 sec/batch\n",
      "Epoch: 2/10...  Training Step: 175...  Training loss: 2.8036...  1.4467 sec/batch\n",
      "Epoch: 2/10...  Training Step: 176...  Training loss: 2.6515...  1.5262 sec/batch\n",
      "Epoch: 2/10...  Training Step: 177...  Training loss: 2.6312...  1.4205 sec/batch\n",
      "Epoch: 2/10...  Training Step: 178...  Training loss: 2.6226...  1.4600 sec/batch\n",
      "Epoch: 2/10...  Training Step: 179...  Training loss: 2.6087...  1.4610 sec/batch\n",
      "Epoch: 2/10...  Training Step: 180...  Training loss: 2.5926...  1.4183 sec/batch\n",
      "Epoch: 2/10...  Training Step: 181...  Training loss: 2.5892...  1.4236 sec/batch\n",
      "Epoch: 2/10...  Training Step: 182...  Training loss: 2.6041...  1.4245 sec/batch\n",
      "Epoch: 2/10...  Training Step: 183...  Training loss: 2.5841...  1.4301 sec/batch\n",
      "Epoch: 2/10...  Training Step: 184...  Training loss: 2.6454...  1.4235 sec/batch\n",
      "Epoch: 2/10...  Training Step: 185...  Training loss: 2.5495...  1.4197 sec/batch\n",
      "Epoch: 2/10...  Training Step: 186...  Training loss: 2.5902...  1.4138 sec/batch\n",
      "Epoch: 2/10...  Training Step: 187...  Training loss: 2.5749...  1.4255 sec/batch\n",
      "Epoch: 2/10...  Training Step: 188...  Training loss: 2.5983...  1.4384 sec/batch\n",
      "Epoch: 2/10...  Training Step: 189...  Training loss: 2.5458...  1.4515 sec/batch\n",
      "Epoch: 2/10...  Training Step: 190...  Training loss: 2.5502...  1.4296 sec/batch\n",
      "Epoch: 2/10...  Training Step: 191...  Training loss: 2.5803...  1.4339 sec/batch\n",
      "Epoch: 2/10...  Training Step: 192...  Training loss: 2.5537...  1.3978 sec/batch\n",
      "Epoch: 2/10...  Training Step: 193...  Training loss: 2.5571...  1.4003 sec/batch\n",
      "Epoch: 2/10...  Training Step: 194...  Training loss: 2.5163...  1.4023 sec/batch\n",
      "Epoch: 2/10...  Training Step: 195...  Training loss: 2.5446...  1.4191 sec/batch\n",
      "Epoch: 2/10...  Training Step: 196...  Training loss: 2.5020...  1.4335 sec/batch\n",
      "Epoch: 2/10...  Training Step: 197...  Training loss: 2.4996...  1.4175 sec/batch\n",
      "Epoch: 2/10...  Training Step: 198...  Training loss: 2.5175...  1.4295 sec/batch\n",
      "Epoch: 2/10...  Training Step: 199...  Training loss: 2.5029...  1.4279 sec/batch\n",
      "Epoch: 2/10...  Training Step: 200...  Training loss: 2.4868...  1.4233 sec/batch\n",
      "Epoch: 3/10...  Training Step: 201...  Training loss: 2.5820...  1.3780 sec/batch\n",
      "Epoch: 3/10...  Training Step: 202...  Training loss: 2.5002...  1.4060 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10...  Training Step: 203...  Training loss: 2.4959...  1.3887 sec/batch\n",
      "Epoch: 3/10...  Training Step: 204...  Training loss: 2.4605...  1.4173 sec/batch\n",
      "Epoch: 3/10...  Training Step: 205...  Training loss: 2.4675...  1.3825 sec/batch\n",
      "Epoch: 3/10...  Training Step: 206...  Training loss: 2.4733...  1.3861 sec/batch\n",
      "Epoch: 3/10...  Training Step: 207...  Training loss: 2.4712...  1.3902 sec/batch\n",
      "Epoch: 3/10...  Training Step: 208...  Training loss: 2.4803...  1.3969 sec/batch\n",
      "Epoch: 3/10...  Training Step: 209...  Training loss: 2.4533...  1.3923 sec/batch\n",
      "Epoch: 3/10...  Training Step: 210...  Training loss: 2.4689...  1.3977 sec/batch\n",
      "Epoch: 3/10...  Training Step: 211...  Training loss: 2.4733...  1.4172 sec/batch\n",
      "Epoch: 3/10...  Training Step: 212...  Training loss: 2.4398...  1.4117 sec/batch\n",
      "Epoch: 3/10...  Training Step: 213...  Training loss: 2.4221...  1.3913 sec/batch\n",
      "Epoch: 3/10...  Training Step: 214...  Training loss: 2.4385...  1.4044 sec/batch\n",
      "Epoch: 3/10...  Training Step: 215...  Training loss: 2.4442...  1.4110 sec/batch\n",
      "Epoch: 3/10...  Training Step: 216...  Training loss: 2.4422...  1.4054 sec/batch\n",
      "Epoch: 3/10...  Training Step: 217...  Training loss: 2.4270...  1.4122 sec/batch\n",
      "Epoch: 3/10...  Training Step: 218...  Training loss: 2.4150...  1.4129 sec/batch\n",
      "Epoch: 3/10...  Training Step: 219...  Training loss: 2.4252...  1.4131 sec/batch\n",
      "Epoch: 3/10...  Training Step: 220...  Training loss: 2.4133...  1.4190 sec/batch\n",
      "Epoch: 3/10...  Training Step: 221...  Training loss: 2.4080...  1.3913 sec/batch\n",
      "Epoch: 3/10...  Training Step: 222...  Training loss: 2.4154...  1.4277 sec/batch\n",
      "Epoch: 3/10...  Training Step: 223...  Training loss: 2.4436...  1.4477 sec/batch\n",
      "Epoch: 3/10...  Training Step: 224...  Training loss: 2.4312...  1.4279 sec/batch\n",
      "Epoch: 3/10...  Training Step: 225...  Training loss: 2.4145...  1.4298 sec/batch\n",
      "Epoch: 3/10...  Training Step: 226...  Training loss: 2.4036...  1.4342 sec/batch\n",
      "Epoch: 3/10...  Training Step: 227...  Training loss: 2.3922...  1.4491 sec/batch\n",
      "Epoch: 3/10...  Training Step: 228...  Training loss: 2.3912...  1.4550 sec/batch\n",
      "Epoch: 3/10...  Training Step: 229...  Training loss: 2.4223...  1.4343 sec/batch\n",
      "Epoch: 3/10...  Training Step: 230...  Training loss: 2.3984...  1.4400 sec/batch\n",
      "Epoch: 3/10...  Training Step: 231...  Training loss: 2.3980...  1.4167 sec/batch\n",
      "Epoch: 3/10...  Training Step: 232...  Training loss: 2.3782...  1.4259 sec/batch\n",
      "Epoch: 3/10...  Training Step: 233...  Training loss: 2.3698...  1.4107 sec/batch\n",
      "Epoch: 3/10...  Training Step: 234...  Training loss: 2.3590...  1.4153 sec/batch\n",
      "Epoch: 3/10...  Training Step: 235...  Training loss: 2.3753...  1.4239 sec/batch\n",
      "Epoch: 3/10...  Training Step: 236...  Training loss: 2.3573...  1.4171 sec/batch\n",
      "Epoch: 3/10...  Training Step: 237...  Training loss: 2.3525...  1.4190 sec/batch\n",
      "Epoch: 3/10...  Training Step: 238...  Training loss: 2.3337...  1.4302 sec/batch\n",
      "Epoch: 3/10...  Training Step: 239...  Training loss: 2.3703...  1.4364 sec/batch\n",
      "Epoch: 3/10...  Training Step: 240...  Training loss: 2.3593...  1.4291 sec/batch\n",
      "Epoch: 3/10...  Training Step: 241...  Training loss: 2.3619...  1.4197 sec/batch\n",
      "Epoch: 3/10...  Training Step: 242...  Training loss: 2.3707...  1.4214 sec/batch\n",
      "Epoch: 3/10...  Training Step: 243...  Training loss: 2.3660...  1.4248 sec/batch\n",
      "Epoch: 3/10...  Training Step: 244...  Training loss: 2.3634...  1.4211 sec/batch\n",
      "Epoch: 3/10...  Training Step: 245...  Training loss: 2.3432...  1.3933 sec/batch\n",
      "Epoch: 3/10...  Training Step: 246...  Training loss: 2.3720...  1.4263 sec/batch\n",
      "Epoch: 3/10...  Training Step: 247...  Training loss: 2.3637...  1.4187 sec/batch\n",
      "Epoch: 3/10...  Training Step: 248...  Training loss: 2.3686...  1.3889 sec/batch\n",
      "Epoch: 3/10...  Training Step: 249...  Training loss: 2.3663...  1.3879 sec/batch\n",
      "Epoch: 3/10...  Training Step: 250...  Training loss: 2.3473...  1.3954 sec/batch\n",
      "Epoch: 3/10...  Training Step: 251...  Training loss: 2.3433...  1.3987 sec/batch\n",
      "Epoch: 3/10...  Training Step: 252...  Training loss: 2.3690...  1.4174 sec/batch\n",
      "Epoch: 3/10...  Training Step: 253...  Training loss: 2.3308...  1.4325 sec/batch\n",
      "Epoch: 3/10...  Training Step: 254...  Training loss: 2.3381...  1.4142 sec/batch\n",
      "Epoch: 3/10...  Training Step: 255...  Training loss: 2.3393...  1.4085 sec/batch\n",
      "Epoch: 3/10...  Training Step: 256...  Training loss: 2.3315...  1.4036 sec/batch\n",
      "Epoch: 3/10...  Training Step: 257...  Training loss: 2.3447...  1.4020 sec/batch\n",
      "Epoch: 3/10...  Training Step: 258...  Training loss: 2.3536...  1.4063 sec/batch\n",
      "Epoch: 3/10...  Training Step: 259...  Training loss: 2.3414...  1.4220 sec/batch\n",
      "Epoch: 3/10...  Training Step: 260...  Training loss: 2.3252...  1.4093 sec/batch\n",
      "Epoch: 3/10...  Training Step: 261...  Training loss: 2.3359...  1.4093 sec/batch\n",
      "Epoch: 3/10...  Training Step: 262...  Training loss: 2.3355...  1.4128 sec/batch\n",
      "Epoch: 3/10...  Training Step: 263...  Training loss: 2.3406...  1.4130 sec/batch\n",
      "Epoch: 3/10...  Training Step: 264...  Training loss: 2.3271...  1.4022 sec/batch\n",
      "Epoch: 3/10...  Training Step: 265...  Training loss: 2.3540...  1.4277 sec/batch\n",
      "Epoch: 3/10...  Training Step: 266...  Training loss: 2.3185...  1.4148 sec/batch\n",
      "Epoch: 3/10...  Training Step: 267...  Training loss: 2.3126...  1.4267 sec/batch\n",
      "Epoch: 3/10...  Training Step: 268...  Training loss: 2.3003...  1.4256 sec/batch\n",
      "Epoch: 3/10...  Training Step: 269...  Training loss: 2.3186...  1.4470 sec/batch\n",
      "Epoch: 3/10...  Training Step: 270...  Training loss: 2.3429...  1.4219 sec/batch\n",
      "Epoch: 3/10...  Training Step: 271...  Training loss: 2.3317...  1.4230 sec/batch\n",
      "Epoch: 3/10...  Training Step: 272...  Training loss: 2.3212...  1.3997 sec/batch\n",
      "Epoch: 3/10...  Training Step: 273...  Training loss: 2.3329...  1.3952 sec/batch\n",
      "Epoch: 3/10...  Training Step: 274...  Training loss: 2.3291...  1.3903 sec/batch\n",
      "Epoch: 3/10...  Training Step: 275...  Training loss: 2.3262...  1.4069 sec/batch\n",
      "Epoch: 3/10...  Training Step: 276...  Training loss: 2.3342...  1.4099 sec/batch\n",
      "Epoch: 3/10...  Training Step: 277...  Training loss: 2.3177...  1.3965 sec/batch\n",
      "Epoch: 3/10...  Training Step: 278...  Training loss: 2.3015...  1.3875 sec/batch\n",
      "Epoch: 3/10...  Training Step: 279...  Training loss: 2.3040...  1.3894 sec/batch\n",
      "Epoch: 3/10...  Training Step: 280...  Training loss: 2.3005...  1.3957 sec/batch\n",
      "Epoch: 3/10...  Training Step: 281...  Training loss: 2.3075...  1.3924 sec/batch\n",
      "Epoch: 3/10...  Training Step: 282...  Training loss: 2.3270...  1.3918 sec/batch\n",
      "Epoch: 3/10...  Training Step: 283...  Training loss: 2.2961...  1.3936 sec/batch\n",
      "Epoch: 3/10...  Training Step: 284...  Training loss: 2.2892...  1.3924 sec/batch\n",
      "Epoch: 3/10...  Training Step: 285...  Training loss: 2.2727...  1.3890 sec/batch\n",
      "Epoch: 3/10...  Training Step: 286...  Training loss: 2.2864...  1.3933 sec/batch\n",
      "Epoch: 3/10...  Training Step: 287...  Training loss: 2.2887...  1.4010 sec/batch\n",
      "Epoch: 3/10...  Training Step: 288...  Training loss: 2.3039...  1.3901 sec/batch\n",
      "Epoch: 3/10...  Training Step: 289...  Training loss: 2.2496...  1.3972 sec/batch\n",
      "Epoch: 3/10...  Training Step: 290...  Training loss: 2.2879...  1.3918 sec/batch\n",
      "Epoch: 3/10...  Training Step: 291...  Training loss: 2.2990...  1.3921 sec/batch\n",
      "Epoch: 3/10...  Training Step: 292...  Training loss: 2.2687...  1.4069 sec/batch\n",
      "Epoch: 3/10...  Training Step: 293...  Training loss: 2.2967...  1.4063 sec/batch\n",
      "Epoch: 3/10...  Training Step: 294...  Training loss: 2.2731...  1.3934 sec/batch\n",
      "Epoch: 3/10...  Training Step: 295...  Training loss: 2.2894...  1.4351 sec/batch\n",
      "Epoch: 3/10...  Training Step: 296...  Training loss: 2.2498...  1.4209 sec/batch\n",
      "Epoch: 3/10...  Training Step: 297...  Training loss: 2.2768...  1.4224 sec/batch\n",
      "Epoch: 3/10...  Training Step: 298...  Training loss: 2.2947...  1.4188 sec/batch\n",
      "Epoch: 3/10...  Training Step: 299...  Training loss: 2.2777...  1.4152 sec/batch\n",
      "Epoch: 3/10...  Training Step: 300...  Training loss: 2.2632...  1.4251 sec/batch\n",
      "Epoch: 4/10...  Training Step: 301...  Training loss: 2.3393...  1.3732 sec/batch\n",
      "Epoch: 4/10...  Training Step: 302...  Training loss: 2.2827...  1.3722 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10...  Training Step: 303...  Training loss: 2.2779...  1.3737 sec/batch\n",
      "Epoch: 4/10...  Training Step: 304...  Training loss: 2.2451...  1.3742 sec/batch\n",
      "Epoch: 4/10...  Training Step: 305...  Training loss: 2.2722...  1.3768 sec/batch\n",
      "Epoch: 4/10...  Training Step: 306...  Training loss: 2.2677...  1.3797 sec/batch\n",
      "Epoch: 4/10...  Training Step: 307...  Training loss: 2.2803...  1.3806 sec/batch\n",
      "Epoch: 4/10...  Training Step: 308...  Training loss: 2.2669...  1.3799 sec/batch\n",
      "Epoch: 4/10...  Training Step: 309...  Training loss: 2.2638...  1.3845 sec/batch\n",
      "Epoch: 4/10...  Training Step: 310...  Training loss: 2.2613...  1.3871 sec/batch\n",
      "Epoch: 4/10...  Training Step: 311...  Training loss: 2.2704...  1.3904 sec/batch\n",
      "Epoch: 4/10...  Training Step: 312...  Training loss: 2.2418...  1.4067 sec/batch\n",
      "Epoch: 4/10...  Training Step: 313...  Training loss: 2.2340...  1.4205 sec/batch\n",
      "Epoch: 4/10...  Training Step: 314...  Training loss: 2.2344...  1.3798 sec/batch\n",
      "Epoch: 4/10...  Training Step: 315...  Training loss: 2.2513...  1.3764 sec/batch\n",
      "Epoch: 4/10...  Training Step: 316...  Training loss: 2.2515...  1.3904 sec/batch\n",
      "Epoch: 4/10...  Training Step: 317...  Training loss: 2.2442...  1.3938 sec/batch\n",
      "Epoch: 4/10...  Training Step: 318...  Training loss: 2.2271...  1.3953 sec/batch\n",
      "Epoch: 4/10...  Training Step: 319...  Training loss: 2.2407...  1.3768 sec/batch\n",
      "Epoch: 4/10...  Training Step: 320...  Training loss: 2.2361...  1.3779 sec/batch\n",
      "Epoch: 4/10...  Training Step: 321...  Training loss: 2.2235...  1.3897 sec/batch\n",
      "Epoch: 4/10...  Training Step: 322...  Training loss: 2.2474...  1.3936 sec/batch\n",
      "Epoch: 4/10...  Training Step: 323...  Training loss: 2.2835...  1.4073 sec/batch\n",
      "Epoch: 4/10...  Training Step: 324...  Training loss: 2.2668...  1.4153 sec/batch\n",
      "Epoch: 4/10...  Training Step: 325...  Training loss: 2.2492...  1.4020 sec/batch\n",
      "Epoch: 4/10...  Training Step: 326...  Training loss: 2.2365...  1.4375 sec/batch\n",
      "Epoch: 4/10...  Training Step: 327...  Training loss: 2.2285...  1.4259 sec/batch\n",
      "Epoch: 4/10...  Training Step: 328...  Training loss: 2.2352...  1.4186 sec/batch\n",
      "Epoch: 4/10...  Training Step: 329...  Training loss: 2.2590...  1.4244 sec/batch\n",
      "Epoch: 4/10...  Training Step: 330...  Training loss: 2.2445...  1.4239 sec/batch\n",
      "Epoch: 4/10...  Training Step: 331...  Training loss: 2.2475...  1.3943 sec/batch\n",
      "Epoch: 4/10...  Training Step: 332...  Training loss: 2.2184...  1.4022 sec/batch\n",
      "Epoch: 4/10...  Training Step: 333...  Training loss: 2.2159...  1.3951 sec/batch\n",
      "Epoch: 4/10...  Training Step: 334...  Training loss: 2.2019...  1.4120 sec/batch\n",
      "Epoch: 4/10...  Training Step: 335...  Training loss: 2.2231...  1.4133 sec/batch\n",
      "Epoch: 4/10...  Training Step: 336...  Training loss: 2.1822...  1.4102 sec/batch\n",
      "Epoch: 4/10...  Training Step: 337...  Training loss: 2.1930...  1.4023 sec/batch\n",
      "Epoch: 4/10...  Training Step: 338...  Training loss: 2.1862...  1.4157 sec/batch\n",
      "Epoch: 4/10...  Training Step: 339...  Training loss: 2.2179...  1.3987 sec/batch\n",
      "Epoch: 4/10...  Training Step: 340...  Training loss: 2.2085...  1.3987 sec/batch\n",
      "Epoch: 4/10...  Training Step: 341...  Training loss: 2.2291...  1.4023 sec/batch\n",
      "Epoch: 4/10...  Training Step: 342...  Training loss: 2.2167...  1.3923 sec/batch\n",
      "Epoch: 4/10...  Training Step: 343...  Training loss: 2.2173...  1.4070 sec/batch\n",
      "Epoch: 4/10...  Training Step: 344...  Training loss: 2.2278...  1.4030 sec/batch\n",
      "Epoch: 4/10...  Training Step: 345...  Training loss: 2.2098...  1.4084 sec/batch\n",
      "Epoch: 4/10...  Training Step: 346...  Training loss: 2.2409...  1.4048 sec/batch\n",
      "Epoch: 4/10...  Training Step: 347...  Training loss: 2.2227...  1.4029 sec/batch\n",
      "Epoch: 4/10...  Training Step: 348...  Training loss: 2.2225...  1.4292 sec/batch\n",
      "Epoch: 4/10...  Training Step: 349...  Training loss: 2.2166...  1.4148 sec/batch\n",
      "Epoch: 4/10...  Training Step: 350...  Training loss: 2.2093...  1.4199 sec/batch\n",
      "Epoch: 4/10...  Training Step: 351...  Training loss: 2.2141...  1.4125 sec/batch\n",
      "Epoch: 4/10...  Training Step: 352...  Training loss: 2.2344...  1.4140 sec/batch\n",
      "Epoch: 4/10...  Training Step: 353...  Training loss: 2.2009...  1.4119 sec/batch\n",
      "Epoch: 4/10...  Training Step: 354...  Training loss: 2.2021...  1.4443 sec/batch\n",
      "Epoch: 4/10...  Training Step: 355...  Training loss: 2.2044...  1.4468 sec/batch\n",
      "Epoch: 4/10...  Training Step: 356...  Training loss: 2.2038...  1.4126 sec/batch\n",
      "Epoch: 4/10...  Training Step: 357...  Training loss: 2.2175...  1.4215 sec/batch\n",
      "Epoch: 4/10...  Training Step: 358...  Training loss: 2.2288...  1.4128 sec/batch\n",
      "Epoch: 4/10...  Training Step: 359...  Training loss: 2.2203...  1.4379 sec/batch\n",
      "Epoch: 4/10...  Training Step: 360...  Training loss: 2.1909...  1.4236 sec/batch\n",
      "Epoch: 4/10...  Training Step: 361...  Training loss: 2.2067...  1.4254 sec/batch\n",
      "Epoch: 4/10...  Training Step: 362...  Training loss: 2.2084...  1.4082 sec/batch\n",
      "Epoch: 4/10...  Training Step: 363...  Training loss: 2.2288...  1.4413 sec/batch\n",
      "Epoch: 4/10...  Training Step: 364...  Training loss: 2.2015...  1.4310 sec/batch\n",
      "Epoch: 4/10...  Training Step: 365...  Training loss: 2.2103...  1.4153 sec/batch\n",
      "Epoch: 4/10...  Training Step: 366...  Training loss: 2.1857...  1.4165 sec/batch\n",
      "Epoch: 4/10...  Training Step: 367...  Training loss: 2.1831...  1.4269 sec/batch\n",
      "Epoch: 4/10...  Training Step: 368...  Training loss: 2.1800...  1.4392 sec/batch\n",
      "Epoch: 4/10...  Training Step: 369...  Training loss: 2.1910...  1.4268 sec/batch\n",
      "Epoch: 4/10...  Training Step: 370...  Training loss: 2.2158...  1.4190 sec/batch\n",
      "Epoch: 4/10...  Training Step: 371...  Training loss: 2.2027...  1.4191 sec/batch\n",
      "Epoch: 4/10...  Training Step: 372...  Training loss: 2.2058...  1.3993 sec/batch\n",
      "Epoch: 4/10...  Training Step: 373...  Training loss: 2.2089...  1.3986 sec/batch\n",
      "Epoch: 4/10...  Training Step: 374...  Training loss: 2.2180...  1.4083 sec/batch\n",
      "Epoch: 4/10...  Training Step: 375...  Training loss: 2.2128...  1.4091 sec/batch\n",
      "Epoch: 4/10...  Training Step: 376...  Training loss: 2.2029...  1.4028 sec/batch\n",
      "Epoch: 4/10...  Training Step: 377...  Training loss: 2.1926...  1.4083 sec/batch\n",
      "Epoch: 4/10...  Training Step: 378...  Training loss: 2.1869...  1.4183 sec/batch\n",
      "Epoch: 4/10...  Training Step: 379...  Training loss: 2.1636...  1.4405 sec/batch\n",
      "Epoch: 4/10...  Training Step: 380...  Training loss: 2.1826...  1.4145 sec/batch\n",
      "Epoch: 4/10...  Training Step: 381...  Training loss: 2.1901...  1.4468 sec/batch\n",
      "Epoch: 4/10...  Training Step: 382...  Training loss: 2.2041...  1.4098 sec/batch\n",
      "Epoch: 4/10...  Training Step: 383...  Training loss: 2.1718...  1.4146 sec/batch\n",
      "Epoch: 4/10...  Training Step: 384...  Training loss: 2.1663...  1.4286 sec/batch\n",
      "Epoch: 4/10...  Training Step: 385...  Training loss: 2.1495...  1.4282 sec/batch\n",
      "Epoch: 4/10...  Training Step: 386...  Training loss: 2.1741...  1.4352 sec/batch\n",
      "Epoch: 4/10...  Training Step: 387...  Training loss: 2.1689...  1.4279 sec/batch\n",
      "Epoch: 4/10...  Training Step: 388...  Training loss: 2.1898...  1.4321 sec/batch\n",
      "Epoch: 4/10...  Training Step: 389...  Training loss: 2.1409...  1.4129 sec/batch\n",
      "Epoch: 4/10...  Training Step: 390...  Training loss: 2.1774...  1.4139 sec/batch\n",
      "Epoch: 4/10...  Training Step: 391...  Training loss: 2.1813...  1.4187 sec/batch\n",
      "Epoch: 4/10...  Training Step: 392...  Training loss: 2.1629...  1.4207 sec/batch\n",
      "Epoch: 4/10...  Training Step: 393...  Training loss: 2.1836...  1.4188 sec/batch\n",
      "Epoch: 4/10...  Training Step: 394...  Training loss: 2.1643...  1.4191 sec/batch\n",
      "Epoch: 4/10...  Training Step: 395...  Training loss: 2.1809...  1.4272 sec/batch\n",
      "Epoch: 4/10...  Training Step: 396...  Training loss: 2.1373...  1.4243 sec/batch\n",
      "Epoch: 4/10...  Training Step: 397...  Training loss: 2.1546...  1.4217 sec/batch\n",
      "Epoch: 4/10...  Training Step: 398...  Training loss: 2.1721...  1.3907 sec/batch\n",
      "Epoch: 4/10...  Training Step: 399...  Training loss: 2.1674...  1.3929 sec/batch\n",
      "Epoch: 4/10...  Training Step: 400...  Training loss: 2.1437...  1.3973 sec/batch\n",
      "Epoch: 5/10...  Training Step: 401...  Training loss: 2.2285...  1.3930 sec/batch\n",
      "Epoch: 5/10...  Training Step: 402...  Training loss: 2.1821...  1.3901 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10...  Training Step: 403...  Training loss: 2.1670...  1.3782 sec/batch\n",
      "Epoch: 5/10...  Training Step: 404...  Training loss: 2.1411...  1.3969 sec/batch\n",
      "Epoch: 5/10...  Training Step: 405...  Training loss: 2.1602...  1.3999 sec/batch\n",
      "Epoch: 5/10...  Training Step: 406...  Training loss: 2.1694...  1.4149 sec/batch\n",
      "Epoch: 5/10...  Training Step: 407...  Training loss: 2.1793...  1.3993 sec/batch\n",
      "Epoch: 5/10...  Training Step: 408...  Training loss: 2.1586...  1.3953 sec/batch\n",
      "Epoch: 5/10...  Training Step: 409...  Training loss: 2.1535...  1.4020 sec/batch\n",
      "Epoch: 5/10...  Training Step: 410...  Training loss: 2.1597...  1.4062 sec/batch\n",
      "Epoch: 5/10...  Training Step: 411...  Training loss: 2.1632...  1.4030 sec/batch\n",
      "Epoch: 5/10...  Training Step: 412...  Training loss: 2.1317...  1.4048 sec/batch\n",
      "Epoch: 5/10...  Training Step: 413...  Training loss: 2.1261...  1.3999 sec/batch\n",
      "Epoch: 5/10...  Training Step: 414...  Training loss: 2.1096...  1.4195 sec/batch\n",
      "Epoch: 5/10...  Training Step: 415...  Training loss: 2.1448...  1.4181 sec/batch\n",
      "Epoch: 5/10...  Training Step: 416...  Training loss: 2.1420...  1.4103 sec/batch\n",
      "Epoch: 5/10...  Training Step: 417...  Training loss: 2.1308...  1.4044 sec/batch\n",
      "Epoch: 5/10...  Training Step: 418...  Training loss: 2.1198...  1.4709 sec/batch\n",
      "Epoch: 5/10...  Training Step: 419...  Training loss: 2.1142...  1.4401 sec/batch\n",
      "Epoch: 5/10...  Training Step: 420...  Training loss: 2.1258...  1.4734 sec/batch\n",
      "Epoch: 5/10...  Training Step: 421...  Training loss: 2.1146...  1.4271 sec/batch\n",
      "Epoch: 5/10...  Training Step: 422...  Training loss: 2.1367...  1.4488 sec/batch\n",
      "Epoch: 5/10...  Training Step: 423...  Training loss: 2.1714...  1.4311 sec/batch\n",
      "Epoch: 5/10...  Training Step: 424...  Training loss: 2.1500...  1.4664 sec/batch\n",
      "Epoch: 5/10...  Training Step: 425...  Training loss: 2.1371...  1.4708 sec/batch\n",
      "Epoch: 5/10...  Training Step: 426...  Training loss: 2.1361...  1.4463 sec/batch\n",
      "Epoch: 5/10...  Training Step: 427...  Training loss: 2.1130...  1.4515 sec/batch\n",
      "Epoch: 5/10...  Training Step: 428...  Training loss: 2.1249...  1.5097 sec/batch\n",
      "Epoch: 5/10...  Training Step: 429...  Training loss: 2.1531...  1.4519 sec/batch\n",
      "Epoch: 5/10...  Training Step: 430...  Training loss: 2.1401...  1.4323 sec/batch\n",
      "Epoch: 5/10...  Training Step: 431...  Training loss: 2.1357...  1.4448 sec/batch\n",
      "Epoch: 5/10...  Training Step: 432...  Training loss: 2.1079...  1.4324 sec/batch\n",
      "Epoch: 5/10...  Training Step: 433...  Training loss: 2.1101...  1.4511 sec/batch\n",
      "Epoch: 5/10...  Training Step: 434...  Training loss: 2.1001...  1.4205 sec/batch\n",
      "Epoch: 5/10...  Training Step: 435...  Training loss: 2.0975...  1.4387 sec/batch\n",
      "Epoch: 5/10...  Training Step: 436...  Training loss: 2.0717...  1.4131 sec/batch\n",
      "Epoch: 5/10...  Training Step: 437...  Training loss: 2.1036...  1.4066 sec/batch\n",
      "Epoch: 5/10...  Training Step: 438...  Training loss: 2.0905...  1.4103 sec/batch\n",
      "Epoch: 5/10...  Training Step: 439...  Training loss: 2.1194...  1.4338 sec/batch\n",
      "Epoch: 5/10...  Training Step: 440...  Training loss: 2.0990...  1.4387 sec/batch\n",
      "Epoch: 5/10...  Training Step: 441...  Training loss: 2.1298...  1.4338 sec/batch\n",
      "Epoch: 5/10...  Training Step: 442...  Training loss: 2.1099...  1.4145 sec/batch\n",
      "Epoch: 5/10...  Training Step: 443...  Training loss: 2.1189...  1.4148 sec/batch\n",
      "Epoch: 5/10...  Training Step: 444...  Training loss: 2.1188...  1.4238 sec/batch\n",
      "Epoch: 5/10...  Training Step: 445...  Training loss: 2.1140...  1.4232 sec/batch\n",
      "Epoch: 5/10...  Training Step: 446...  Training loss: 2.1458...  1.4270 sec/batch\n",
      "Epoch: 5/10...  Training Step: 447...  Training loss: 2.1261...  1.4333 sec/batch\n",
      "Epoch: 5/10...  Training Step: 448...  Training loss: 2.1195...  1.4090 sec/batch\n",
      "Epoch: 5/10...  Training Step: 449...  Training loss: 2.0998...  1.4264 sec/batch\n",
      "Epoch: 5/10...  Training Step: 450...  Training loss: 2.0870...  1.4130 sec/batch\n",
      "Epoch: 5/10...  Training Step: 451...  Training loss: 2.1136...  1.4438 sec/batch\n",
      "Epoch: 5/10...  Training Step: 452...  Training loss: 2.1311...  1.4093 sec/batch\n",
      "Epoch: 5/10...  Training Step: 453...  Training loss: 2.0950...  1.4127 sec/batch\n",
      "Epoch: 5/10...  Training Step: 454...  Training loss: 2.1092...  1.4204 sec/batch\n",
      "Epoch: 5/10...  Training Step: 455...  Training loss: 2.0965...  1.5260 sec/batch\n",
      "Epoch: 5/10...  Training Step: 456...  Training loss: 2.0885...  1.4169 sec/batch\n",
      "Epoch: 5/10...  Training Step: 457...  Training loss: 2.1142...  1.4140 sec/batch\n",
      "Epoch: 5/10...  Training Step: 458...  Training loss: 2.1201...  1.3977 sec/batch\n",
      "Epoch: 5/10...  Training Step: 459...  Training loss: 2.1103...  1.4077 sec/batch\n",
      "Epoch: 5/10...  Training Step: 460...  Training loss: 2.0898...  1.3701 sec/batch\n",
      "Epoch: 5/10...  Training Step: 461...  Training loss: 2.1047...  1.3767 sec/batch\n",
      "Epoch: 5/10...  Training Step: 462...  Training loss: 2.0899...  1.3778 sec/batch\n",
      "Epoch: 5/10...  Training Step: 463...  Training loss: 2.1150...  1.3742 sec/batch\n",
      "Epoch: 5/10...  Training Step: 464...  Training loss: 2.0935...  1.3714 sec/batch\n",
      "Epoch: 5/10...  Training Step: 465...  Training loss: 2.0944...  1.3741 sec/batch\n",
      "Epoch: 5/10...  Training Step: 466...  Training loss: 2.0793...  1.3722 sec/batch\n",
      "Epoch: 5/10...  Training Step: 467...  Training loss: 2.0766...  1.3816 sec/batch\n",
      "Epoch: 5/10...  Training Step: 468...  Training loss: 2.0832...  1.3895 sec/batch\n",
      "Epoch: 5/10...  Training Step: 469...  Training loss: 2.0975...  1.4041 sec/batch\n",
      "Epoch: 5/10...  Training Step: 470...  Training loss: 2.1005...  1.4031 sec/batch\n",
      "Epoch: 5/10...  Training Step: 471...  Training loss: 2.0983...  1.4215 sec/batch\n",
      "Epoch: 5/10...  Training Step: 472...  Training loss: 2.0968...  1.4076 sec/batch\n",
      "Epoch: 5/10...  Training Step: 473...  Training loss: 2.1025...  1.3976 sec/batch\n",
      "Epoch: 5/10...  Training Step: 474...  Training loss: 2.1117...  1.4346 sec/batch\n",
      "Epoch: 5/10...  Training Step: 475...  Training loss: 2.1015...  1.3935 sec/batch\n",
      "Epoch: 5/10...  Training Step: 476...  Training loss: 2.1156...  1.3905 sec/batch\n",
      "Epoch: 5/10...  Training Step: 477...  Training loss: 2.0829...  1.3776 sec/batch\n",
      "Epoch: 5/10...  Training Step: 478...  Training loss: 2.0897...  1.3998 sec/batch\n",
      "Epoch: 5/10...  Training Step: 479...  Training loss: 2.0477...  1.3983 sec/batch\n",
      "Epoch: 5/10...  Training Step: 480...  Training loss: 2.0849...  1.4061 sec/batch\n",
      "Epoch: 5/10...  Training Step: 481...  Training loss: 2.0789...  1.3956 sec/batch\n",
      "Epoch: 5/10...  Training Step: 482...  Training loss: 2.1082...  1.4010 sec/batch\n",
      "Epoch: 5/10...  Training Step: 483...  Training loss: 2.0669...  1.4169 sec/batch\n",
      "Epoch: 5/10...  Training Step: 484...  Training loss: 2.0734...  1.4123 sec/batch\n",
      "Epoch: 5/10...  Training Step: 485...  Training loss: 2.0500...  1.4248 sec/batch\n",
      "Epoch: 5/10...  Training Step: 486...  Training loss: 2.0696...  1.4102 sec/batch\n",
      "Epoch: 5/10...  Training Step: 487...  Training loss: 2.0707...  1.4094 sec/batch\n",
      "Epoch: 5/10...  Training Step: 488...  Training loss: 2.0875...  1.4119 sec/batch\n",
      "Epoch: 5/10...  Training Step: 489...  Training loss: 2.0461...  1.4221 sec/batch\n",
      "Epoch: 5/10...  Training Step: 490...  Training loss: 2.0776...  1.4439 sec/batch\n",
      "Epoch: 5/10...  Training Step: 491...  Training loss: 2.0671...  1.4561 sec/batch\n",
      "Epoch: 5/10...  Training Step: 492...  Training loss: 2.0612...  1.4297 sec/batch\n",
      "Epoch: 5/10...  Training Step: 493...  Training loss: 2.0806...  1.4297 sec/batch\n",
      "Epoch: 5/10...  Training Step: 494...  Training loss: 2.0683...  1.4375 sec/batch\n",
      "Epoch: 5/10...  Training Step: 495...  Training loss: 2.0761...  1.4173 sec/batch\n",
      "Epoch: 5/10...  Training Step: 496...  Training loss: 2.0385...  1.4181 sec/batch\n",
      "Epoch: 5/10...  Training Step: 497...  Training loss: 2.0577...  1.4423 sec/batch\n",
      "Epoch: 5/10...  Training Step: 498...  Training loss: 2.0662...  1.4537 sec/batch\n",
      "Epoch: 5/10...  Training Step: 499...  Training loss: 2.0715...  1.4153 sec/batch\n",
      "Epoch: 5/10...  Training Step: 500...  Training loss: 2.0435...  1.4200 sec/batch\n",
      "Epoch: 6/10...  Training Step: 501...  Training loss: 2.1203...  1.3763 sec/batch\n",
      "Epoch: 6/10...  Training Step: 502...  Training loss: 2.0851...  1.3787 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10...  Training Step: 503...  Training loss: 2.0588...  1.3747 sec/batch\n",
      "Epoch: 6/10...  Training Step: 504...  Training loss: 2.0471...  1.3789 sec/batch\n",
      "Epoch: 6/10...  Training Step: 505...  Training loss: 2.0528...  1.3782 sec/batch\n",
      "Epoch: 6/10...  Training Step: 506...  Training loss: 2.0729...  1.3799 sec/batch\n",
      "Epoch: 6/10...  Training Step: 507...  Training loss: 2.0817...  1.3790 sec/batch\n",
      "Epoch: 6/10...  Training Step: 508...  Training loss: 2.0503...  1.3795 sec/batch\n",
      "Epoch: 6/10...  Training Step: 509...  Training loss: 2.0522...  1.3812 sec/batch\n",
      "Epoch: 6/10...  Training Step: 510...  Training loss: 2.0564...  1.4132 sec/batch\n",
      "Epoch: 6/10...  Training Step: 511...  Training loss: 2.0547...  1.3900 sec/batch\n",
      "Epoch: 6/10...  Training Step: 512...  Training loss: 2.0318...  1.3997 sec/batch\n",
      "Epoch: 6/10...  Training Step: 513...  Training loss: 2.0298...  1.3978 sec/batch\n",
      "Epoch: 6/10...  Training Step: 514...  Training loss: 2.0152...  1.4111 sec/batch\n",
      "Epoch: 6/10...  Training Step: 515...  Training loss: 2.0435...  1.4189 sec/batch\n",
      "Epoch: 6/10...  Training Step: 516...  Training loss: 2.0440...  1.4405 sec/batch\n",
      "Epoch: 6/10...  Training Step: 517...  Training loss: 2.0362...  1.4254 sec/batch\n",
      "Epoch: 6/10...  Training Step: 518...  Training loss: 2.0219...  1.4519 sec/batch\n",
      "Epoch: 6/10...  Training Step: 519...  Training loss: 2.0264...  1.4443 sec/batch\n",
      "Epoch: 6/10...  Training Step: 520...  Training loss: 2.0180...  1.4170 sec/batch\n",
      "Epoch: 6/10...  Training Step: 521...  Training loss: 2.0184...  1.4121 sec/batch\n",
      "Epoch: 6/10...  Training Step: 522...  Training loss: 2.0358...  1.4301 sec/batch\n",
      "Epoch: 6/10...  Training Step: 523...  Training loss: 2.0762...  1.4382 sec/batch\n",
      "Epoch: 6/10...  Training Step: 524...  Training loss: 2.0491...  1.4621 sec/batch\n",
      "Epoch: 6/10...  Training Step: 525...  Training loss: 2.0365...  1.4634 sec/batch\n",
      "Epoch: 6/10...  Training Step: 526...  Training loss: 2.0410...  1.4471 sec/batch\n",
      "Epoch: 6/10...  Training Step: 527...  Training loss: 2.0318...  1.4293 sec/batch\n",
      "Epoch: 6/10...  Training Step: 528...  Training loss: 2.0297...  1.4438 sec/batch\n",
      "Epoch: 6/10...  Training Step: 529...  Training loss: 2.0545...  1.4292 sec/batch\n",
      "Epoch: 6/10...  Training Step: 530...  Training loss: 2.0450...  1.4277 sec/batch\n",
      "Epoch: 6/10...  Training Step: 531...  Training loss: 2.0419...  1.4520 sec/batch\n",
      "Epoch: 6/10...  Training Step: 532...  Training loss: 2.0157...  1.4503 sec/batch\n",
      "Epoch: 6/10...  Training Step: 533...  Training loss: 2.0312...  1.4311 sec/batch\n",
      "Epoch: 6/10...  Training Step: 534...  Training loss: 1.9970...  1.4384 sec/batch\n",
      "Epoch: 6/10...  Training Step: 535...  Training loss: 1.9908...  1.4453 sec/batch\n",
      "Epoch: 6/10...  Training Step: 536...  Training loss: 1.9761...  1.4318 sec/batch\n",
      "Epoch: 6/10...  Training Step: 537...  Training loss: 2.0106...  1.4481 sec/batch\n",
      "Epoch: 6/10...  Training Step: 538...  Training loss: 1.9868...  1.4288 sec/batch\n",
      "Epoch: 6/10...  Training Step: 539...  Training loss: 2.0124...  1.4467 sec/batch\n",
      "Epoch: 6/10...  Training Step: 540...  Training loss: 2.0002...  1.3969 sec/batch\n",
      "Epoch: 6/10...  Training Step: 541...  Training loss: 2.0410...  1.5931 sec/batch\n",
      "Epoch: 6/10...  Training Step: 542...  Training loss: 1.9983...  1.4559 sec/batch\n",
      "Epoch: 6/10...  Training Step: 543...  Training loss: 2.0270...  1.3932 sec/batch\n",
      "Epoch: 6/10...  Training Step: 544...  Training loss: 2.0254...  1.3913 sec/batch\n",
      "Epoch: 6/10...  Training Step: 545...  Training loss: 2.0114...  1.4006 sec/batch\n",
      "Epoch: 6/10...  Training Step: 546...  Training loss: 2.0451...  1.3938 sec/batch\n",
      "Epoch: 6/10...  Training Step: 547...  Training loss: 2.0285...  1.4177 sec/batch\n",
      "Epoch: 6/10...  Training Step: 548...  Training loss: 2.0196...  1.3977 sec/batch\n",
      "Epoch: 6/10...  Training Step: 549...  Training loss: 1.9987...  1.3974 sec/batch\n",
      "Epoch: 6/10...  Training Step: 550...  Training loss: 1.9929...  1.4426 sec/batch\n",
      "Epoch: 6/10...  Training Step: 551...  Training loss: 2.0038...  1.4581 sec/batch\n",
      "Epoch: 6/10...  Training Step: 552...  Training loss: 2.0239...  1.4032 sec/batch\n",
      "Epoch: 6/10...  Training Step: 553...  Training loss: 1.9943...  1.3894 sec/batch\n",
      "Epoch: 6/10...  Training Step: 554...  Training loss: 1.9913...  1.4032 sec/batch\n",
      "Epoch: 6/10...  Training Step: 555...  Training loss: 1.9944...  1.4016 sec/batch\n",
      "Epoch: 6/10...  Training Step: 556...  Training loss: 1.9919...  1.4022 sec/batch\n",
      "Epoch: 6/10...  Training Step: 557...  Training loss: 2.0149...  1.4207 sec/batch\n",
      "Epoch: 6/10...  Training Step: 558...  Training loss: 2.0213...  1.4060 sec/batch\n",
      "Epoch: 6/10...  Training Step: 559...  Training loss: 2.0180...  1.4349 sec/batch\n",
      "Epoch: 6/10...  Training Step: 560...  Training loss: 1.9820...  1.4491 sec/batch\n",
      "Epoch: 6/10...  Training Step: 561...  Training loss: 2.0147...  1.4353 sec/batch\n",
      "Epoch: 6/10...  Training Step: 562...  Training loss: 2.0024...  1.4075 sec/batch\n",
      "Epoch: 6/10...  Training Step: 563...  Training loss: 2.0104...  1.4208 sec/batch\n",
      "Epoch: 6/10...  Training Step: 564...  Training loss: 1.9999...  1.4291 sec/batch\n",
      "Epoch: 6/10...  Training Step: 565...  Training loss: 2.0047...  1.4082 sec/batch\n",
      "Epoch: 6/10...  Training Step: 566...  Training loss: 1.9857...  1.4213 sec/batch\n",
      "Epoch: 6/10...  Training Step: 567...  Training loss: 1.9723...  1.4200 sec/batch\n",
      "Epoch: 6/10...  Training Step: 568...  Training loss: 1.9875...  1.4199 sec/batch\n",
      "Epoch: 6/10...  Training Step: 569...  Training loss: 1.9912...  1.4526 sec/batch\n",
      "Epoch: 6/10...  Training Step: 570...  Training loss: 1.9971...  1.4567 sec/batch\n",
      "Epoch: 6/10...  Training Step: 571...  Training loss: 1.9981...  1.4096 sec/batch\n",
      "Epoch: 6/10...  Training Step: 572...  Training loss: 2.0097...  1.3949 sec/batch\n",
      "Epoch: 6/10...  Training Step: 573...  Training loss: 2.0007...  1.4301 sec/batch\n",
      "Epoch: 6/10...  Training Step: 574...  Training loss: 2.0049...  1.4213 sec/batch\n",
      "Epoch: 6/10...  Training Step: 575...  Training loss: 2.0085...  1.4288 sec/batch\n",
      "Epoch: 6/10...  Training Step: 576...  Training loss: 2.0122...  1.4321 sec/batch\n",
      "Epoch: 6/10...  Training Step: 577...  Training loss: 1.9944...  1.4373 sec/batch\n",
      "Epoch: 6/10...  Training Step: 578...  Training loss: 1.9923...  1.4485 sec/batch\n",
      "Epoch: 6/10...  Training Step: 579...  Training loss: 1.9439...  1.4453 sec/batch\n",
      "Epoch: 6/10...  Training Step: 580...  Training loss: 1.9826...  1.4396 sec/batch\n",
      "Epoch: 6/10...  Training Step: 581...  Training loss: 1.9791...  1.4382 sec/batch\n",
      "Epoch: 6/10...  Training Step: 582...  Training loss: 2.0024...  1.4284 sec/batch\n",
      "Epoch: 6/10...  Training Step: 583...  Training loss: 1.9677...  1.4277 sec/batch\n",
      "Epoch: 6/10...  Training Step: 584...  Training loss: 1.9709...  1.4354 sec/batch\n",
      "Epoch: 6/10...  Training Step: 585...  Training loss: 1.9602...  1.4421 sec/batch\n",
      "Epoch: 6/10...  Training Step: 586...  Training loss: 1.9705...  1.4310 sec/batch\n",
      "Epoch: 6/10...  Training Step: 587...  Training loss: 1.9573...  1.4424 sec/batch\n",
      "Epoch: 6/10...  Training Step: 588...  Training loss: 1.9846...  1.4365 sec/batch\n",
      "Epoch: 6/10...  Training Step: 589...  Training loss: 1.9464...  1.4378 sec/batch\n",
      "Epoch: 6/10...  Training Step: 590...  Training loss: 1.9818...  1.4197 sec/batch\n",
      "Epoch: 6/10...  Training Step: 591...  Training loss: 1.9830...  1.4155 sec/batch\n",
      "Epoch: 6/10...  Training Step: 592...  Training loss: 1.9690...  1.4660 sec/batch\n",
      "Epoch: 6/10...  Training Step: 593...  Training loss: 1.9869...  1.4822 sec/batch\n",
      "Epoch: 6/10...  Training Step: 594...  Training loss: 1.9673...  1.4535 sec/batch\n",
      "Epoch: 6/10...  Training Step: 595...  Training loss: 1.9855...  1.4223 sec/batch\n",
      "Epoch: 6/10...  Training Step: 596...  Training loss: 1.9282...  1.4159 sec/batch\n",
      "Epoch: 6/10...  Training Step: 597...  Training loss: 1.9510...  1.4133 sec/batch\n",
      "Epoch: 6/10...  Training Step: 598...  Training loss: 1.9614...  1.4141 sec/batch\n",
      "Epoch: 6/10...  Training Step: 599...  Training loss: 1.9813...  1.4384 sec/batch\n",
      "Epoch: 6/10...  Training Step: 600...  Training loss: 1.9468...  1.4418 sec/batch\n",
      "Epoch: 7/10...  Training Step: 601...  Training loss: 2.0209...  1.3891 sec/batch\n",
      "Epoch: 7/10...  Training Step: 602...  Training loss: 1.9831...  1.3910 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10...  Training Step: 603...  Training loss: 1.9615...  1.3812 sec/batch\n",
      "Epoch: 7/10...  Training Step: 604...  Training loss: 1.9582...  1.3859 sec/batch\n",
      "Epoch: 7/10...  Training Step: 605...  Training loss: 1.9543...  1.3986 sec/batch\n",
      "Epoch: 7/10...  Training Step: 606...  Training loss: 1.9789...  1.3899 sec/batch\n",
      "Epoch: 7/10...  Training Step: 607...  Training loss: 1.9874...  1.4056 sec/batch\n",
      "Epoch: 7/10...  Training Step: 608...  Training loss: 1.9471...  1.4097 sec/batch\n",
      "Epoch: 7/10...  Training Step: 609...  Training loss: 1.9617...  1.4116 sec/batch\n",
      "Epoch: 7/10...  Training Step: 610...  Training loss: 1.9596...  1.4210 sec/batch\n",
      "Epoch: 7/10...  Training Step: 611...  Training loss: 1.9521...  1.4110 sec/batch\n",
      "Epoch: 7/10...  Training Step: 612...  Training loss: 1.9341...  1.4390 sec/batch\n",
      "Epoch: 7/10...  Training Step: 613...  Training loss: 1.9332...  1.4557 sec/batch\n",
      "Epoch: 7/10...  Training Step: 614...  Training loss: 1.9183...  1.4216 sec/batch\n",
      "Epoch: 7/10...  Training Step: 615...  Training loss: 1.9398...  1.4223 sec/batch\n",
      "Epoch: 7/10...  Training Step: 616...  Training loss: 1.9497...  1.4327 sec/batch\n",
      "Epoch: 7/10...  Training Step: 617...  Training loss: 1.9294...  1.4209 sec/batch\n",
      "Epoch: 7/10...  Training Step: 618...  Training loss: 1.9301...  1.4311 sec/batch\n",
      "Epoch: 7/10...  Training Step: 619...  Training loss: 1.9261...  1.4120 sec/batch\n",
      "Epoch: 7/10...  Training Step: 620...  Training loss: 1.9267...  1.4023 sec/batch\n",
      "Epoch: 7/10...  Training Step: 621...  Training loss: 1.9254...  1.4241 sec/batch\n",
      "Epoch: 7/10...  Training Step: 622...  Training loss: 1.9382...  1.4240 sec/batch\n",
      "Epoch: 7/10...  Training Step: 623...  Training loss: 1.9767...  1.4138 sec/batch\n",
      "Epoch: 7/10...  Training Step: 624...  Training loss: 1.9436...  1.4203 sec/batch\n",
      "Epoch: 7/10...  Training Step: 625...  Training loss: 1.9281...  1.4298 sec/batch\n",
      "Epoch: 7/10...  Training Step: 626...  Training loss: 1.9528...  1.4444 sec/batch\n",
      "Epoch: 7/10...  Training Step: 627...  Training loss: 1.9346...  1.5053 sec/batch\n",
      "Epoch: 7/10...  Training Step: 628...  Training loss: 1.9386...  1.4106 sec/batch\n",
      "Epoch: 7/10...  Training Step: 629...  Training loss: 1.9519...  1.4104 sec/batch\n",
      "Epoch: 7/10...  Training Step: 630...  Training loss: 1.9472...  1.4106 sec/batch\n",
      "Epoch: 7/10...  Training Step: 631...  Training loss: 1.9430...  1.4316 sec/batch\n",
      "Epoch: 7/10...  Training Step: 632...  Training loss: 1.9079...  1.4421 sec/batch\n",
      "Epoch: 7/10...  Training Step: 633...  Training loss: 1.9226...  1.4406 sec/batch\n",
      "Epoch: 7/10...  Training Step: 634...  Training loss: 1.8971...  1.4154 sec/batch\n",
      "Epoch: 7/10...  Training Step: 635...  Training loss: 1.8853...  1.4317 sec/batch\n",
      "Epoch: 7/10...  Training Step: 636...  Training loss: 1.8812...  1.4163 sec/batch\n",
      "Epoch: 7/10...  Training Step: 637...  Training loss: 1.9161...  1.4341 sec/batch\n",
      "Epoch: 7/10...  Training Step: 638...  Training loss: 1.8907...  1.4475 sec/batch\n",
      "Epoch: 7/10...  Training Step: 639...  Training loss: 1.9238...  1.4317 sec/batch\n",
      "Epoch: 7/10...  Training Step: 640...  Training loss: 1.9029...  1.4150 sec/batch\n",
      "Epoch: 7/10...  Training Step: 641...  Training loss: 1.9395...  1.4105 sec/batch\n",
      "Epoch: 7/10...  Training Step: 642...  Training loss: 1.9173...  1.4173 sec/batch\n",
      "Epoch: 7/10...  Training Step: 643...  Training loss: 1.9230...  1.3989 sec/batch\n",
      "Epoch: 7/10...  Training Step: 644...  Training loss: 1.9316...  1.4075 sec/batch\n",
      "Epoch: 7/10...  Training Step: 645...  Training loss: 1.9110...  1.4112 sec/batch\n",
      "Epoch: 7/10...  Training Step: 646...  Training loss: 1.9438...  1.4225 sec/batch\n",
      "Epoch: 7/10...  Training Step: 647...  Training loss: 1.9377...  1.4192 sec/batch\n",
      "Epoch: 7/10...  Training Step: 648...  Training loss: 1.9144...  1.4138 sec/batch\n",
      "Epoch: 7/10...  Training Step: 649...  Training loss: 1.9152...  1.4313 sec/batch\n",
      "Epoch: 7/10...  Training Step: 650...  Training loss: 1.8902...  1.4300 sec/batch\n",
      "Epoch: 7/10...  Training Step: 651...  Training loss: 1.9174...  1.4363 sec/batch\n",
      "Epoch: 7/10...  Training Step: 652...  Training loss: 1.9317...  1.4401 sec/batch\n",
      "Epoch: 7/10...  Training Step: 653...  Training loss: 1.8988...  1.3910 sec/batch\n",
      "Epoch: 7/10...  Training Step: 654...  Training loss: 1.9060...  1.4090 sec/batch\n",
      "Epoch: 7/10...  Training Step: 655...  Training loss: 1.9053...  1.4406 sec/batch\n",
      "Epoch: 7/10...  Training Step: 656...  Training loss: 1.9004...  1.4189 sec/batch\n",
      "Epoch: 7/10...  Training Step: 657...  Training loss: 1.9302...  1.4769 sec/batch\n",
      "Epoch: 7/10...  Training Step: 658...  Training loss: 1.9354...  1.4840 sec/batch\n",
      "Epoch: 7/10...  Training Step: 659...  Training loss: 1.9243...  1.4637 sec/batch\n",
      "Epoch: 7/10...  Training Step: 660...  Training loss: 1.8955...  1.4345 sec/batch\n",
      "Epoch: 7/10...  Training Step: 661...  Training loss: 1.9348...  1.4334 sec/batch\n",
      "Epoch: 7/10...  Training Step: 662...  Training loss: 1.9169...  1.4412 sec/batch\n",
      "Epoch: 7/10...  Training Step: 663...  Training loss: 1.9267...  1.4235 sec/batch\n",
      "Epoch: 7/10...  Training Step: 664...  Training loss: 1.9033...  1.4264 sec/batch\n",
      "Epoch: 7/10...  Training Step: 665...  Training loss: 1.9128...  1.4352 sec/batch\n",
      "Epoch: 7/10...  Training Step: 666...  Training loss: 1.8945...  1.4356 sec/batch\n",
      "Epoch: 7/10...  Training Step: 667...  Training loss: 1.8832...  1.4392 sec/batch\n",
      "Epoch: 7/10...  Training Step: 668...  Training loss: 1.8891...  1.4428 sec/batch\n",
      "Epoch: 7/10...  Training Step: 669...  Training loss: 1.8984...  1.4387 sec/batch\n",
      "Epoch: 7/10...  Training Step: 670...  Training loss: 1.9110...  1.4106 sec/batch\n",
      "Epoch: 7/10...  Training Step: 671...  Training loss: 1.9151...  1.4070 sec/batch\n",
      "Epoch: 7/10...  Training Step: 672...  Training loss: 1.9248...  1.4116 sec/batch\n",
      "Epoch: 7/10...  Training Step: 673...  Training loss: 1.9170...  1.4416 sec/batch\n",
      "Epoch: 7/10...  Training Step: 674...  Training loss: 1.9230...  1.4414 sec/batch\n",
      "Epoch: 7/10...  Training Step: 675...  Training loss: 1.9155...  1.4357 sec/batch\n",
      "Epoch: 7/10...  Training Step: 676...  Training loss: 1.9234...  1.4318 sec/batch\n",
      "Epoch: 7/10...  Training Step: 677...  Training loss: 1.9108...  1.4230 sec/batch\n",
      "Epoch: 7/10...  Training Step: 678...  Training loss: 1.9033...  1.4064 sec/batch\n",
      "Epoch: 7/10...  Training Step: 679...  Training loss: 1.8449...  1.4186 sec/batch\n",
      "Epoch: 7/10...  Training Step: 680...  Training loss: 1.8776...  1.4284 sec/batch\n",
      "Epoch: 7/10...  Training Step: 681...  Training loss: 1.8903...  1.4342 sec/batch\n",
      "Epoch: 7/10...  Training Step: 682...  Training loss: 1.9179...  1.4418 sec/batch\n",
      "Epoch: 7/10...  Training Step: 683...  Training loss: 1.8778...  1.4292 sec/batch\n",
      "Epoch: 7/10...  Training Step: 684...  Training loss: 1.8749...  1.4500 sec/batch\n",
      "Epoch: 7/10...  Training Step: 685...  Training loss: 1.8611...  1.4232 sec/batch\n",
      "Epoch: 7/10...  Training Step: 686...  Training loss: 1.8806...  1.4264 sec/batch\n",
      "Epoch: 7/10...  Training Step: 687...  Training loss: 1.8657...  1.4276 sec/batch\n",
      "Epoch: 7/10...  Training Step: 688...  Training loss: 1.8887...  1.4239 sec/batch\n",
      "Epoch: 7/10...  Training Step: 689...  Training loss: 1.8580...  1.4441 sec/batch\n",
      "Epoch: 7/10...  Training Step: 690...  Training loss: 1.8825...  1.4422 sec/batch\n",
      "Epoch: 7/10...  Training Step: 691...  Training loss: 1.8892...  1.4358 sec/batch\n",
      "Epoch: 7/10...  Training Step: 692...  Training loss: 1.8756...  1.4510 sec/batch\n",
      "Epoch: 7/10...  Training Step: 693...  Training loss: 1.8971...  1.4258 sec/batch\n",
      "Epoch: 7/10...  Training Step: 694...  Training loss: 1.8699...  1.4583 sec/batch\n",
      "Epoch: 7/10...  Training Step: 695...  Training loss: 1.9038...  1.4387 sec/batch\n",
      "Epoch: 7/10...  Training Step: 696...  Training loss: 1.8393...  1.4490 sec/batch\n",
      "Epoch: 7/10...  Training Step: 697...  Training loss: 1.8638...  1.4790 sec/batch\n",
      "Epoch: 7/10...  Training Step: 698...  Training loss: 1.8590...  1.4042 sec/batch\n",
      "Epoch: 7/10...  Training Step: 699...  Training loss: 1.8695...  1.3996 sec/batch\n",
      "Epoch: 7/10...  Training Step: 700...  Training loss: 1.8656...  1.4275 sec/batch\n",
      "Epoch: 8/10...  Training Step: 701...  Training loss: 1.9309...  1.3765 sec/batch\n",
      "Epoch: 8/10...  Training Step: 702...  Training loss: 1.8984...  1.3800 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10...  Training Step: 703...  Training loss: 1.8678...  1.3776 sec/batch\n",
      "Epoch: 8/10...  Training Step: 704...  Training loss: 1.8599...  1.3780 sec/batch\n",
      "Epoch: 8/10...  Training Step: 705...  Training loss: 1.8561...  1.3791 sec/batch\n",
      "Epoch: 8/10...  Training Step: 706...  Training loss: 1.8814...  1.3777 sec/batch\n",
      "Epoch: 8/10...  Training Step: 707...  Training loss: 1.8996...  1.3776 sec/batch\n",
      "Epoch: 8/10...  Training Step: 708...  Training loss: 1.8597...  1.3773 sec/batch\n",
      "Epoch: 8/10...  Training Step: 709...  Training loss: 1.8786...  1.3773 sec/batch\n",
      "Epoch: 8/10...  Training Step: 710...  Training loss: 1.8663...  1.3821 sec/batch\n",
      "Epoch: 8/10...  Training Step: 711...  Training loss: 1.8687...  1.3899 sec/batch\n",
      "Epoch: 8/10...  Training Step: 712...  Training loss: 1.8526...  1.3974 sec/batch\n",
      "Epoch: 8/10...  Training Step: 713...  Training loss: 1.8402...  1.4133 sec/batch\n",
      "Epoch: 8/10...  Training Step: 714...  Training loss: 1.8191...  1.4060 sec/batch\n",
      "Epoch: 8/10...  Training Step: 715...  Training loss: 1.8586...  1.4090 sec/batch\n",
      "Epoch: 8/10...  Training Step: 716...  Training loss: 1.8632...  1.4074 sec/batch\n",
      "Epoch: 8/10...  Training Step: 717...  Training loss: 1.8445...  1.4304 sec/batch\n",
      "Epoch: 8/10...  Training Step: 718...  Training loss: 1.8469...  1.4622 sec/batch\n",
      "Epoch: 8/10...  Training Step: 719...  Training loss: 1.8455...  1.4348 sec/batch\n",
      "Epoch: 8/10...  Training Step: 720...  Training loss: 1.8387...  1.4362 sec/batch\n",
      "Epoch: 8/10...  Training Step: 721...  Training loss: 1.8465...  1.4480 sec/batch\n",
      "Epoch: 8/10...  Training Step: 722...  Training loss: 1.8541...  1.4875 sec/batch\n",
      "Epoch: 8/10...  Training Step: 723...  Training loss: 1.8987...  1.4488 sec/batch\n",
      "Epoch: 8/10...  Training Step: 724...  Training loss: 1.8485...  1.4704 sec/batch\n",
      "Epoch: 8/10...  Training Step: 725...  Training loss: 1.8458...  1.4275 sec/batch\n",
      "Epoch: 8/10...  Training Step: 726...  Training loss: 1.8780...  1.4356 sec/batch\n",
      "Epoch: 8/10...  Training Step: 727...  Training loss: 1.8448...  1.4503 sec/batch\n",
      "Epoch: 8/10...  Training Step: 728...  Training loss: 1.8522...  1.4334 sec/batch\n",
      "Epoch: 8/10...  Training Step: 729...  Training loss: 1.8712...  1.4395 sec/batch\n",
      "Epoch: 8/10...  Training Step: 730...  Training loss: 1.8788...  1.4149 sec/batch\n",
      "Epoch: 8/10...  Training Step: 731...  Training loss: 1.8398...  1.4464 sec/batch\n",
      "Epoch: 8/10...  Training Step: 732...  Training loss: 1.8173...  1.4667 sec/batch\n",
      "Epoch: 8/10...  Training Step: 733...  Training loss: 1.8358...  1.4711 sec/batch\n",
      "Epoch: 8/10...  Training Step: 734...  Training loss: 1.8197...  1.4419 sec/batch\n",
      "Epoch: 8/10...  Training Step: 735...  Training loss: 1.8044...  1.4662 sec/batch\n",
      "Epoch: 8/10...  Training Step: 736...  Training loss: 1.7933...  1.4565 sec/batch\n",
      "Epoch: 8/10...  Training Step: 737...  Training loss: 1.8272...  1.4344 sec/batch\n",
      "Epoch: 8/10...  Training Step: 738...  Training loss: 1.8010...  1.4383 sec/batch\n",
      "Epoch: 8/10...  Training Step: 739...  Training loss: 1.8278...  1.4404 sec/batch\n",
      "Epoch: 8/10...  Training Step: 740...  Training loss: 1.8201...  1.4304 sec/batch\n",
      "Epoch: 8/10...  Training Step: 741...  Training loss: 1.8668...  1.4337 sec/batch\n",
      "Epoch: 8/10...  Training Step: 742...  Training loss: 1.8384...  1.4358 sec/batch\n",
      "Epoch: 8/10...  Training Step: 743...  Training loss: 1.8351...  1.4286 sec/batch\n",
      "Epoch: 8/10...  Training Step: 744...  Training loss: 1.8497...  1.4309 sec/batch\n",
      "Epoch: 8/10...  Training Step: 745...  Training loss: 1.8267...  1.4368 sec/batch\n",
      "Epoch: 8/10...  Training Step: 746...  Training loss: 1.8607...  1.4369 sec/batch\n",
      "Epoch: 8/10...  Training Step: 747...  Training loss: 1.8607...  1.4408 sec/batch\n",
      "Epoch: 8/10...  Training Step: 748...  Training loss: 1.8434...  1.4142 sec/batch\n",
      "Epoch: 8/10...  Training Step: 749...  Training loss: 1.8255...  1.4175 sec/batch\n",
      "Epoch: 8/10...  Training Step: 750...  Training loss: 1.8222...  1.4124 sec/batch\n",
      "Epoch: 8/10...  Training Step: 751...  Training loss: 1.8436...  1.4465 sec/batch\n",
      "Epoch: 8/10...  Training Step: 752...  Training loss: 1.8344...  1.4323 sec/batch\n",
      "Epoch: 8/10...  Training Step: 753...  Training loss: 1.8122...  1.4214 sec/batch\n",
      "Epoch: 8/10...  Training Step: 754...  Training loss: 1.8246...  1.4115 sec/batch\n",
      "Epoch: 8/10...  Training Step: 755...  Training loss: 1.8189...  1.4145 sec/batch\n",
      "Epoch: 8/10...  Training Step: 756...  Training loss: 1.8154...  1.4281 sec/batch\n",
      "Epoch: 8/10...  Training Step: 757...  Training loss: 1.8588...  1.4143 sec/batch\n",
      "Epoch: 8/10...  Training Step: 758...  Training loss: 1.8621...  1.4222 sec/batch\n",
      "Epoch: 8/10...  Training Step: 759...  Training loss: 1.8447...  1.4164 sec/batch\n",
      "Epoch: 8/10...  Training Step: 760...  Training loss: 1.8101...  1.4289 sec/batch\n",
      "Epoch: 8/10...  Training Step: 761...  Training loss: 1.8498...  1.4483 sec/batch\n",
      "Epoch: 8/10...  Training Step: 762...  Training loss: 1.8342...  1.4334 sec/batch\n",
      "Epoch: 8/10...  Training Step: 763...  Training loss: 1.8426...  1.4849 sec/batch\n",
      "Epoch: 8/10...  Training Step: 764...  Training loss: 1.8190...  1.4151 sec/batch\n",
      "Epoch: 8/10...  Training Step: 765...  Training loss: 1.8227...  1.4139 sec/batch\n",
      "Epoch: 8/10...  Training Step: 766...  Training loss: 1.8216...  1.4285 sec/batch\n",
      "Epoch: 8/10...  Training Step: 767...  Training loss: 1.8079...  1.4222 sec/batch\n",
      "Epoch: 8/10...  Training Step: 768...  Training loss: 1.8107...  1.4323 sec/batch\n",
      "Epoch: 8/10...  Training Step: 769...  Training loss: 1.8146...  1.4575 sec/batch\n",
      "Epoch: 8/10...  Training Step: 770...  Training loss: 1.8284...  1.4616 sec/batch\n",
      "Epoch: 8/10...  Training Step: 771...  Training loss: 1.8412...  1.4409 sec/batch\n",
      "Epoch: 8/10...  Training Step: 772...  Training loss: 1.8465...  1.4278 sec/batch\n",
      "Epoch: 8/10...  Training Step: 773...  Training loss: 1.8263...  1.4420 sec/batch\n",
      "Epoch: 8/10...  Training Step: 774...  Training loss: 1.8378...  1.4465 sec/batch\n",
      "Epoch: 8/10...  Training Step: 775...  Training loss: 1.8376...  1.4622 sec/batch\n",
      "Epoch: 8/10...  Training Step: 776...  Training loss: 1.8533...  1.4259 sec/batch\n",
      "Epoch: 8/10...  Training Step: 777...  Training loss: 1.8409...  1.4692 sec/batch\n",
      "Epoch: 8/10...  Training Step: 778...  Training loss: 1.8339...  1.4624 sec/batch\n",
      "Epoch: 8/10...  Training Step: 779...  Training loss: 1.7627...  1.4391 sec/batch\n",
      "Epoch: 8/10...  Training Step: 780...  Training loss: 1.8062...  1.4325 sec/batch\n",
      "Epoch: 8/10...  Training Step: 781...  Training loss: 1.8184...  1.4406 sec/batch\n",
      "Epoch: 8/10...  Training Step: 782...  Training loss: 1.8378...  1.4473 sec/batch\n",
      "Epoch: 8/10...  Training Step: 783...  Training loss: 1.7959...  1.4654 sec/batch\n",
      "Epoch: 8/10...  Training Step: 784...  Training loss: 1.7874...  1.4429 sec/batch\n",
      "Epoch: 8/10...  Training Step: 785...  Training loss: 1.7969...  1.4441 sec/batch\n",
      "Epoch: 8/10...  Training Step: 786...  Training loss: 1.8071...  1.4225 sec/batch\n",
      "Epoch: 8/10...  Training Step: 787...  Training loss: 1.7843...  1.4157 sec/batch\n",
      "Epoch: 8/10...  Training Step: 788...  Training loss: 1.8156...  1.4169 sec/batch\n",
      "Epoch: 8/10...  Training Step: 789...  Training loss: 1.7738...  1.4259 sec/batch\n",
      "Epoch: 8/10...  Training Step: 790...  Training loss: 1.7940...  1.4369 sec/batch\n",
      "Epoch: 8/10...  Training Step: 791...  Training loss: 1.8030...  1.4739 sec/batch\n",
      "Epoch: 8/10...  Training Step: 792...  Training loss: 1.7923...  1.4171 sec/batch\n",
      "Epoch: 8/10...  Training Step: 793...  Training loss: 1.8103...  1.4520 sec/batch\n",
      "Epoch: 8/10...  Training Step: 794...  Training loss: 1.7950...  1.4498 sec/batch\n",
      "Epoch: 8/10...  Training Step: 795...  Training loss: 1.8146...  1.4317 sec/batch\n",
      "Epoch: 8/10...  Training Step: 796...  Training loss: 1.7578...  1.4389 sec/batch\n",
      "Epoch: 8/10...  Training Step: 797...  Training loss: 1.7881...  1.4465 sec/batch\n",
      "Epoch: 8/10...  Training Step: 798...  Training loss: 1.7796...  1.4236 sec/batch\n",
      "Epoch: 8/10...  Training Step: 799...  Training loss: 1.7885...  1.4380 sec/batch\n",
      "Epoch: 8/10...  Training Step: 800...  Training loss: 1.7843...  1.4170 sec/batch\n",
      "Epoch: 9/10...  Training Step: 801...  Training loss: 1.8573...  1.3762 sec/batch\n",
      "Epoch: 9/10...  Training Step: 802...  Training loss: 1.8098...  1.3778 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10...  Training Step: 803...  Training loss: 1.7802...  1.3928 sec/batch\n",
      "Epoch: 9/10...  Training Step: 804...  Training loss: 1.7775...  1.3893 sec/batch\n",
      "Epoch: 9/10...  Training Step: 805...  Training loss: 1.7946...  1.3916 sec/batch\n",
      "Epoch: 9/10...  Training Step: 806...  Training loss: 1.8089...  1.4060 sec/batch\n",
      "Epoch: 9/10...  Training Step: 807...  Training loss: 1.8370...  1.4466 sec/batch\n",
      "Epoch: 9/10...  Training Step: 808...  Training loss: 1.7827...  1.4016 sec/batch\n",
      "Epoch: 9/10...  Training Step: 809...  Training loss: 1.7979...  1.3971 sec/batch\n",
      "Epoch: 9/10...  Training Step: 810...  Training loss: 1.7805...  1.4177 sec/batch\n",
      "Epoch: 9/10...  Training Step: 811...  Training loss: 1.7881...  1.4260 sec/batch\n",
      "Epoch: 9/10...  Training Step: 812...  Training loss: 1.7784...  1.4304 sec/batch\n",
      "Epoch: 9/10...  Training Step: 813...  Training loss: 1.7595...  1.4818 sec/batch\n",
      "Epoch: 9/10...  Training Step: 814...  Training loss: 1.7413...  1.4780 sec/batch\n",
      "Epoch: 9/10...  Training Step: 815...  Training loss: 1.7760...  1.4055 sec/batch\n",
      "Epoch: 9/10...  Training Step: 816...  Training loss: 1.7942...  1.3969 sec/batch\n",
      "Epoch: 9/10...  Training Step: 817...  Training loss: 1.7731...  1.4148 sec/batch\n",
      "Epoch: 9/10...  Training Step: 818...  Training loss: 1.7752...  1.4144 sec/batch\n",
      "Epoch: 9/10...  Training Step: 819...  Training loss: 1.7780...  1.4145 sec/batch\n",
      "Epoch: 9/10...  Training Step: 820...  Training loss: 1.7682...  1.4183 sec/batch\n",
      "Epoch: 9/10...  Training Step: 821...  Training loss: 1.7685...  1.4093 sec/batch\n",
      "Epoch: 9/10...  Training Step: 822...  Training loss: 1.7752...  1.4175 sec/batch\n",
      "Epoch: 9/10...  Training Step: 823...  Training loss: 1.8233...  1.4111 sec/batch\n",
      "Epoch: 9/10...  Training Step: 824...  Training loss: 1.7756...  1.4197 sec/batch\n",
      "Epoch: 9/10...  Training Step: 825...  Training loss: 1.7775...  1.4179 sec/batch\n",
      "Epoch: 9/10...  Training Step: 826...  Training loss: 1.8083...  1.4172 sec/batch\n",
      "Epoch: 9/10...  Training Step: 827...  Training loss: 1.7707...  1.4285 sec/batch\n",
      "Epoch: 9/10...  Training Step: 828...  Training loss: 1.7829...  1.4125 sec/batch\n",
      "Epoch: 9/10...  Training Step: 829...  Training loss: 1.8044...  1.4447 sec/batch\n",
      "Epoch: 9/10...  Training Step: 830...  Training loss: 1.8046...  1.4321 sec/batch\n",
      "Epoch: 9/10...  Training Step: 831...  Training loss: 1.7760...  1.4374 sec/batch\n",
      "Epoch: 9/10...  Training Step: 832...  Training loss: 1.7624...  1.4305 sec/batch\n",
      "Epoch: 9/10...  Training Step: 833...  Training loss: 1.7701...  1.4301 sec/batch\n",
      "Epoch: 9/10...  Training Step: 834...  Training loss: 1.7447...  1.4283 sec/batch\n",
      "Epoch: 9/10...  Training Step: 835...  Training loss: 1.7250...  1.4249 sec/batch\n",
      "Epoch: 9/10...  Training Step: 836...  Training loss: 1.7192...  1.4441 sec/batch\n",
      "Epoch: 9/10...  Training Step: 837...  Training loss: 1.7597...  1.4383 sec/batch\n",
      "Epoch: 9/10...  Training Step: 838...  Training loss: 1.7315...  1.4339 sec/batch\n",
      "Epoch: 9/10...  Training Step: 839...  Training loss: 1.7657...  1.4312 sec/batch\n",
      "Epoch: 9/10...  Training Step: 840...  Training loss: 1.7346...  1.4531 sec/batch\n",
      "Epoch: 9/10...  Training Step: 841...  Training loss: 1.8025...  1.4264 sec/batch\n",
      "Epoch: 9/10...  Training Step: 842...  Training loss: 1.7677...  1.4368 sec/batch\n",
      "Epoch: 9/10...  Training Step: 843...  Training loss: 1.7661...  1.4282 sec/batch\n",
      "Epoch: 9/10...  Training Step: 844...  Training loss: 1.7886...  1.4168 sec/batch\n",
      "Epoch: 9/10...  Training Step: 845...  Training loss: 1.7501...  1.4260 sec/batch\n",
      "Epoch: 9/10...  Training Step: 846...  Training loss: 1.7967...  1.4505 sec/batch\n",
      "Epoch: 9/10...  Training Step: 847...  Training loss: 1.7895...  1.4431 sec/batch\n",
      "Epoch: 9/10...  Training Step: 848...  Training loss: 1.7732...  1.4345 sec/batch\n",
      "Epoch: 9/10...  Training Step: 849...  Training loss: 1.7623...  1.4433 sec/batch\n",
      "Epoch: 9/10...  Training Step: 850...  Training loss: 1.7429...  1.4228 sec/batch\n",
      "Epoch: 9/10...  Training Step: 851...  Training loss: 1.7792...  1.4216 sec/batch\n",
      "Epoch: 9/10...  Training Step: 852...  Training loss: 1.7636...  1.4419 sec/batch\n",
      "Epoch: 9/10...  Training Step: 853...  Training loss: 1.7268...  1.4146 sec/batch\n",
      "Epoch: 9/10...  Training Step: 854...  Training loss: 1.7473...  1.4165 sec/batch\n",
      "Epoch: 9/10...  Training Step: 855...  Training loss: 1.7416...  1.4198 sec/batch\n",
      "Epoch: 9/10...  Training Step: 856...  Training loss: 1.7571...  1.4109 sec/batch\n",
      "Epoch: 9/10...  Training Step: 857...  Training loss: 1.7954...  1.4102 sec/batch\n",
      "Epoch: 9/10...  Training Step: 858...  Training loss: 1.7850...  1.4103 sec/batch\n",
      "Epoch: 9/10...  Training Step: 859...  Training loss: 1.7781...  1.4146 sec/batch\n",
      "Epoch: 9/10...  Training Step: 860...  Training loss: 1.7381...  1.4109 sec/batch\n",
      "Epoch: 9/10...  Training Step: 861...  Training loss: 1.7890...  1.4140 sec/batch\n",
      "Epoch: 9/10...  Training Step: 862...  Training loss: 1.7662...  1.4230 sec/batch\n",
      "Epoch: 9/10...  Training Step: 863...  Training loss: 1.7697...  1.4054 sec/batch\n",
      "Epoch: 9/10...  Training Step: 864...  Training loss: 1.7476...  1.4555 sec/batch\n",
      "Epoch: 9/10...  Training Step: 865...  Training loss: 1.7578...  1.4375 sec/batch\n",
      "Epoch: 9/10...  Training Step: 866...  Training loss: 1.7540...  1.4327 sec/batch\n",
      "Epoch: 9/10...  Training Step: 867...  Training loss: 1.7379...  1.4696 sec/batch\n",
      "Epoch: 9/10...  Training Step: 868...  Training loss: 1.7426...  1.4455 sec/batch\n",
      "Epoch: 9/10...  Training Step: 869...  Training loss: 1.7484...  1.4397 sec/batch\n",
      "Epoch: 9/10...  Training Step: 870...  Training loss: 1.7620...  1.4269 sec/batch\n",
      "Epoch: 9/10...  Training Step: 871...  Training loss: 1.7728...  1.4156 sec/batch\n",
      "Epoch: 9/10...  Training Step: 872...  Training loss: 1.7873...  1.4209 sec/batch\n",
      "Epoch: 9/10...  Training Step: 873...  Training loss: 1.7602...  1.4229 sec/batch\n",
      "Epoch: 9/10...  Training Step: 874...  Training loss: 1.7668...  1.4638 sec/batch\n",
      "Epoch: 9/10...  Training Step: 875...  Training loss: 1.7810...  1.4583 sec/batch\n",
      "Epoch: 9/10...  Training Step: 876...  Training loss: 1.7738...  1.4083 sec/batch\n",
      "Epoch: 9/10...  Training Step: 877...  Training loss: 1.7765...  1.4011 sec/batch\n",
      "Epoch: 9/10...  Training Step: 878...  Training loss: 1.7612...  1.4332 sec/batch\n",
      "Epoch: 9/10...  Training Step: 879...  Training loss: 1.6978...  1.4107 sec/batch\n",
      "Epoch: 9/10...  Training Step: 880...  Training loss: 1.7382...  1.4271 sec/batch\n",
      "Epoch: 9/10...  Training Step: 881...  Training loss: 1.7434...  1.4357 sec/batch\n",
      "Epoch: 9/10...  Training Step: 882...  Training loss: 1.7695...  1.4395 sec/batch\n",
      "Epoch: 9/10...  Training Step: 883...  Training loss: 1.7295...  1.4168 sec/batch\n",
      "Epoch: 9/10...  Training Step: 884...  Training loss: 1.7281...  1.4114 sec/batch\n",
      "Epoch: 9/10...  Training Step: 885...  Training loss: 1.7193...  1.4133 sec/batch\n",
      "Epoch: 9/10...  Training Step: 886...  Training loss: 1.7415...  1.4158 sec/batch\n",
      "Epoch: 9/10...  Training Step: 887...  Training loss: 1.7259...  1.4118 sec/batch\n",
      "Epoch: 9/10...  Training Step: 888...  Training loss: 1.7330...  1.4569 sec/batch\n",
      "Epoch: 9/10...  Training Step: 889...  Training loss: 1.7065...  1.4162 sec/batch\n",
      "Epoch: 9/10...  Training Step: 890...  Training loss: 1.7278...  1.4113 sec/batch\n",
      "Epoch: 9/10...  Training Step: 891...  Training loss: 1.7259...  1.4199 sec/batch\n",
      "Epoch: 9/10...  Training Step: 892...  Training loss: 1.7147...  1.4554 sec/batch\n",
      "Epoch: 9/10...  Training Step: 893...  Training loss: 1.7428...  1.4230 sec/batch\n",
      "Epoch: 9/10...  Training Step: 894...  Training loss: 1.7310...  1.4373 sec/batch\n",
      "Epoch: 9/10...  Training Step: 895...  Training loss: 1.7510...  1.4249 sec/batch\n",
      "Epoch: 9/10...  Training Step: 896...  Training loss: 1.6971...  1.4244 sec/batch\n",
      "Epoch: 9/10...  Training Step: 897...  Training loss: 1.7221...  1.4416 sec/batch\n",
      "Epoch: 9/10...  Training Step: 898...  Training loss: 1.7183...  1.4453 sec/batch\n",
      "Epoch: 9/10...  Training Step: 899...  Training loss: 1.7245...  1.4745 sec/batch\n",
      "Epoch: 9/10...  Training Step: 900...  Training loss: 1.7235...  1.4956 sec/batch\n",
      "Epoch: 10/10...  Training Step: 901...  Training loss: 1.7964...  1.3750 sec/batch\n",
      "Epoch: 10/10...  Training Step: 902...  Training loss: 1.7275...  1.3737 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10...  Training Step: 903...  Training loss: 1.7252...  1.3767 sec/batch\n",
      "Epoch: 10/10...  Training Step: 904...  Training loss: 1.7108...  1.3787 sec/batch\n",
      "Epoch: 10/10...  Training Step: 905...  Training loss: 1.7298...  1.3757 sec/batch\n",
      "Epoch: 10/10...  Training Step: 906...  Training loss: 1.7445...  1.3767 sec/batch\n",
      "Epoch: 10/10...  Training Step: 907...  Training loss: 1.7722...  1.3792 sec/batch\n",
      "Epoch: 10/10...  Training Step: 908...  Training loss: 1.7185...  1.3779 sec/batch\n",
      "Epoch: 10/10...  Training Step: 909...  Training loss: 1.7265...  1.3789 sec/batch\n",
      "Epoch: 10/10...  Training Step: 910...  Training loss: 1.7066...  1.3885 sec/batch\n",
      "Epoch: 10/10...  Training Step: 911...  Training loss: 1.7281...  1.4036 sec/batch\n",
      "Epoch: 10/10...  Training Step: 912...  Training loss: 1.7142...  1.3916 sec/batch\n",
      "Epoch: 10/10...  Training Step: 913...  Training loss: 1.6915...  1.4294 sec/batch\n",
      "Epoch: 10/10...  Training Step: 914...  Training loss: 1.6749...  1.4149 sec/batch\n",
      "Epoch: 10/10...  Training Step: 915...  Training loss: 1.7149...  1.4433 sec/batch\n",
      "Epoch: 10/10...  Training Step: 916...  Training loss: 1.7271...  1.4107 sec/batch\n",
      "Epoch: 10/10...  Training Step: 917...  Training loss: 1.7021...  1.4387 sec/batch\n",
      "Epoch: 10/10...  Training Step: 918...  Training loss: 1.7166...  1.4188 sec/batch\n",
      "Epoch: 10/10...  Training Step: 919...  Training loss: 1.7219...  1.4173 sec/batch\n",
      "Epoch: 10/10...  Training Step: 920...  Training loss: 1.7080...  1.4270 sec/batch\n",
      "Epoch: 10/10...  Training Step: 921...  Training loss: 1.7064...  1.4270 sec/batch\n",
      "Epoch: 10/10...  Training Step: 922...  Training loss: 1.7111...  1.4404 sec/batch\n",
      "Epoch: 10/10...  Training Step: 923...  Training loss: 1.7658...  1.4169 sec/batch\n",
      "Epoch: 10/10...  Training Step: 924...  Training loss: 1.7160...  1.4341 sec/batch\n",
      "Epoch: 10/10...  Training Step: 925...  Training loss: 1.7023...  1.4166 sec/batch\n",
      "Epoch: 10/10...  Training Step: 926...  Training loss: 1.7342...  1.4174 sec/batch\n",
      "Epoch: 10/10...  Training Step: 927...  Training loss: 1.7114...  1.4431 sec/batch\n",
      "Epoch: 10/10...  Training Step: 928...  Training loss: 1.7317...  1.4666 sec/batch\n",
      "Epoch: 10/10...  Training Step: 929...  Training loss: 1.7317...  1.4011 sec/batch\n",
      "Epoch: 10/10...  Training Step: 930...  Training loss: 1.7429...  1.3883 sec/batch\n",
      "Epoch: 10/10...  Training Step: 931...  Training loss: 1.7155...  1.3914 sec/batch\n",
      "Epoch: 10/10...  Training Step: 932...  Training loss: 1.6953...  1.4254 sec/batch\n",
      "Epoch: 10/10...  Training Step: 933...  Training loss: 1.7046...  1.4063 sec/batch\n",
      "Epoch: 10/10...  Training Step: 934...  Training loss: 1.6812...  1.4180 sec/batch\n",
      "Epoch: 10/10...  Training Step: 935...  Training loss: 1.6577...  1.4053 sec/batch\n",
      "Epoch: 10/10...  Training Step: 936...  Training loss: 1.6590...  1.4203 sec/batch\n",
      "Epoch: 10/10...  Training Step: 937...  Training loss: 1.6915...  1.4050 sec/batch\n",
      "Epoch: 10/10...  Training Step: 938...  Training loss: 1.6640...  1.4284 sec/batch\n",
      "Epoch: 10/10...  Training Step: 939...  Training loss: 1.6988...  1.4472 sec/batch\n",
      "Epoch: 10/10...  Training Step: 940...  Training loss: 1.6745...  1.4320 sec/batch\n",
      "Epoch: 10/10...  Training Step: 941...  Training loss: 1.7433...  1.4218 sec/batch\n",
      "Epoch: 10/10...  Training Step: 942...  Training loss: 1.7132...  1.4003 sec/batch\n",
      "Epoch: 10/10...  Training Step: 943...  Training loss: 1.7119...  1.4168 sec/batch\n",
      "Epoch: 10/10...  Training Step: 944...  Training loss: 1.7209...  1.4137 sec/batch\n",
      "Epoch: 10/10...  Training Step: 945...  Training loss: 1.6935...  1.4195 sec/batch\n",
      "Epoch: 10/10...  Training Step: 946...  Training loss: 1.7357...  1.4169 sec/batch\n",
      "Epoch: 10/10...  Training Step: 947...  Training loss: 1.7397...  1.3944 sec/batch\n",
      "Epoch: 10/10...  Training Step: 948...  Training loss: 1.7099...  1.4222 sec/batch\n",
      "Epoch: 10/10...  Training Step: 949...  Training loss: 1.6906...  1.4102 sec/batch\n",
      "Epoch: 10/10...  Training Step: 950...  Training loss: 1.6869...  1.4159 sec/batch\n",
      "Epoch: 10/10...  Training Step: 951...  Training loss: 1.7155...  1.4179 sec/batch\n",
      "Epoch: 10/10...  Training Step: 952...  Training loss: 1.7009...  1.4160 sec/batch\n",
      "Epoch: 10/10...  Training Step: 953...  Training loss: 1.6682...  1.4285 sec/batch\n",
      "Epoch: 10/10...  Training Step: 954...  Training loss: 1.6889...  1.4260 sec/batch\n",
      "Epoch: 10/10...  Training Step: 955...  Training loss: 1.6819...  1.4203 sec/batch\n",
      "Epoch: 10/10...  Training Step: 956...  Training loss: 1.6905...  1.4216 sec/batch\n",
      "Epoch: 10/10...  Training Step: 957...  Training loss: 1.7273...  1.4418 sec/batch\n",
      "Epoch: 10/10...  Training Step: 958...  Training loss: 1.7230...  1.4163 sec/batch\n",
      "Epoch: 10/10...  Training Step: 959...  Training loss: 1.7142...  1.4224 sec/batch\n",
      "Epoch: 10/10...  Training Step: 960...  Training loss: 1.6682...  1.4347 sec/batch\n",
      "Epoch: 10/10...  Training Step: 961...  Training loss: 1.7248...  1.4512 sec/batch\n",
      "Epoch: 10/10...  Training Step: 962...  Training loss: 1.7045...  1.4411 sec/batch\n",
      "Epoch: 10/10...  Training Step: 963...  Training loss: 1.7115...  1.4479 sec/batch\n",
      "Epoch: 10/10...  Training Step: 964...  Training loss: 1.6893...  1.4244 sec/batch\n",
      "Epoch: 10/10...  Training Step: 965...  Training loss: 1.7024...  1.4291 sec/batch\n",
      "Epoch: 10/10...  Training Step: 966...  Training loss: 1.6922...  1.4461 sec/batch\n",
      "Epoch: 10/10...  Training Step: 967...  Training loss: 1.6811...  1.4308 sec/batch\n",
      "Epoch: 10/10...  Training Step: 968...  Training loss: 1.6901...  1.4167 sec/batch\n",
      "Epoch: 10/10...  Training Step: 969...  Training loss: 1.6835...  1.4317 sec/batch\n",
      "Epoch: 10/10...  Training Step: 970...  Training loss: 1.6875...  1.4097 sec/batch\n",
      "Epoch: 10/10...  Training Step: 971...  Training loss: 1.7108...  1.4087 sec/batch\n",
      "Epoch: 10/10...  Training Step: 972...  Training loss: 1.7261...  1.4263 sec/batch\n",
      "Epoch: 10/10...  Training Step: 973...  Training loss: 1.7008...  1.4426 sec/batch\n",
      "Epoch: 10/10...  Training Step: 974...  Training loss: 1.7173...  1.4440 sec/batch\n",
      "Epoch: 10/10...  Training Step: 975...  Training loss: 1.7078...  1.4378 sec/batch\n",
      "Epoch: 10/10...  Training Step: 976...  Training loss: 1.7182...  1.4420 sec/batch\n",
      "Epoch: 10/10...  Training Step: 977...  Training loss: 1.7052...  1.4516 sec/batch\n",
      "Epoch: 10/10...  Training Step: 978...  Training loss: 1.7001...  1.4611 sec/batch\n",
      "Epoch: 10/10...  Training Step: 979...  Training loss: 1.6298...  1.4756 sec/batch\n",
      "Epoch: 10/10...  Training Step: 980...  Training loss: 1.6741...  1.4400 sec/batch\n",
      "Epoch: 10/10...  Training Step: 981...  Training loss: 1.6835...  1.4418 sec/batch\n",
      "Epoch: 10/10...  Training Step: 982...  Training loss: 1.7088...  1.4366 sec/batch\n",
      "Epoch: 10/10...  Training Step: 983...  Training loss: 1.6865...  1.4305 sec/batch\n",
      "Epoch: 10/10...  Training Step: 984...  Training loss: 1.6705...  1.4252 sec/batch\n",
      "Epoch: 10/10...  Training Step: 985...  Training loss: 1.6642...  1.4344 sec/batch\n",
      "Epoch: 10/10...  Training Step: 986...  Training loss: 1.6949...  1.4839 sec/batch\n",
      "Epoch: 10/10...  Training Step: 987...  Training loss: 1.6564...  1.5040 sec/batch\n",
      "Epoch: 10/10...  Training Step: 988...  Training loss: 1.6761...  1.4630 sec/batch\n",
      "Epoch: 10/10...  Training Step: 989...  Training loss: 1.6595...  1.3962 sec/batch\n",
      "Epoch: 10/10...  Training Step: 990...  Training loss: 1.6715...  1.3909 sec/batch\n",
      "Epoch: 10/10...  Training Step: 991...  Training loss: 1.6602...  1.4011 sec/batch\n",
      "Epoch: 10/10...  Training Step: 992...  Training loss: 1.6631...  1.4117 sec/batch\n",
      "Epoch: 10/10...  Training Step: 993...  Training loss: 1.6861...  1.4159 sec/batch\n",
      "Epoch: 10/10...  Training Step: 994...  Training loss: 1.6640...  1.4173 sec/batch\n",
      "Epoch: 10/10...  Training Step: 995...  Training loss: 1.6895...  1.4451 sec/batch\n",
      "Epoch: 10/10...  Training Step: 996...  Training loss: 1.6256...  1.4208 sec/batch\n",
      "Epoch: 10/10...  Training Step: 997...  Training loss: 1.6641...  1.4384 sec/batch\n",
      "Epoch: 10/10...  Training Step: 998...  Training loss: 1.6490...  1.4195 sec/batch\n",
      "Epoch: 10/10...  Training Step: 999...  Training loss: 1.6644...  1.3764 sec/batch\n",
      "Epoch: 10/10...  Training Step: 1000...  Training loss: 1.6654...  1.3765 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bora treinar mais um pouquinho, que 1.66 é um custo ainda muito alto!\n",
    "\n",
    "Mas, agora, vamos iniciar o treinamento do último checkpoint, com o learning_rate menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i1000_l512.ckpt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20             # Number of full passes on all text\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.0005  # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "save_every_n = 100      # Save trainning progress every N iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i1000_l512.ckpt\n",
      "Epoch: 1/20...  Training Step: 1...  Training loss: 1.7380...  1.4652 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 1.6807...  1.3927 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 1.6765...  1.3901 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 1.6602...  1.4199 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 1.6738...  1.4193 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 1.6918...  1.4205 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 1.7110...  1.4125 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 1.6636...  1.4258 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 1.6714...  1.4293 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 1.6581...  1.4105 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 1.6695...  1.4280 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 1.6530...  1.4168 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 1.6267...  1.4391 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 1.6259...  1.4408 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 1.6583...  1.4390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 1.6557...  1.4413 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 1.6454...  1.4405 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 1.6557...  1.4184 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 1.6637...  1.4118 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 1.6499...  1.4401 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 1.6432...  1.4233 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 1.6452...  1.4423 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 1.7064...  1.4363 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 1.6495...  1.4664 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 1.6342...  1.4154 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 1.6832...  1.4229 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 1.6472...  1.4386 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 1.6714...  1.4413 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 1.6692...  1.4175 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 1.6887...  1.4103 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 1.6522...  1.4243 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 1.6474...  1.4403 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 1.6426...  1.4433 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 1.6235...  1.4276 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 1.6087...  1.4354 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 1.6110...  1.4474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 1.6311...  1.4366 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 1.6056...  1.4241 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 1.6522...  1.4273 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 1.6181...  1.4400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 1.6793...  1.4745 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 1.6548...  1.4336 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 1.6563...  1.4324 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 1.6631...  1.4380 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 1.6284...  1.4484 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 1.6903...  1.4478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 1.6807...  1.4314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 1.6497...  1.4475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 1.6374...  1.4540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 1.6329...  1.4316 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 1.6688...  1.4542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 1.6495...  1.4374 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 1.6242...  1.4409 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 1.6445...  1.4248 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 1.6294...  1.4231 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 1.6445...  1.4248 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 1.6805...  1.4403 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 1.6739...  1.4439 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 1.6635...  1.4081 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 1.6370...  1.4108 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 1.6834...  1.4173 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 1.6574...  1.4111 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 1.6579...  1.4110 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 1.6373...  1.4110 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 1.6479...  1.4116 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 1.6454...  1.4385 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 1.6348...  1.4314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 1.6249...  1.4619 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 1.6336...  1.4216 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 1.6469...  1.3985 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 1.6569...  1.4015 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 1.6694...  1.4000 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 1.6510...  1.4102 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 1.6495...  1.4008 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 1.6551...  1.4104 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 1.6579...  1.4333 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 1.6625...  1.4660 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 1.6494...  1.3916 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 1.5860...  1.3766 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 1.6247...  1.3871 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 1.6403...  1.3845 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 1.6592...  1.3891 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 1.6214...  1.4011 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 1.6263...  1.4024 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 1.6294...  1.3878 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 1.6390...  1.3946 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 1.5981...  1.3952 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 1.6353...  1.4228 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 1.6070...  1.3923 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 1.6170...  1.3902 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 1.6261...  1.4009 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 1.6158...  1.4026 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 1.6416...  1.4300 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 1.6227...  1.4103 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 1.6490...  1.4218 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 1.6028...  1.4025 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 1.6201...  1.3904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 1.6289...  1.3893 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 1.6210...  1.3882 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 1.6270...  1.3869 sec/batch\n",
      "Epoch: 2/20...  Training Step: 101...  Training loss: 1.7009...  1.3744 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 102...  Training loss: 1.6435...  1.4018 sec/batch\n",
      "Epoch: 2/20...  Training Step: 103...  Training loss: 1.6268...  1.3802 sec/batch\n",
      "Epoch: 2/20...  Training Step: 104...  Training loss: 1.6262...  1.3876 sec/batch\n",
      "Epoch: 2/20...  Training Step: 105...  Training loss: 1.6451...  1.4126 sec/batch\n",
      "Epoch: 2/20...  Training Step: 106...  Training loss: 1.6604...  1.3947 sec/batch\n",
      "Epoch: 2/20...  Training Step: 107...  Training loss: 1.6785...  1.4353 sec/batch\n",
      "Epoch: 2/20...  Training Step: 108...  Training loss: 1.6335...  1.4335 sec/batch\n",
      "Epoch: 2/20...  Training Step: 109...  Training loss: 1.6377...  1.4124 sec/batch\n",
      "Epoch: 2/20...  Training Step: 110...  Training loss: 1.6259...  1.4489 sec/batch\n",
      "Epoch: 2/20...  Training Step: 111...  Training loss: 1.6339...  1.4369 sec/batch\n",
      "Epoch: 2/20...  Training Step: 112...  Training loss: 1.6233...  1.4199 sec/batch\n",
      "Epoch: 2/20...  Training Step: 113...  Training loss: 1.5885...  1.4164 sec/batch\n",
      "Epoch: 2/20...  Training Step: 114...  Training loss: 1.5807...  1.4324 sec/batch\n",
      "Epoch: 2/20...  Training Step: 115...  Training loss: 1.6297...  1.4342 sec/batch\n",
      "Epoch: 2/20...  Training Step: 116...  Training loss: 1.6423...  1.4649 sec/batch\n",
      "Epoch: 2/20...  Training Step: 117...  Training loss: 1.6125...  1.4109 sec/batch\n",
      "Epoch: 2/20...  Training Step: 118...  Training loss: 1.6202...  1.3947 sec/batch\n",
      "Epoch: 2/20...  Training Step: 119...  Training loss: 1.6290...  1.3944 sec/batch\n",
      "Epoch: 2/20...  Training Step: 120...  Training loss: 1.6153...  1.3941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 121...  Training loss: 1.6260...  1.4116 sec/batch\n",
      "Epoch: 2/20...  Training Step: 122...  Training loss: 1.6249...  1.4437 sec/batch\n",
      "Epoch: 2/20...  Training Step: 123...  Training loss: 1.6826...  1.4029 sec/batch\n",
      "Epoch: 2/20...  Training Step: 124...  Training loss: 1.6243...  1.4673 sec/batch\n",
      "Epoch: 2/20...  Training Step: 125...  Training loss: 1.6137...  1.3856 sec/batch\n",
      "Epoch: 2/20...  Training Step: 126...  Training loss: 1.6548...  1.4041 sec/batch\n",
      "Epoch: 2/20...  Training Step: 127...  Training loss: 1.6220...  1.3969 sec/batch\n",
      "Epoch: 2/20...  Training Step: 128...  Training loss: 1.6482...  1.4049 sec/batch\n",
      "Epoch: 2/20...  Training Step: 129...  Training loss: 1.6407...  1.4162 sec/batch\n",
      "Epoch: 2/20...  Training Step: 130...  Training loss: 1.6633...  1.4259 sec/batch\n",
      "Epoch: 2/20...  Training Step: 131...  Training loss: 1.6298...  1.3711 sec/batch\n",
      "Epoch: 2/20...  Training Step: 132...  Training loss: 1.6126...  1.3737 sec/batch\n",
      "Epoch: 2/20...  Training Step: 133...  Training loss: 1.6213...  1.3784 sec/batch\n",
      "Epoch: 2/20...  Training Step: 134...  Training loss: 1.5963...  1.3902 sec/batch\n",
      "Epoch: 2/20...  Training Step: 135...  Training loss: 1.5891...  1.3902 sec/batch\n",
      "Epoch: 2/20...  Training Step: 136...  Training loss: 1.5796...  1.3929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 137...  Training loss: 1.6123...  1.3941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 138...  Training loss: 1.5882...  1.4075 sec/batch\n",
      "Epoch: 2/20...  Training Step: 139...  Training loss: 1.6168...  1.4303 sec/batch\n",
      "Epoch: 2/20...  Training Step: 140...  Training loss: 1.5919...  1.4080 sec/batch\n",
      "Epoch: 2/20...  Training Step: 141...  Training loss: 1.6478...  1.4008 sec/batch\n",
      "Epoch: 2/20...  Training Step: 142...  Training loss: 1.6166...  1.4110 sec/batch\n",
      "Epoch: 2/20...  Training Step: 143...  Training loss: 1.6380...  1.3970 sec/batch\n",
      "Epoch: 2/20...  Training Step: 144...  Training loss: 1.6389...  1.3933 sec/batch\n",
      "Epoch: 2/20...  Training Step: 145...  Training loss: 1.6136...  1.4033 sec/batch\n",
      "Epoch: 2/20...  Training Step: 146...  Training loss: 1.6642...  1.4069 sec/batch\n",
      "Epoch: 2/20...  Training Step: 147...  Training loss: 1.6533...  1.4086 sec/batch\n",
      "Epoch: 2/20...  Training Step: 148...  Training loss: 1.6211...  1.4006 sec/batch\n",
      "Epoch: 2/20...  Training Step: 149...  Training loss: 1.6086...  1.4037 sec/batch\n",
      "Epoch: 2/20...  Training Step: 150...  Training loss: 1.5973...  1.3935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 151...  Training loss: 1.6403...  1.4219 sec/batch\n",
      "Epoch: 2/20...  Training Step: 152...  Training loss: 1.6201...  1.4223 sec/batch\n",
      "Epoch: 2/20...  Training Step: 153...  Training loss: 1.5916...  1.4364 sec/batch\n",
      "Epoch: 2/20...  Training Step: 154...  Training loss: 1.6085...  1.4062 sec/batch\n",
      "Epoch: 2/20...  Training Step: 155...  Training loss: 1.5976...  1.4064 sec/batch\n",
      "Epoch: 2/20...  Training Step: 156...  Training loss: 1.6192...  1.4361 sec/batch\n",
      "Epoch: 2/20...  Training Step: 157...  Training loss: 1.6536...  1.4212 sec/batch\n",
      "Epoch: 2/20...  Training Step: 158...  Training loss: 1.6383...  1.4124 sec/batch\n",
      "Epoch: 2/20...  Training Step: 159...  Training loss: 1.6368...  1.4061 sec/batch\n",
      "Epoch: 2/20...  Training Step: 160...  Training loss: 1.5948...  1.4120 sec/batch\n",
      "Epoch: 2/20...  Training Step: 161...  Training loss: 1.6520...  1.4151 sec/batch\n",
      "Epoch: 2/20...  Training Step: 162...  Training loss: 1.6281...  1.4206 sec/batch\n",
      "Epoch: 2/20...  Training Step: 163...  Training loss: 1.6346...  1.4349 sec/batch\n",
      "Epoch: 2/20...  Training Step: 164...  Training loss: 1.6197...  1.4111 sec/batch\n",
      "Epoch: 2/20...  Training Step: 165...  Training loss: 1.6276...  1.4013 sec/batch\n",
      "Epoch: 2/20...  Training Step: 166...  Training loss: 1.6154...  1.4042 sec/batch\n",
      "Epoch: 2/20...  Training Step: 167...  Training loss: 1.6106...  1.3899 sec/batch\n",
      "Epoch: 2/20...  Training Step: 168...  Training loss: 1.6015...  1.4389 sec/batch\n",
      "Epoch: 2/20...  Training Step: 169...  Training loss: 1.6114...  1.4392 sec/batch\n",
      "Epoch: 2/20...  Training Step: 170...  Training loss: 1.6274...  1.3888 sec/batch\n",
      "Epoch: 2/20...  Training Step: 171...  Training loss: 1.6343...  1.3768 sec/batch\n",
      "Epoch: 2/20...  Training Step: 172...  Training loss: 1.6503...  1.3773 sec/batch\n",
      "Epoch: 2/20...  Training Step: 173...  Training loss: 1.6263...  1.3886 sec/batch\n",
      "Epoch: 2/20...  Training Step: 174...  Training loss: 1.6309...  1.3924 sec/batch\n",
      "Epoch: 2/20...  Training Step: 175...  Training loss: 1.6369...  1.3905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 176...  Training loss: 1.6440...  1.3846 sec/batch\n",
      "Epoch: 2/20...  Training Step: 177...  Training loss: 1.6435...  1.4072 sec/batch\n",
      "Epoch: 2/20...  Training Step: 178...  Training loss: 1.6318...  1.3938 sec/batch\n",
      "Epoch: 2/20...  Training Step: 179...  Training loss: 1.5648...  1.3983 sec/batch\n",
      "Epoch: 2/20...  Training Step: 180...  Training loss: 1.6000...  1.4186 sec/batch\n",
      "Epoch: 2/20...  Training Step: 181...  Training loss: 1.6077...  1.4194 sec/batch\n",
      "Epoch: 2/20...  Training Step: 182...  Training loss: 1.6342...  1.4341 sec/batch\n",
      "Epoch: 2/20...  Training Step: 183...  Training loss: 1.5972...  1.4480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 184...  Training loss: 1.5995...  1.4669 sec/batch\n",
      "Epoch: 2/20...  Training Step: 185...  Training loss: 1.5987...  1.3901 sec/batch\n",
      "Epoch: 2/20...  Training Step: 186...  Training loss: 1.6155...  1.3942 sec/batch\n",
      "Epoch: 2/20...  Training Step: 187...  Training loss: 1.5898...  1.3924 sec/batch\n",
      "Epoch: 2/20...  Training Step: 188...  Training loss: 1.6193...  1.3884 sec/batch\n",
      "Epoch: 2/20...  Training Step: 189...  Training loss: 1.5871...  1.4045 sec/batch\n",
      "Epoch: 2/20...  Training Step: 190...  Training loss: 1.5971...  1.4075 sec/batch\n",
      "Epoch: 2/20...  Training Step: 191...  Training loss: 1.5920...  1.4014 sec/batch\n",
      "Epoch: 2/20...  Training Step: 192...  Training loss: 1.5939...  1.4245 sec/batch\n",
      "Epoch: 2/20...  Training Step: 193...  Training loss: 1.6244...  1.4187 sec/batch\n",
      "Epoch: 2/20...  Training Step: 194...  Training loss: 1.5969...  1.4359 sec/batch\n",
      "Epoch: 2/20...  Training Step: 195...  Training loss: 1.6269...  1.4460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 196...  Training loss: 1.5763...  1.4182 sec/batch\n",
      "Epoch: 2/20...  Training Step: 197...  Training loss: 1.5985...  1.4046 sec/batch\n",
      "Epoch: 2/20...  Training Step: 198...  Training loss: 1.5888...  1.4008 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 1.6020...  1.4155 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 1.6048...  1.4255 sec/batch\n",
      "Epoch: 3/20...  Training Step: 201...  Training loss: 1.6696...  1.3762 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 202...  Training loss: 1.6186...  1.3800 sec/batch\n",
      "Epoch: 3/20...  Training Step: 203...  Training loss: 1.6205...  1.3898 sec/batch\n",
      "Epoch: 3/20...  Training Step: 204...  Training loss: 1.5882...  1.3962 sec/batch\n",
      "Epoch: 3/20...  Training Step: 205...  Training loss: 1.6235...  1.4008 sec/batch\n",
      "Epoch: 3/20...  Training Step: 206...  Training loss: 1.6402...  1.4083 sec/batch\n",
      "Epoch: 3/20...  Training Step: 207...  Training loss: 1.6593...  1.4065 sec/batch\n",
      "Epoch: 3/20...  Training Step: 208...  Training loss: 1.6086...  1.4209 sec/batch\n",
      "Epoch: 3/20...  Training Step: 209...  Training loss: 1.6172...  1.4401 sec/batch\n",
      "Epoch: 3/20...  Training Step: 210...  Training loss: 1.5933...  1.4060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 211...  Training loss: 1.6187...  1.4279 sec/batch\n",
      "Epoch: 3/20...  Training Step: 212...  Training loss: 1.6025...  1.4300 sec/batch\n",
      "Epoch: 3/20...  Training Step: 213...  Training loss: 1.5843...  1.4308 sec/batch\n",
      "Epoch: 3/20...  Training Step: 214...  Training loss: 1.5566...  1.4287 sec/batch\n",
      "Epoch: 3/20...  Training Step: 215...  Training loss: 1.5958...  1.4106 sec/batch\n",
      "Epoch: 3/20...  Training Step: 216...  Training loss: 1.6149...  1.4081 sec/batch\n",
      "Epoch: 3/20...  Training Step: 217...  Training loss: 1.5964...  1.4112 sec/batch\n",
      "Epoch: 3/20...  Training Step: 218...  Training loss: 1.5995...  1.4280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 219...  Training loss: 1.6037...  1.4311 sec/batch\n",
      "Epoch: 3/20...  Training Step: 220...  Training loss: 1.5916...  1.4267 sec/batch\n",
      "Epoch: 3/20...  Training Step: 221...  Training loss: 1.5961...  1.4150 sec/batch\n",
      "Epoch: 3/20...  Training Step: 222...  Training loss: 1.6045...  1.4026 sec/batch\n",
      "Epoch: 3/20...  Training Step: 223...  Training loss: 1.6509...  1.4213 sec/batch\n",
      "Epoch: 3/20...  Training Step: 224...  Training loss: 1.5871...  1.4383 sec/batch\n",
      "Epoch: 3/20...  Training Step: 225...  Training loss: 1.5833...  1.4056 sec/batch\n",
      "Epoch: 3/20...  Training Step: 226...  Training loss: 1.6351...  1.4017 sec/batch\n",
      "Epoch: 3/20...  Training Step: 227...  Training loss: 1.5940...  1.3862 sec/batch\n",
      "Epoch: 3/20...  Training Step: 228...  Training loss: 1.6279...  1.3877 sec/batch\n",
      "Epoch: 3/20...  Training Step: 229...  Training loss: 1.6129...  1.3877 sec/batch\n",
      "Epoch: 3/20...  Training Step: 230...  Training loss: 1.6382...  1.3885 sec/batch\n",
      "Epoch: 3/20...  Training Step: 231...  Training loss: 1.6079...  1.3900 sec/batch\n",
      "Epoch: 3/20...  Training Step: 232...  Training loss: 1.5870...  1.3944 sec/batch\n",
      "Epoch: 3/20...  Training Step: 233...  Training loss: 1.5849...  1.3990 sec/batch\n",
      "Epoch: 3/20...  Training Step: 234...  Training loss: 1.5750...  1.4188 sec/batch\n",
      "Epoch: 3/20...  Training Step: 235...  Training loss: 1.5485...  1.4213 sec/batch\n",
      "Epoch: 3/20...  Training Step: 236...  Training loss: 1.5552...  1.4125 sec/batch\n",
      "Epoch: 3/20...  Training Step: 237...  Training loss: 1.5901...  1.4390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 238...  Training loss: 1.5598...  1.4545 sec/batch\n",
      "Epoch: 3/20...  Training Step: 239...  Training loss: 1.5951...  1.4327 sec/batch\n",
      "Epoch: 3/20...  Training Step: 240...  Training loss: 1.5708...  1.4336 sec/batch\n",
      "Epoch: 3/20...  Training Step: 241...  Training loss: 1.6235...  1.4347 sec/batch\n",
      "Epoch: 3/20...  Training Step: 242...  Training loss: 1.6055...  1.4396 sec/batch\n",
      "Epoch: 3/20...  Training Step: 243...  Training loss: 1.6030...  1.4379 sec/batch\n",
      "Epoch: 3/20...  Training Step: 244...  Training loss: 1.6131...  1.4392 sec/batch\n",
      "Epoch: 3/20...  Training Step: 245...  Training loss: 1.5860...  1.4740 sec/batch\n",
      "Epoch: 3/20...  Training Step: 246...  Training loss: 1.6307...  1.4226 sec/batch\n",
      "Epoch: 3/20...  Training Step: 247...  Training loss: 1.6389...  1.4119 sec/batch\n",
      "Epoch: 3/20...  Training Step: 248...  Training loss: 1.6043...  1.4145 sec/batch\n",
      "Epoch: 3/20...  Training Step: 249...  Training loss: 1.5868...  1.4431 sec/batch\n",
      "Epoch: 3/20...  Training Step: 250...  Training loss: 1.5834...  1.4945 sec/batch\n",
      "Epoch: 3/20...  Training Step: 251...  Training loss: 1.6161...  1.3964 sec/batch\n",
      "Epoch: 3/20...  Training Step: 252...  Training loss: 1.6050...  1.3950 sec/batch\n",
      "Epoch: 3/20...  Training Step: 253...  Training loss: 1.5612...  1.4269 sec/batch\n",
      "Epoch: 3/20...  Training Step: 254...  Training loss: 1.5898...  1.4135 sec/batch\n",
      "Epoch: 3/20...  Training Step: 255...  Training loss: 1.5793...  1.4077 sec/batch\n",
      "Epoch: 3/20...  Training Step: 256...  Training loss: 1.5894...  1.4230 sec/batch\n",
      "Epoch: 3/20...  Training Step: 257...  Training loss: 1.6226...  1.4454 sec/batch\n",
      "Epoch: 3/20...  Training Step: 258...  Training loss: 1.6299...  1.4253 sec/batch\n",
      "Epoch: 3/20...  Training Step: 259...  Training loss: 1.6199...  1.4223 sec/batch\n",
      "Epoch: 3/20...  Training Step: 260...  Training loss: 1.5880...  1.4482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 261...  Training loss: 1.6277...  1.4814 sec/batch\n",
      "Epoch: 3/20...  Training Step: 262...  Training loss: 1.6054...  1.4311 sec/batch\n",
      "Epoch: 3/20...  Training Step: 263...  Training loss: 1.6125...  1.4373 sec/batch\n",
      "Epoch: 3/20...  Training Step: 264...  Training loss: 1.5878...  1.4281 sec/batch\n",
      "Epoch: 3/20...  Training Step: 265...  Training loss: 1.6013...  1.3980 sec/batch\n",
      "Epoch: 3/20...  Training Step: 266...  Training loss: 1.5930...  1.4102 sec/batch\n",
      "Epoch: 3/20...  Training Step: 267...  Training loss: 1.5831...  1.4067 sec/batch\n",
      "Epoch: 3/20...  Training Step: 268...  Training loss: 1.5777...  1.4143 sec/batch\n",
      "Epoch: 3/20...  Training Step: 269...  Training loss: 1.5930...  1.4186 sec/batch\n",
      "Epoch: 3/20...  Training Step: 270...  Training loss: 1.5930...  1.4092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 271...  Training loss: 1.6132...  1.4091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 272...  Training loss: 1.6264...  1.4095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 273...  Training loss: 1.6011...  1.4316 sec/batch\n",
      "Epoch: 3/20...  Training Step: 274...  Training loss: 1.6141...  1.4275 sec/batch\n",
      "Epoch: 3/20...  Training Step: 275...  Training loss: 1.6191...  1.4242 sec/batch\n",
      "Epoch: 3/20...  Training Step: 276...  Training loss: 1.6115...  1.4320 sec/batch\n",
      "Epoch: 3/20...  Training Step: 277...  Training loss: 1.6134...  1.3899 sec/batch\n",
      "Epoch: 3/20...  Training Step: 278...  Training loss: 1.6039...  1.3975 sec/batch\n",
      "Epoch: 3/20...  Training Step: 279...  Training loss: 1.5363...  1.4341 sec/batch\n",
      "Epoch: 3/20...  Training Step: 280...  Training loss: 1.5786...  1.3980 sec/batch\n",
      "Epoch: 3/20...  Training Step: 281...  Training loss: 1.5842...  1.4130 sec/batch\n",
      "Epoch: 3/20...  Training Step: 282...  Training loss: 1.6171...  1.4091 sec/batch\n",
      "Epoch: 3/20...  Training Step: 283...  Training loss: 1.5876...  1.3978 sec/batch\n",
      "Epoch: 3/20...  Training Step: 284...  Training loss: 1.5787...  1.4151 sec/batch\n",
      "Epoch: 3/20...  Training Step: 285...  Training loss: 1.5769...  1.4152 sec/batch\n",
      "Epoch: 3/20...  Training Step: 286...  Training loss: 1.5937...  1.4071 sec/batch\n",
      "Epoch: 3/20...  Training Step: 287...  Training loss: 1.5647...  1.4318 sec/batch\n",
      "Epoch: 3/20...  Training Step: 288...  Training loss: 1.5872...  1.4357 sec/batch\n",
      "Epoch: 3/20...  Training Step: 289...  Training loss: 1.5619...  1.4708 sec/batch\n",
      "Epoch: 3/20...  Training Step: 290...  Training loss: 1.5718...  1.4516 sec/batch\n",
      "Epoch: 3/20...  Training Step: 291...  Training loss: 1.5698...  1.4217 sec/batch\n",
      "Epoch: 3/20...  Training Step: 292...  Training loss: 1.5696...  1.3961 sec/batch\n",
      "Epoch: 3/20...  Training Step: 293...  Training loss: 1.5921...  1.4163 sec/batch\n",
      "Epoch: 3/20...  Training Step: 294...  Training loss: 1.5715...  1.4246 sec/batch\n",
      "Epoch: 3/20...  Training Step: 295...  Training loss: 1.6089...  1.4244 sec/batch\n",
      "Epoch: 3/20...  Training Step: 296...  Training loss: 1.5437...  1.4210 sec/batch\n",
      "Epoch: 3/20...  Training Step: 297...  Training loss: 1.5734...  1.4047 sec/batch\n",
      "Epoch: 3/20...  Training Step: 298...  Training loss: 1.5657...  1.4228 sec/batch\n",
      "Epoch: 3/20...  Training Step: 299...  Training loss: 1.5709...  1.4579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 300...  Training loss: 1.5818...  1.4384 sec/batch\n",
      "Epoch: 4/20...  Training Step: 301...  Training loss: 1.6503...  1.3779 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 302...  Training loss: 1.5942...  1.3890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 303...  Training loss: 1.5889...  1.3860 sec/batch\n",
      "Epoch: 4/20...  Training Step: 304...  Training loss: 1.5712...  1.4182 sec/batch\n",
      "Epoch: 4/20...  Training Step: 305...  Training loss: 1.6041...  1.4100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 306...  Training loss: 1.6145...  1.4004 sec/batch\n",
      "Epoch: 4/20...  Training Step: 307...  Training loss: 1.6322...  1.4283 sec/batch\n",
      "Epoch: 4/20...  Training Step: 308...  Training loss: 1.5796...  1.4119 sec/batch\n",
      "Epoch: 4/20...  Training Step: 309...  Training loss: 1.6068...  1.4179 sec/batch\n",
      "Epoch: 4/20...  Training Step: 310...  Training loss: 1.5787...  1.4104 sec/batch\n",
      "Epoch: 4/20...  Training Step: 311...  Training loss: 1.5833...  1.4292 sec/batch\n",
      "Epoch: 4/20...  Training Step: 312...  Training loss: 1.5853...  1.4314 sec/batch\n",
      "Epoch: 4/20...  Training Step: 313...  Training loss: 1.5414...  1.4139 sec/batch\n",
      "Epoch: 4/20...  Training Step: 314...  Training loss: 1.5398...  1.4067 sec/batch\n",
      "Epoch: 4/20...  Training Step: 315...  Training loss: 1.5799...  1.4201 sec/batch\n",
      "Epoch: 4/20...  Training Step: 316...  Training loss: 1.5847...  1.4282 sec/batch\n",
      "Epoch: 4/20...  Training Step: 317...  Training loss: 1.5804...  1.4295 sec/batch\n",
      "Epoch: 4/20...  Training Step: 318...  Training loss: 1.5796...  1.4141 sec/batch\n",
      "Epoch: 4/20...  Training Step: 319...  Training loss: 1.5935...  1.4037 sec/batch\n",
      "Epoch: 4/20...  Training Step: 320...  Training loss: 1.5674...  1.4497 sec/batch\n",
      "Epoch: 4/20...  Training Step: 321...  Training loss: 1.5684...  1.4275 sec/batch\n",
      "Epoch: 4/20...  Training Step: 322...  Training loss: 1.5790...  1.3991 sec/batch\n",
      "Epoch: 4/20...  Training Step: 323...  Training loss: 1.6255...  1.4009 sec/batch\n",
      "Epoch: 4/20...  Training Step: 324...  Training loss: 1.5712...  1.4090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 325...  Training loss: 1.5571...  1.4278 sec/batch\n",
      "Epoch: 4/20...  Training Step: 326...  Training loss: 1.6074...  1.3999 sec/batch\n",
      "Epoch: 4/20...  Training Step: 327...  Training loss: 1.5790...  1.3891 sec/batch\n",
      "Epoch: 4/20...  Training Step: 328...  Training loss: 1.6057...  1.3945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 329...  Training loss: 1.5980...  1.4058 sec/batch\n",
      "Epoch: 4/20...  Training Step: 330...  Training loss: 1.6068...  1.4123 sec/batch\n",
      "Epoch: 4/20...  Training Step: 331...  Training loss: 1.5782...  1.4043 sec/batch\n",
      "Epoch: 4/20...  Training Step: 332...  Training loss: 1.5702...  1.4286 sec/batch\n",
      "Epoch: 4/20...  Training Step: 333...  Training loss: 1.5627...  1.4175 sec/batch\n",
      "Epoch: 4/20...  Training Step: 334...  Training loss: 1.5564...  1.4104 sec/batch\n",
      "Epoch: 4/20...  Training Step: 335...  Training loss: 1.5329...  1.4266 sec/batch\n",
      "Epoch: 4/20...  Training Step: 336...  Training loss: 1.5303...  1.4299 sec/batch\n",
      "Epoch: 4/20...  Training Step: 337...  Training loss: 1.5677...  1.4320 sec/batch\n",
      "Epoch: 4/20...  Training Step: 338...  Training loss: 1.5379...  1.4523 sec/batch\n",
      "Epoch: 4/20...  Training Step: 339...  Training loss: 1.5654...  1.4371 sec/batch\n",
      "Epoch: 4/20...  Training Step: 340...  Training loss: 1.5437...  1.4468 sec/batch\n",
      "Epoch: 4/20...  Training Step: 341...  Training loss: 1.6095...  1.4347 sec/batch\n",
      "Epoch: 4/20...  Training Step: 342...  Training loss: 1.5839...  1.5098 sec/batch\n",
      "Epoch: 4/20...  Training Step: 343...  Training loss: 1.5784...  1.3972 sec/batch\n",
      "Epoch: 4/20...  Training Step: 344...  Training loss: 1.5870...  1.3977 sec/batch\n",
      "Epoch: 4/20...  Training Step: 345...  Training loss: 1.5746...  1.4136 sec/batch\n",
      "Epoch: 4/20...  Training Step: 346...  Training loss: 1.6200...  1.4153 sec/batch\n",
      "Epoch: 4/20...  Training Step: 347...  Training loss: 1.6094...  1.4093 sec/batch\n",
      "Epoch: 4/20...  Training Step: 348...  Training loss: 1.5779...  1.4133 sec/batch\n",
      "Epoch: 4/20...  Training Step: 349...  Training loss: 1.5711...  1.4039 sec/batch\n",
      "Epoch: 4/20...  Training Step: 350...  Training loss: 1.5586...  1.4172 sec/batch\n",
      "Epoch: 4/20...  Training Step: 351...  Training loss: 1.6000...  1.3739 sec/batch\n",
      "Epoch: 4/20...  Training Step: 352...  Training loss: 1.5731...  1.3785 sec/batch\n",
      "Epoch: 4/20...  Training Step: 353...  Training loss: 1.5413...  1.3765 sec/batch\n",
      "Epoch: 4/20...  Training Step: 354...  Training loss: 1.5670...  1.3850 sec/batch\n",
      "Epoch: 4/20...  Training Step: 355...  Training loss: 1.5536...  1.3744 sec/batch\n",
      "Epoch: 4/20...  Training Step: 356...  Training loss: 1.5742...  1.3800 sec/batch\n",
      "Epoch: 4/20...  Training Step: 357...  Training loss: 1.6039...  1.3798 sec/batch\n",
      "Epoch: 4/20...  Training Step: 358...  Training loss: 1.6066...  1.3773 sec/batch\n",
      "Epoch: 4/20...  Training Step: 359...  Training loss: 1.5896...  1.3984 sec/batch\n",
      "Epoch: 4/20...  Training Step: 360...  Training loss: 1.5608...  1.3784 sec/batch\n",
      "Epoch: 4/20...  Training Step: 361...  Training loss: 1.6055...  1.3936 sec/batch\n",
      "Epoch: 4/20...  Training Step: 362...  Training loss: 1.5889...  1.4293 sec/batch\n",
      "Epoch: 4/20...  Training Step: 363...  Training loss: 1.5923...  1.3862 sec/batch\n",
      "Epoch: 4/20...  Training Step: 364...  Training loss: 1.5650...  1.4203 sec/batch\n",
      "Epoch: 4/20...  Training Step: 365...  Training loss: 1.5684...  1.3757 sec/batch\n",
      "Epoch: 4/20...  Training Step: 366...  Training loss: 1.5649...  1.3767 sec/batch\n",
      "Epoch: 4/20...  Training Step: 367...  Training loss: 1.5676...  1.3759 sec/batch\n",
      "Epoch: 4/20...  Training Step: 368...  Training loss: 1.5633...  1.3748 sec/batch\n",
      "Epoch: 4/20...  Training Step: 369...  Training loss: 1.5675...  1.3957 sec/batch\n",
      "Epoch: 4/20...  Training Step: 370...  Training loss: 1.5828...  1.3897 sec/batch\n",
      "Epoch: 4/20...  Training Step: 371...  Training loss: 1.5929...  1.3842 sec/batch\n",
      "Epoch: 4/20...  Training Step: 372...  Training loss: 1.6044...  1.3772 sec/batch\n",
      "Epoch: 4/20...  Training Step: 373...  Training loss: 1.5788...  1.3797 sec/batch\n",
      "Epoch: 4/20...  Training Step: 374...  Training loss: 1.5876...  1.3752 sec/batch\n",
      "Epoch: 4/20...  Training Step: 375...  Training loss: 1.5926...  1.3742 sec/batch\n",
      "Epoch: 4/20...  Training Step: 376...  Training loss: 1.5845...  1.3757 sec/batch\n",
      "Epoch: 4/20...  Training Step: 377...  Training loss: 1.5996...  1.3742 sec/batch\n",
      "Epoch: 4/20...  Training Step: 378...  Training loss: 1.5804...  1.3792 sec/batch\n",
      "Epoch: 4/20...  Training Step: 379...  Training loss: 1.5143...  1.3747 sec/batch\n",
      "Epoch: 4/20...  Training Step: 380...  Training loss: 1.5609...  1.3907 sec/batch\n",
      "Epoch: 4/20...  Training Step: 381...  Training loss: 1.5628...  1.3782 sec/batch\n",
      "Epoch: 4/20...  Training Step: 382...  Training loss: 1.5915...  1.3812 sec/batch\n",
      "Epoch: 4/20...  Training Step: 383...  Training loss: 1.5607...  1.3732 sec/batch\n",
      "Epoch: 4/20...  Training Step: 384...  Training loss: 1.5606...  1.3762 sec/batch\n",
      "Epoch: 4/20...  Training Step: 385...  Training loss: 1.5595...  1.3721 sec/batch\n",
      "Epoch: 4/20...  Training Step: 386...  Training loss: 1.5729...  1.3721 sec/batch\n",
      "Epoch: 4/20...  Training Step: 387...  Training loss: 1.5481...  1.3721 sec/batch\n",
      "Epoch: 4/20...  Training Step: 388...  Training loss: 1.5669...  1.3742 sec/batch\n",
      "Epoch: 4/20...  Training Step: 389...  Training loss: 1.5448...  1.3727 sec/batch\n",
      "Epoch: 4/20...  Training Step: 390...  Training loss: 1.5578...  1.3716 sec/batch\n",
      "Epoch: 4/20...  Training Step: 391...  Training loss: 1.5532...  1.3767 sec/batch\n",
      "Epoch: 4/20...  Training Step: 392...  Training loss: 1.5496...  1.3732 sec/batch\n",
      "Epoch: 4/20...  Training Step: 393...  Training loss: 1.5807...  1.3726 sec/batch\n",
      "Epoch: 4/20...  Training Step: 394...  Training loss: 1.5525...  1.3752 sec/batch\n",
      "Epoch: 4/20...  Training Step: 395...  Training loss: 1.5811...  1.3732 sec/batch\n",
      "Epoch: 4/20...  Training Step: 396...  Training loss: 1.5384...  1.3752 sec/batch\n",
      "Epoch: 4/20...  Training Step: 397...  Training loss: 1.5539...  1.3701 sec/batch\n",
      "Epoch: 4/20...  Training Step: 398...  Training loss: 1.5429...  1.3731 sec/batch\n",
      "Epoch: 4/20...  Training Step: 399...  Training loss: 1.5524...  1.3737 sec/batch\n",
      "Epoch: 4/20...  Training Step: 400...  Training loss: 1.5653...  1.3732 sec/batch\n",
      "Epoch: 5/20...  Training Step: 401...  Training loss: 1.6252...  1.3711 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 402...  Training loss: 1.5787...  1.3742 sec/batch\n",
      "Epoch: 5/20...  Training Step: 403...  Training loss: 1.5688...  1.3747 sec/batch\n",
      "Epoch: 5/20...  Training Step: 404...  Training loss: 1.5570...  1.3727 sec/batch\n",
      "Epoch: 5/20...  Training Step: 405...  Training loss: 1.5897...  1.3742 sec/batch\n",
      "Epoch: 5/20...  Training Step: 406...  Training loss: 1.5849...  1.3727 sec/batch\n",
      "Epoch: 5/20...  Training Step: 407...  Training loss: 1.6119...  1.3731 sec/batch\n",
      "Epoch: 5/20...  Training Step: 408...  Training loss: 1.5676...  1.3726 sec/batch\n",
      "Epoch: 5/20...  Training Step: 409...  Training loss: 1.5775...  1.3737 sec/batch\n",
      "Epoch: 5/20...  Training Step: 410...  Training loss: 1.5530...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 411...  Training loss: 1.5719...  1.3706 sec/batch\n",
      "Epoch: 5/20...  Training Step: 412...  Training loss: 1.5585...  1.3721 sec/batch\n",
      "Epoch: 5/20...  Training Step: 413...  Training loss: 1.5361...  1.3711 sec/batch\n",
      "Epoch: 5/20...  Training Step: 414...  Training loss: 1.5200...  1.3747 sec/batch\n",
      "Epoch: 5/20...  Training Step: 415...  Training loss: 1.5647...  1.3732 sec/batch\n",
      "Epoch: 5/20...  Training Step: 416...  Training loss: 1.5580...  1.3706 sec/batch\n",
      "Epoch: 5/20...  Training Step: 417...  Training loss: 1.5516...  1.3732 sec/batch\n",
      "Epoch: 5/20...  Training Step: 418...  Training loss: 1.5481...  1.3726 sec/batch\n",
      "Epoch: 5/20...  Training Step: 419...  Training loss: 1.5611...  1.3711 sec/batch\n",
      "Epoch: 5/20...  Training Step: 420...  Training loss: 1.5525...  1.3726 sec/batch\n",
      "Epoch: 5/20...  Training Step: 421...  Training loss: 1.5559...  1.3727 sec/batch\n",
      "Epoch: 5/20...  Training Step: 422...  Training loss: 1.5599...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 423...  Training loss: 1.6110...  1.3721 sec/batch\n",
      "Epoch: 5/20...  Training Step: 424...  Training loss: 1.5531...  1.3726 sec/batch\n",
      "Epoch: 5/20...  Training Step: 425...  Training loss: 1.5425...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 426...  Training loss: 1.5836...  1.3721 sec/batch\n",
      "Epoch: 5/20...  Training Step: 427...  Training loss: 1.5575...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 428...  Training loss: 1.5793...  1.3721 sec/batch\n",
      "Epoch: 5/20...  Training Step: 429...  Training loss: 1.5741...  1.3721 sec/batch\n",
      "Epoch: 5/20...  Training Step: 430...  Training loss: 1.6009...  1.3726 sec/batch\n",
      "Epoch: 5/20...  Training Step: 431...  Training loss: 1.5500...  1.3706 sec/batch\n",
      "Epoch: 5/20...  Training Step: 432...  Training loss: 1.5427...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 433...  Training loss: 1.5538...  1.3711 sec/batch\n",
      "Epoch: 5/20...  Training Step: 434...  Training loss: 1.5297...  1.3706 sec/batch\n",
      "Epoch: 5/20...  Training Step: 435...  Training loss: 1.5117...  1.3732 sec/batch\n",
      "Epoch: 5/20...  Training Step: 436...  Training loss: 1.5206...  1.3742 sec/batch\n",
      "Epoch: 5/20...  Training Step: 437...  Training loss: 1.5507...  1.3726 sec/batch\n",
      "Epoch: 5/20...  Training Step: 438...  Training loss: 1.5224...  1.3742 sec/batch\n",
      "Epoch: 5/20...  Training Step: 439...  Training loss: 1.5438...  1.3747 sec/batch\n",
      "Epoch: 5/20...  Training Step: 440...  Training loss: 1.5263...  1.3762 sec/batch\n",
      "Epoch: 5/20...  Training Step: 441...  Training loss: 1.5841...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 442...  Training loss: 1.5705...  1.3696 sec/batch\n",
      "Epoch: 5/20...  Training Step: 443...  Training loss: 1.5822...  1.3721 sec/batch\n",
      "Epoch: 5/20...  Training Step: 444...  Training loss: 1.5746...  1.3716 sec/batch\n",
      "Epoch: 5/20...  Training Step: 445...  Training loss: 1.5468...  1.3711 sec/batch\n",
      "Epoch: 5/20...  Training Step: 446...  Training loss: 1.6114...  1.3798 sec/batch\n",
      "Epoch: 5/20...  Training Step: 447...  Training loss: 1.5964...  1.3891 sec/batch\n",
      "Epoch: 5/20...  Training Step: 448...  Training loss: 1.5543...  1.3872 sec/batch\n",
      "Epoch: 5/20...  Training Step: 449...  Training loss: 1.5465...  1.3957 sec/batch\n",
      "Epoch: 5/20...  Training Step: 450...  Training loss: 1.5407...  1.3878 sec/batch\n",
      "Epoch: 5/20...  Training Step: 451...  Training loss: 1.5797...  1.3930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 452...  Training loss: 1.5516...  1.3842 sec/batch\n",
      "Epoch: 5/20...  Training Step: 453...  Training loss: 1.5260...  1.3917 sec/batch\n",
      "Epoch: 5/20...  Training Step: 454...  Training loss: 1.5465...  1.3927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 455...  Training loss: 1.5370...  1.3835 sec/batch\n",
      "Epoch: 5/20...  Training Step: 456...  Training loss: 1.5515...  1.3913 sec/batch\n",
      "Epoch: 5/20...  Training Step: 457...  Training loss: 1.5862...  1.3916 sec/batch\n",
      "Epoch: 5/20...  Training Step: 458...  Training loss: 1.5906...  1.4193 sec/batch\n",
      "Epoch: 5/20...  Training Step: 459...  Training loss: 1.5644...  1.4145 sec/batch\n",
      "Epoch: 5/20...  Training Step: 460...  Training loss: 1.5484...  1.4428 sec/batch\n",
      "Epoch: 5/20...  Training Step: 461...  Training loss: 1.5970...  1.4440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 462...  Training loss: 1.5712...  1.4341 sec/batch\n",
      "Epoch: 5/20...  Training Step: 463...  Training loss: 1.5741...  1.4417 sec/batch\n",
      "Epoch: 5/20...  Training Step: 464...  Training loss: 1.5454...  1.4095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 465...  Training loss: 1.5610...  1.4244 sec/batch\n",
      "Epoch: 5/20...  Training Step: 466...  Training loss: 1.5533...  1.4098 sec/batch\n",
      "Epoch: 5/20...  Training Step: 467...  Training loss: 1.5458...  1.4182 sec/batch\n",
      "Epoch: 5/20...  Training Step: 468...  Training loss: 1.5412...  1.4199 sec/batch\n",
      "Epoch: 5/20...  Training Step: 469...  Training loss: 1.5453...  1.4127 sec/batch\n",
      "Epoch: 5/20...  Training Step: 470...  Training loss: 1.5562...  1.4269 sec/batch\n",
      "Epoch: 5/20...  Training Step: 471...  Training loss: 1.5728...  1.4263 sec/batch\n",
      "Epoch: 5/20...  Training Step: 472...  Training loss: 1.5884...  1.4489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 473...  Training loss: 1.5612...  1.4840 sec/batch\n",
      "Epoch: 5/20...  Training Step: 474...  Training loss: 1.5662...  1.4299 sec/batch\n",
      "Epoch: 5/20...  Training Step: 475...  Training loss: 1.5814...  1.4169 sec/batch\n",
      "Epoch: 5/20...  Training Step: 476...  Training loss: 1.5769...  1.4423 sec/batch\n",
      "Epoch: 5/20...  Training Step: 477...  Training loss: 1.5758...  1.4264 sec/batch\n",
      "Epoch: 5/20...  Training Step: 478...  Training loss: 1.5568...  1.4434 sec/batch\n",
      "Epoch: 5/20...  Training Step: 479...  Training loss: 1.4950...  1.4359 sec/batch\n",
      "Epoch: 5/20...  Training Step: 480...  Training loss: 1.5394...  1.4221 sec/batch\n",
      "Epoch: 5/20...  Training Step: 481...  Training loss: 1.5483...  1.4277 sec/batch\n",
      "Epoch: 5/20...  Training Step: 482...  Training loss: 1.5701...  1.4378 sec/batch\n",
      "Epoch: 5/20...  Training Step: 483...  Training loss: 1.5426...  1.4343 sec/batch\n",
      "Epoch: 5/20...  Training Step: 484...  Training loss: 1.5360...  1.4150 sec/batch\n",
      "Epoch: 5/20...  Training Step: 485...  Training loss: 1.5323...  1.4224 sec/batch\n",
      "Epoch: 5/20...  Training Step: 486...  Training loss: 1.5438...  1.4549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 487...  Training loss: 1.5295...  1.4590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 488...  Training loss: 1.5498...  1.4205 sec/batch\n",
      "Epoch: 5/20...  Training Step: 489...  Training loss: 1.5204...  1.4181 sec/batch\n",
      "Epoch: 5/20...  Training Step: 490...  Training loss: 1.5290...  1.4327 sec/batch\n",
      "Epoch: 5/20...  Training Step: 491...  Training loss: 1.5301...  1.4301 sec/batch\n",
      "Epoch: 5/20...  Training Step: 492...  Training loss: 1.5251...  1.4491 sec/batch\n",
      "Epoch: 5/20...  Training Step: 493...  Training loss: 1.5513...  1.4256 sec/batch\n",
      "Epoch: 5/20...  Training Step: 494...  Training loss: 1.5307...  1.4272 sec/batch\n",
      "Epoch: 5/20...  Training Step: 495...  Training loss: 1.5704...  1.4242 sec/batch\n",
      "Epoch: 5/20...  Training Step: 496...  Training loss: 1.5119...  1.4397 sec/batch\n",
      "Epoch: 5/20...  Training Step: 497...  Training loss: 1.5253...  1.4152 sec/batch\n",
      "Epoch: 5/20...  Training Step: 498...  Training loss: 1.5190...  1.3995 sec/batch\n",
      "Epoch: 5/20...  Training Step: 499...  Training loss: 1.5276...  1.4080 sec/batch\n",
      "Epoch: 5/20...  Training Step: 500...  Training loss: 1.5345...  1.4489 sec/batch\n",
      "Epoch: 6/20...  Training Step: 501...  Training loss: 1.6090...  1.3767 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 502...  Training loss: 1.5593...  1.3939 sec/batch\n",
      "Epoch: 6/20...  Training Step: 503...  Training loss: 1.5538...  1.4111 sec/batch\n",
      "Epoch: 6/20...  Training Step: 504...  Training loss: 1.5341...  1.3988 sec/batch\n",
      "Epoch: 6/20...  Training Step: 505...  Training loss: 1.5597...  1.3916 sec/batch\n",
      "Epoch: 6/20...  Training Step: 506...  Training loss: 1.5808...  1.4005 sec/batch\n",
      "Epoch: 6/20...  Training Step: 507...  Training loss: 1.5925...  1.3726 sec/batch\n",
      "Epoch: 6/20...  Training Step: 508...  Training loss: 1.5487...  1.3740 sec/batch\n",
      "Epoch: 6/20...  Training Step: 509...  Training loss: 1.5548...  1.3726 sec/batch\n",
      "Epoch: 6/20...  Training Step: 510...  Training loss: 1.5295...  1.3864 sec/batch\n",
      "Epoch: 6/20...  Training Step: 511...  Training loss: 1.5500...  1.3741 sec/batch\n",
      "Epoch: 6/20...  Training Step: 512...  Training loss: 1.5385...  1.3721 sec/batch\n",
      "Epoch: 6/20...  Training Step: 513...  Training loss: 1.5123...  1.3756 sec/batch\n",
      "Epoch: 6/20...  Training Step: 514...  Training loss: 1.4928...  1.3757 sec/batch\n",
      "Epoch: 6/20...  Training Step: 515...  Training loss: 1.5368...  1.3798 sec/batch\n",
      "Epoch: 6/20...  Training Step: 516...  Training loss: 1.5467...  1.3767 sec/batch\n",
      "Epoch: 6/20...  Training Step: 517...  Training loss: 1.5304...  1.3758 sec/batch\n",
      "Epoch: 6/20...  Training Step: 518...  Training loss: 1.5456...  1.3753 sec/batch\n",
      "Epoch: 6/20...  Training Step: 519...  Training loss: 1.5503...  1.3803 sec/batch\n",
      "Epoch: 6/20...  Training Step: 520...  Training loss: 1.5371...  1.3814 sec/batch\n",
      "Epoch: 6/20...  Training Step: 521...  Training loss: 1.5287...  1.3797 sec/batch\n",
      "Epoch: 6/20...  Training Step: 522...  Training loss: 1.5432...  1.3857 sec/batch\n",
      "Epoch: 6/20...  Training Step: 523...  Training loss: 1.5913...  1.3956 sec/batch\n",
      "Epoch: 6/20...  Training Step: 524...  Training loss: 1.5320...  1.3960 sec/batch\n",
      "Epoch: 6/20...  Training Step: 525...  Training loss: 1.5272...  1.4298 sec/batch\n",
      "Epoch: 6/20...  Training Step: 526...  Training loss: 1.5740...  1.4178 sec/batch\n",
      "Epoch: 6/20...  Training Step: 527...  Training loss: 1.5347...  1.4274 sec/batch\n",
      "Epoch: 6/20...  Training Step: 528...  Training loss: 1.5627...  1.4315 sec/batch\n",
      "Epoch: 6/20...  Training Step: 529...  Training loss: 1.5490...  1.4478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 530...  Training loss: 1.5786...  1.4220 sec/batch\n",
      "Epoch: 6/20...  Training Step: 531...  Training loss: 1.5326...  1.4278 sec/batch\n",
      "Epoch: 6/20...  Training Step: 532...  Training loss: 1.5319...  1.4420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 533...  Training loss: 1.5271...  1.4364 sec/batch\n",
      "Epoch: 6/20...  Training Step: 534...  Training loss: 1.5134...  1.4377 sec/batch\n",
      "Epoch: 6/20...  Training Step: 535...  Training loss: 1.4945...  1.4143 sec/batch\n",
      "Epoch: 6/20...  Training Step: 536...  Training loss: 1.5030...  1.4031 sec/batch\n",
      "Epoch: 6/20...  Training Step: 537...  Training loss: 1.5355...  1.4351 sec/batch\n",
      "Epoch: 6/20...  Training Step: 538...  Training loss: 1.5089...  1.4680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 539...  Training loss: 1.5285...  1.4405 sec/batch\n",
      "Epoch: 6/20...  Training Step: 540...  Training loss: 1.5131...  1.4664 sec/batch\n",
      "Epoch: 6/20...  Training Step: 541...  Training loss: 1.5716...  1.4113 sec/batch\n",
      "Epoch: 6/20...  Training Step: 542...  Training loss: 1.5435...  1.4188 sec/batch\n",
      "Epoch: 6/20...  Training Step: 543...  Training loss: 1.5502...  1.4148 sec/batch\n",
      "Epoch: 6/20...  Training Step: 544...  Training loss: 1.5460...  1.3742 sec/batch\n",
      "Epoch: 6/20...  Training Step: 545...  Training loss: 1.5297...  1.3738 sec/batch\n",
      "Epoch: 6/20...  Training Step: 546...  Training loss: 1.5837...  1.3765 sec/batch\n",
      "Epoch: 6/20...  Training Step: 547...  Training loss: 1.5792...  1.3907 sec/batch\n",
      "Epoch: 6/20...  Training Step: 548...  Training loss: 1.5418...  1.3730 sec/batch\n",
      "Epoch: 6/20...  Training Step: 549...  Training loss: 1.5257...  1.3738 sec/batch\n",
      "Epoch: 6/20...  Training Step: 550...  Training loss: 1.5171...  1.3739 sec/batch\n",
      "Epoch: 6/20...  Training Step: 551...  Training loss: 1.5597...  1.3754 sec/batch\n",
      "Epoch: 6/20...  Training Step: 552...  Training loss: 1.5310...  1.3764 sec/batch\n",
      "Epoch: 6/20...  Training Step: 553...  Training loss: 1.4969...  1.3770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 554...  Training loss: 1.5341...  1.3751 sec/batch\n",
      "Epoch: 6/20...  Training Step: 555...  Training loss: 1.5212...  1.3736 sec/batch\n",
      "Epoch: 6/20...  Training Step: 556...  Training loss: 1.5312...  1.3738 sec/batch\n",
      "Epoch: 6/20...  Training Step: 557...  Training loss: 1.5714...  1.3735 sec/batch\n",
      "Epoch: 6/20...  Training Step: 558...  Training loss: 1.5668...  1.3754 sec/batch\n",
      "Epoch: 6/20...  Training Step: 559...  Training loss: 1.5429...  1.3752 sec/batch\n",
      "Epoch: 6/20...  Training Step: 560...  Training loss: 1.5224...  1.3773 sec/batch\n",
      "Epoch: 6/20...  Training Step: 561...  Training loss: 1.5715...  1.3702 sec/batch\n",
      "Epoch: 6/20...  Training Step: 562...  Training loss: 1.5535...  1.3716 sec/batch\n",
      "Epoch: 6/20...  Training Step: 563...  Training loss: 1.5498...  1.3797 sec/batch\n",
      "Epoch: 6/20...  Training Step: 564...  Training loss: 1.5348...  1.3737 sec/batch\n",
      "Epoch: 6/20...  Training Step: 565...  Training loss: 1.5339...  1.3765 sec/batch\n",
      "Epoch: 6/20...  Training Step: 566...  Training loss: 1.5357...  1.3771 sec/batch\n",
      "Epoch: 6/20...  Training Step: 567...  Training loss: 1.5286...  1.3716 sec/batch\n",
      "Epoch: 6/20...  Training Step: 568...  Training loss: 1.5284...  1.3747 sec/batch\n",
      "Epoch: 6/20...  Training Step: 569...  Training loss: 1.5368...  1.3952 sec/batch\n",
      "Epoch: 6/20...  Training Step: 570...  Training loss: 1.5382...  1.3737 sec/batch\n",
      "Epoch: 6/20...  Training Step: 571...  Training loss: 1.5628...  1.3636 sec/batch\n",
      "Epoch: 6/20...  Training Step: 572...  Training loss: 1.5638...  1.3721 sec/batch\n",
      "Epoch: 6/20...  Training Step: 573...  Training loss: 1.5378...  1.3752 sec/batch\n",
      "Epoch: 6/20...  Training Step: 574...  Training loss: 1.5516...  1.3767 sec/batch\n",
      "Epoch: 6/20...  Training Step: 575...  Training loss: 1.5568...  1.3777 sec/batch\n",
      "Epoch: 6/20...  Training Step: 576...  Training loss: 1.5482...  1.3772 sec/batch\n",
      "Epoch: 6/20...  Training Step: 577...  Training loss: 1.5517...  1.3847 sec/batch\n",
      "Epoch: 6/20...  Training Step: 578...  Training loss: 1.5435...  1.3767 sec/batch\n",
      "Epoch: 6/20...  Training Step: 579...  Training loss: 1.4712...  1.3692 sec/batch\n",
      "Epoch: 6/20...  Training Step: 580...  Training loss: 1.5163...  1.3757 sec/batch\n",
      "Epoch: 6/20...  Training Step: 581...  Training loss: 1.5247...  1.3792 sec/batch\n",
      "Epoch: 6/20...  Training Step: 582...  Training loss: 1.5512...  1.3772 sec/batch\n",
      "Epoch: 6/20...  Training Step: 583...  Training loss: 1.5149...  1.3739 sec/batch\n",
      "Epoch: 6/20...  Training Step: 584...  Training loss: 1.5122...  1.3776 sec/batch\n",
      "Epoch: 6/20...  Training Step: 585...  Training loss: 1.5275...  1.3728 sec/batch\n",
      "Epoch: 6/20...  Training Step: 586...  Training loss: 1.5253...  1.3765 sec/batch\n",
      "Epoch: 6/20...  Training Step: 587...  Training loss: 1.4987...  1.3755 sec/batch\n",
      "Epoch: 6/20...  Training Step: 588...  Training loss: 1.5188...  1.3751 sec/batch\n",
      "Epoch: 6/20...  Training Step: 589...  Training loss: 1.5060...  1.3729 sec/batch\n",
      "Epoch: 6/20...  Training Step: 590...  Training loss: 1.5096...  1.3833 sec/batch\n",
      "Epoch: 6/20...  Training Step: 591...  Training loss: 1.5112...  1.3780 sec/batch\n",
      "Epoch: 6/20...  Training Step: 592...  Training loss: 1.5133...  1.3746 sec/batch\n",
      "Epoch: 6/20...  Training Step: 593...  Training loss: 1.5480...  1.3748 sec/batch\n",
      "Epoch: 6/20...  Training Step: 594...  Training loss: 1.5151...  1.3768 sec/batch\n",
      "Epoch: 6/20...  Training Step: 595...  Training loss: 1.5574...  1.3782 sec/batch\n",
      "Epoch: 6/20...  Training Step: 596...  Training loss: 1.4869...  1.3753 sec/batch\n",
      "Epoch: 6/20...  Training Step: 597...  Training loss: 1.5157...  1.3742 sec/batch\n",
      "Epoch: 6/20...  Training Step: 598...  Training loss: 1.5040...  1.3735 sec/batch\n",
      "Epoch: 6/20...  Training Step: 599...  Training loss: 1.5204...  1.3753 sec/batch\n",
      "Epoch: 6/20...  Training Step: 600...  Training loss: 1.5271...  1.3770 sec/batch\n",
      "Epoch: 7/20...  Training Step: 601...  Training loss: 1.5854...  1.3772 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 602...  Training loss: 1.5224...  1.3869 sec/batch\n",
      "Epoch: 7/20...  Training Step: 603...  Training loss: 1.5296...  1.3855 sec/batch\n",
      "Epoch: 7/20...  Training Step: 604...  Training loss: 1.5135...  1.3846 sec/batch\n",
      "Epoch: 7/20...  Training Step: 605...  Training loss: 1.5442...  1.3908 sec/batch\n",
      "Epoch: 7/20...  Training Step: 606...  Training loss: 1.5625...  1.3912 sec/batch\n",
      "Epoch: 7/20...  Training Step: 607...  Training loss: 1.5783...  1.3701 sec/batch\n",
      "Epoch: 7/20...  Training Step: 608...  Training loss: 1.5331...  1.3730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 609...  Training loss: 1.5443...  1.3735 sec/batch\n",
      "Epoch: 7/20...  Training Step: 610...  Training loss: 1.5131...  1.3715 sec/batch\n",
      "Epoch: 7/20...  Training Step: 611...  Training loss: 1.5339...  1.3732 sec/batch\n",
      "Epoch: 7/20...  Training Step: 612...  Training loss: 1.5296...  1.3741 sec/batch\n",
      "Epoch: 7/20...  Training Step: 613...  Training loss: 1.4960...  1.3761 sec/batch\n",
      "Epoch: 7/20...  Training Step: 614...  Training loss: 1.4846...  1.3763 sec/batch\n",
      "Epoch: 7/20...  Training Step: 615...  Training loss: 1.5282...  1.3862 sec/batch\n",
      "Epoch: 7/20...  Training Step: 616...  Training loss: 1.5280...  1.3846 sec/batch\n",
      "Epoch: 7/20...  Training Step: 617...  Training loss: 1.5213...  1.3761 sec/batch\n",
      "Epoch: 7/20...  Training Step: 618...  Training loss: 1.5224...  1.3879 sec/batch\n",
      "Epoch: 7/20...  Training Step: 619...  Training loss: 1.5234...  1.3863 sec/batch\n",
      "Epoch: 7/20...  Training Step: 620...  Training loss: 1.5146...  1.3903 sec/batch\n",
      "Epoch: 7/20...  Training Step: 621...  Training loss: 1.5126...  1.3796 sec/batch\n",
      "Epoch: 7/20...  Training Step: 622...  Training loss: 1.5141...  1.3812 sec/batch\n",
      "Epoch: 7/20...  Training Step: 623...  Training loss: 1.5619...  1.3798 sec/batch\n",
      "Epoch: 7/20...  Training Step: 624...  Training loss: 1.5191...  1.3903 sec/batch\n",
      "Epoch: 7/20...  Training Step: 625...  Training loss: 1.5074...  1.3793 sec/batch\n",
      "Epoch: 7/20...  Training Step: 626...  Training loss: 1.5466...  1.3711 sec/batch\n",
      "Epoch: 7/20...  Training Step: 627...  Training loss: 1.5142...  1.3742 sec/batch\n",
      "Epoch: 7/20...  Training Step: 628...  Training loss: 1.5435...  1.3742 sec/batch\n",
      "Epoch: 7/20...  Training Step: 629...  Training loss: 1.5244...  1.3757 sec/batch\n",
      "Epoch: 7/20...  Training Step: 630...  Training loss: 1.5574...  1.3751 sec/batch\n",
      "Epoch: 7/20...  Training Step: 631...  Training loss: 1.5170...  1.3947 sec/batch\n",
      "Epoch: 7/20...  Training Step: 632...  Training loss: 1.5012...  1.3724 sec/batch\n",
      "Epoch: 7/20...  Training Step: 633...  Training loss: 1.5124...  1.3751 sec/batch\n",
      "Epoch: 7/20...  Training Step: 634...  Training loss: 1.4982...  1.3745 sec/batch\n",
      "Epoch: 7/20...  Training Step: 635...  Training loss: 1.4732...  1.3851 sec/batch\n",
      "Epoch: 7/20...  Training Step: 636...  Training loss: 1.4799...  1.3735 sec/batch\n",
      "Epoch: 7/20...  Training Step: 637...  Training loss: 1.5116...  1.3946 sec/batch\n",
      "Epoch: 7/20...  Training Step: 638...  Training loss: 1.4735...  1.3762 sec/batch\n",
      "Epoch: 7/20...  Training Step: 639...  Training loss: 1.5189...  1.3733 sec/batch\n",
      "Epoch: 7/20...  Training Step: 640...  Training loss: 1.4925...  1.3779 sec/batch\n",
      "Epoch: 7/20...  Training Step: 641...  Training loss: 1.5478...  1.3861 sec/batch\n",
      "Epoch: 7/20...  Training Step: 642...  Training loss: 1.5184...  1.3905 sec/batch\n",
      "Epoch: 7/20...  Training Step: 643...  Training loss: 1.5373...  1.3892 sec/batch\n",
      "Epoch: 7/20...  Training Step: 644...  Training loss: 1.5333...  1.3948 sec/batch\n",
      "Epoch: 7/20...  Training Step: 645...  Training loss: 1.5056...  1.3902 sec/batch\n",
      "Epoch: 7/20...  Training Step: 646...  Training loss: 1.5646...  1.3992 sec/batch\n",
      "Epoch: 7/20...  Training Step: 647...  Training loss: 1.5633...  1.3779 sec/batch\n",
      "Epoch: 7/20...  Training Step: 648...  Training loss: 1.5139...  1.3742 sec/batch\n",
      "Epoch: 7/20...  Training Step: 649...  Training loss: 1.5090...  1.3776 sec/batch\n",
      "Epoch: 7/20...  Training Step: 650...  Training loss: 1.5051...  1.3745 sec/batch\n",
      "Epoch: 7/20...  Training Step: 651...  Training loss: 1.5398...  1.3872 sec/batch\n",
      "Epoch: 7/20...  Training Step: 652...  Training loss: 1.5170...  1.3959 sec/batch\n",
      "Epoch: 7/20...  Training Step: 653...  Training loss: 1.4850...  1.4023 sec/batch\n",
      "Epoch: 7/20...  Training Step: 654...  Training loss: 1.5137...  1.4305 sec/batch\n",
      "Epoch: 7/20...  Training Step: 655...  Training loss: 1.5022...  1.4062 sec/batch\n",
      "Epoch: 7/20...  Training Step: 656...  Training loss: 1.5188...  1.4313 sec/batch\n",
      "Epoch: 7/20...  Training Step: 657...  Training loss: 1.5473...  1.4292 sec/batch\n",
      "Epoch: 7/20...  Training Step: 658...  Training loss: 1.5414...  1.3928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 659...  Training loss: 1.5261...  1.4050 sec/batch\n",
      "Epoch: 7/20...  Training Step: 660...  Training loss: 1.5007...  1.4022 sec/batch\n",
      "Epoch: 7/20...  Training Step: 661...  Training loss: 1.5485...  1.4829 sec/batch\n",
      "Epoch: 7/20...  Training Step: 662...  Training loss: 1.5337...  1.3862 sec/batch\n",
      "Epoch: 7/20...  Training Step: 663...  Training loss: 1.5413...  1.3929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 664...  Training loss: 1.5051...  1.3721 sec/batch\n",
      "Epoch: 7/20...  Training Step: 665...  Training loss: 1.5076...  1.3767 sec/batch\n",
      "Epoch: 7/20...  Training Step: 666...  Training loss: 1.5172...  1.3981 sec/batch\n",
      "Epoch: 7/20...  Training Step: 667...  Training loss: 1.5153...  1.4099 sec/batch\n",
      "Epoch: 7/20...  Training Step: 668...  Training loss: 1.5015...  1.4088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 669...  Training loss: 1.5078...  1.3867 sec/batch\n",
      "Epoch: 7/20...  Training Step: 670...  Training loss: 1.5190...  1.3853 sec/batch\n",
      "Epoch: 7/20...  Training Step: 671...  Training loss: 1.5462...  1.3852 sec/batch\n",
      "Epoch: 7/20...  Training Step: 672...  Training loss: 1.5489...  1.3864 sec/batch\n",
      "Epoch: 7/20...  Training Step: 673...  Training loss: 1.5272...  1.3875 sec/batch\n",
      "Epoch: 7/20...  Training Step: 674...  Training loss: 1.5391...  1.3992 sec/batch\n",
      "Epoch: 7/20...  Training Step: 675...  Training loss: 1.5339...  1.3998 sec/batch\n",
      "Epoch: 7/20...  Training Step: 676...  Training loss: 1.5262...  1.4108 sec/batch\n",
      "Epoch: 7/20...  Training Step: 677...  Training loss: 1.5363...  1.4305 sec/batch\n",
      "Epoch: 7/20...  Training Step: 678...  Training loss: 1.5342...  1.3982 sec/batch\n",
      "Epoch: 7/20...  Training Step: 679...  Training loss: 1.4564...  1.4009 sec/batch\n",
      "Epoch: 7/20...  Training Step: 680...  Training loss: 1.4943...  1.4237 sec/batch\n",
      "Epoch: 7/20...  Training Step: 681...  Training loss: 1.5152...  1.3858 sec/batch\n",
      "Epoch: 7/20...  Training Step: 682...  Training loss: 1.5391...  1.3801 sec/batch\n",
      "Epoch: 7/20...  Training Step: 683...  Training loss: 1.5011...  1.3863 sec/batch\n",
      "Epoch: 7/20...  Training Step: 684...  Training loss: 1.4958...  1.3815 sec/batch\n",
      "Epoch: 7/20...  Training Step: 685...  Training loss: 1.5093...  1.3889 sec/batch\n",
      "Epoch: 7/20...  Training Step: 686...  Training loss: 1.5184...  1.3712 sec/batch\n",
      "Epoch: 7/20...  Training Step: 687...  Training loss: 1.4898...  1.3731 sec/batch\n",
      "Epoch: 7/20...  Training Step: 688...  Training loss: 1.5166...  1.3730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 689...  Training loss: 1.4885...  1.3747 sec/batch\n",
      "Epoch: 7/20...  Training Step: 690...  Training loss: 1.4984...  1.3760 sec/batch\n",
      "Epoch: 7/20...  Training Step: 691...  Training loss: 1.4972...  1.3731 sec/batch\n",
      "Epoch: 7/20...  Training Step: 692...  Training loss: 1.4982...  1.3917 sec/batch\n",
      "Epoch: 7/20...  Training Step: 693...  Training loss: 1.5304...  1.3831 sec/batch\n",
      "Epoch: 7/20...  Training Step: 694...  Training loss: 1.4973...  1.4039 sec/batch\n",
      "Epoch: 7/20...  Training Step: 695...  Training loss: 1.5337...  1.3836 sec/batch\n",
      "Epoch: 7/20...  Training Step: 696...  Training loss: 1.4734...  1.3913 sec/batch\n",
      "Epoch: 7/20...  Training Step: 697...  Training loss: 1.4963...  1.3752 sec/batch\n",
      "Epoch: 7/20...  Training Step: 698...  Training loss: 1.4837...  1.3726 sec/batch\n",
      "Epoch: 7/20...  Training Step: 699...  Training loss: 1.4920...  1.3787 sec/batch\n",
      "Epoch: 7/20...  Training Step: 700...  Training loss: 1.5058...  1.3792 sec/batch\n",
      "Epoch: 8/20...  Training Step: 701...  Training loss: 1.5718...  1.3752 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 702...  Training loss: 1.5156...  1.3777 sec/batch\n",
      "Epoch: 8/20...  Training Step: 703...  Training loss: 1.5186...  1.3768 sec/batch\n",
      "Epoch: 8/20...  Training Step: 704...  Training loss: 1.5012...  1.3754 sec/batch\n",
      "Epoch: 8/20...  Training Step: 705...  Training loss: 1.5243...  1.3668 sec/batch\n",
      "Epoch: 8/20...  Training Step: 706...  Training loss: 1.5439...  1.3727 sec/batch\n",
      "Epoch: 8/20...  Training Step: 707...  Training loss: 1.5549...  1.3782 sec/batch\n",
      "Epoch: 8/20...  Training Step: 708...  Training loss: 1.5229...  1.3755 sec/batch\n",
      "Epoch: 8/20...  Training Step: 709...  Training loss: 1.5296...  1.3890 sec/batch\n",
      "Epoch: 8/20...  Training Step: 710...  Training loss: 1.4970...  1.3760 sec/batch\n",
      "Epoch: 8/20...  Training Step: 711...  Training loss: 1.5148...  1.3748 sec/batch\n",
      "Epoch: 8/20...  Training Step: 712...  Training loss: 1.5113...  1.3759 sec/batch\n",
      "Epoch: 8/20...  Training Step: 713...  Training loss: 1.4781...  1.3772 sec/batch\n",
      "Epoch: 8/20...  Training Step: 714...  Training loss: 1.4601...  1.3748 sec/batch\n",
      "Epoch: 8/20...  Training Step: 715...  Training loss: 1.5066...  1.3895 sec/batch\n",
      "Epoch: 8/20...  Training Step: 716...  Training loss: 1.5087...  1.3748 sec/batch\n",
      "Epoch: 8/20...  Training Step: 717...  Training loss: 1.5011...  1.3758 sec/batch\n",
      "Epoch: 8/20...  Training Step: 718...  Training loss: 1.5007...  1.3748 sec/batch\n",
      "Epoch: 8/20...  Training Step: 719...  Training loss: 1.5182...  1.3728 sec/batch\n",
      "Epoch: 8/20...  Training Step: 720...  Training loss: 1.4964...  1.3847 sec/batch\n",
      "Epoch: 8/20...  Training Step: 721...  Training loss: 1.4962...  1.3978 sec/batch\n",
      "Epoch: 8/20...  Training Step: 722...  Training loss: 1.5030...  1.3873 sec/batch\n",
      "Epoch: 8/20...  Training Step: 723...  Training loss: 1.5562...  1.4004 sec/batch\n",
      "Epoch: 8/20...  Training Step: 724...  Training loss: 1.4973...  1.3972 sec/batch\n",
      "Epoch: 8/20...  Training Step: 725...  Training loss: 1.4829...  1.4177 sec/batch\n",
      "Epoch: 8/20...  Training Step: 726...  Training loss: 1.5196...  1.4113 sec/batch\n",
      "Epoch: 8/20...  Training Step: 727...  Training loss: 1.4981...  1.4183 sec/batch\n",
      "Epoch: 8/20...  Training Step: 728...  Training loss: 1.5239...  1.4368 sec/batch\n",
      "Epoch: 8/20...  Training Step: 729...  Training loss: 1.5042...  1.4473 sec/batch\n",
      "Epoch: 8/20...  Training Step: 730...  Training loss: 1.5500...  1.3798 sec/batch\n",
      "Epoch: 8/20...  Training Step: 731...  Training loss: 1.4884...  1.3767 sec/batch\n",
      "Epoch: 8/20...  Training Step: 732...  Training loss: 1.4851...  1.4092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 733...  Training loss: 1.5000...  1.3972 sec/batch\n",
      "Epoch: 8/20...  Training Step: 734...  Training loss: 1.4811...  1.4199 sec/batch\n",
      "Epoch: 8/20...  Training Step: 735...  Training loss: 1.4573...  1.4084 sec/batch\n",
      "Epoch: 8/20...  Training Step: 736...  Training loss: 1.4665...  1.4301 sec/batch\n",
      "Epoch: 8/20...  Training Step: 737...  Training loss: 1.5003...  1.4255 sec/batch\n",
      "Epoch: 8/20...  Training Step: 738...  Training loss: 1.4627...  1.4189 sec/batch\n",
      "Epoch: 8/20...  Training Step: 739...  Training loss: 1.5012...  1.4198 sec/batch\n",
      "Epoch: 8/20...  Training Step: 740...  Training loss: 1.4670...  1.4298 sec/batch\n",
      "Epoch: 8/20...  Training Step: 741...  Training loss: 1.5338...  1.4332 sec/batch\n",
      "Epoch: 8/20...  Training Step: 742...  Training loss: 1.5090...  1.4650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 743...  Training loss: 1.5157...  1.5018 sec/batch\n",
      "Epoch: 8/20...  Training Step: 744...  Training loss: 1.5181...  1.4074 sec/batch\n",
      "Epoch: 8/20...  Training Step: 745...  Training loss: 1.4886...  1.3938 sec/batch\n",
      "Epoch: 8/20...  Training Step: 746...  Training loss: 1.5415...  1.4028 sec/batch\n",
      "Epoch: 8/20...  Training Step: 747...  Training loss: 1.5394...  1.4460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 748...  Training loss: 1.5100...  1.4088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 749...  Training loss: 1.5024...  1.4225 sec/batch\n",
      "Epoch: 8/20...  Training Step: 750...  Training loss: 1.4935...  1.4161 sec/batch\n",
      "Epoch: 8/20...  Training Step: 751...  Training loss: 1.5246...  1.4220 sec/batch\n",
      "Epoch: 8/20...  Training Step: 752...  Training loss: 1.5008...  1.4283 sec/batch\n",
      "Epoch: 8/20...  Training Step: 753...  Training loss: 1.4676...  1.4201 sec/batch\n",
      "Epoch: 8/20...  Training Step: 754...  Training loss: 1.4903...  1.4438 sec/batch\n",
      "Epoch: 8/20...  Training Step: 755...  Training loss: 1.4900...  1.4344 sec/batch\n",
      "Epoch: 8/20...  Training Step: 756...  Training loss: 1.5017...  1.4778 sec/batch\n",
      "Epoch: 8/20...  Training Step: 757...  Training loss: 1.5346...  1.4451 sec/batch\n",
      "Epoch: 8/20...  Training Step: 758...  Training loss: 1.5271...  1.4150 sec/batch\n",
      "Epoch: 8/20...  Training Step: 759...  Training loss: 1.5153...  1.3989 sec/batch\n",
      "Epoch: 8/20...  Training Step: 760...  Training loss: 1.4943...  1.4221 sec/batch\n",
      "Epoch: 8/20...  Training Step: 761...  Training loss: 1.5379...  1.4421 sec/batch\n",
      "Epoch: 8/20...  Training Step: 762...  Training loss: 1.5215...  1.4282 sec/batch\n",
      "Epoch: 8/20...  Training Step: 763...  Training loss: 1.5234...  1.4421 sec/batch\n",
      "Epoch: 8/20...  Training Step: 764...  Training loss: 1.4926...  1.4446 sec/batch\n",
      "Epoch: 8/20...  Training Step: 765...  Training loss: 1.4977...  1.4168 sec/batch\n",
      "Epoch: 8/20...  Training Step: 766...  Training loss: 1.4934...  1.4182 sec/batch\n",
      "Epoch: 8/20...  Training Step: 767...  Training loss: 1.4983...  1.5054 sec/batch\n",
      "Epoch: 8/20...  Training Step: 768...  Training loss: 1.4829...  1.4186 sec/batch\n",
      "Epoch: 8/20...  Training Step: 769...  Training loss: 1.4977...  1.4107 sec/batch\n",
      "Epoch: 8/20...  Training Step: 770...  Training loss: 1.5040...  1.4403 sec/batch\n",
      "Epoch: 8/20...  Training Step: 771...  Training loss: 1.5247...  1.4412 sec/batch\n",
      "Epoch: 8/20...  Training Step: 772...  Training loss: 1.5405...  1.4222 sec/batch\n",
      "Epoch: 8/20...  Training Step: 773...  Training loss: 1.5064...  1.3878 sec/batch\n",
      "Epoch: 8/20...  Training Step: 774...  Training loss: 1.5142...  1.4034 sec/batch\n",
      "Epoch: 8/20...  Training Step: 775...  Training loss: 1.5221...  1.4058 sec/batch\n",
      "Epoch: 8/20...  Training Step: 776...  Training loss: 1.5116...  1.4005 sec/batch\n",
      "Epoch: 8/20...  Training Step: 777...  Training loss: 1.5249...  1.4159 sec/batch\n",
      "Epoch: 8/20...  Training Step: 778...  Training loss: 1.5136...  1.3973 sec/batch\n",
      "Epoch: 8/20...  Training Step: 779...  Training loss: 1.4523...  1.4210 sec/batch\n",
      "Epoch: 8/20...  Training Step: 780...  Training loss: 1.4851...  1.4056 sec/batch\n",
      "Epoch: 8/20...  Training Step: 781...  Training loss: 1.4865...  1.4209 sec/batch\n",
      "Epoch: 8/20...  Training Step: 782...  Training loss: 1.5219...  1.4293 sec/batch\n",
      "Epoch: 8/20...  Training Step: 783...  Training loss: 1.4774...  1.4193 sec/batch\n",
      "Epoch: 8/20...  Training Step: 784...  Training loss: 1.4757...  1.4141 sec/batch\n",
      "Epoch: 8/20...  Training Step: 785...  Training loss: 1.4884...  1.4268 sec/batch\n",
      "Epoch: 8/20...  Training Step: 786...  Training loss: 1.4907...  1.4347 sec/batch\n",
      "Epoch: 8/20...  Training Step: 787...  Training loss: 1.4684...  1.4297 sec/batch\n",
      "Epoch: 8/20...  Training Step: 788...  Training loss: 1.5009...  1.4191 sec/batch\n",
      "Epoch: 8/20...  Training Step: 789...  Training loss: 1.4709...  1.4389 sec/batch\n",
      "Epoch: 8/20...  Training Step: 790...  Training loss: 1.4707...  1.4335 sec/batch\n",
      "Epoch: 8/20...  Training Step: 791...  Training loss: 1.4724...  1.4417 sec/batch\n",
      "Epoch: 8/20...  Training Step: 792...  Training loss: 1.4720...  1.4279 sec/batch\n",
      "Epoch: 8/20...  Training Step: 793...  Training loss: 1.5063...  1.4205 sec/batch\n",
      "Epoch: 8/20...  Training Step: 794...  Training loss: 1.4748...  1.4398 sec/batch\n",
      "Epoch: 8/20...  Training Step: 795...  Training loss: 1.5148...  1.4204 sec/batch\n",
      "Epoch: 8/20...  Training Step: 796...  Training loss: 1.4575...  1.4263 sec/batch\n",
      "Epoch: 8/20...  Training Step: 797...  Training loss: 1.4921...  1.4514 sec/batch\n",
      "Epoch: 8/20...  Training Step: 798...  Training loss: 1.4725...  1.4385 sec/batch\n",
      "Epoch: 8/20...  Training Step: 799...  Training loss: 1.4777...  1.4359 sec/batch\n",
      "Epoch: 8/20...  Training Step: 800...  Training loss: 1.4841...  1.4348 sec/batch\n",
      "Epoch: 9/20...  Training Step: 801...  Training loss: 1.5592...  1.3769 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 802...  Training loss: 1.4989...  1.4040 sec/batch\n",
      "Epoch: 9/20...  Training Step: 803...  Training loss: 1.4981...  1.3944 sec/batch\n",
      "Epoch: 9/20...  Training Step: 804...  Training loss: 1.4717...  1.4204 sec/batch\n",
      "Epoch: 9/20...  Training Step: 805...  Training loss: 1.5131...  1.4098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 806...  Training loss: 1.5130...  1.4447 sec/batch\n",
      "Epoch: 9/20...  Training Step: 807...  Training loss: 1.5373...  1.4815 sec/batch\n",
      "Epoch: 9/20...  Training Step: 808...  Training loss: 1.4942...  1.4752 sec/batch\n",
      "Epoch: 9/20...  Training Step: 809...  Training loss: 1.5037...  1.4333 sec/batch\n",
      "Epoch: 9/20...  Training Step: 810...  Training loss: 1.4799...  1.4626 sec/batch\n",
      "Epoch: 9/20...  Training Step: 811...  Training loss: 1.4912...  1.4486 sec/batch\n",
      "Epoch: 9/20...  Training Step: 812...  Training loss: 1.4891...  1.4333 sec/batch\n",
      "Epoch: 9/20...  Training Step: 813...  Training loss: 1.4657...  1.4024 sec/batch\n",
      "Epoch: 9/20...  Training Step: 814...  Training loss: 1.4426...  1.4051 sec/batch\n",
      "Epoch: 9/20...  Training Step: 815...  Training loss: 1.4861...  1.4153 sec/batch\n",
      "Epoch: 9/20...  Training Step: 816...  Training loss: 1.4908...  1.4166 sec/batch\n",
      "Epoch: 9/20...  Training Step: 817...  Training loss: 1.4765...  1.4416 sec/batch\n",
      "Epoch: 9/20...  Training Step: 818...  Training loss: 1.4913...  1.4158 sec/batch\n",
      "Epoch: 9/20...  Training Step: 819...  Training loss: 1.5009...  1.4286 sec/batch\n",
      "Epoch: 9/20...  Training Step: 820...  Training loss: 1.4774...  1.4141 sec/batch\n",
      "Epoch: 9/20...  Training Step: 821...  Training loss: 1.4853...  1.4103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 822...  Training loss: 1.4820...  1.4091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 823...  Training loss: 1.5339...  1.4170 sec/batch\n",
      "Epoch: 9/20...  Training Step: 824...  Training loss: 1.4834...  1.4425 sec/batch\n",
      "Epoch: 9/20...  Training Step: 825...  Training loss: 1.4685...  1.4258 sec/batch\n",
      "Epoch: 9/20...  Training Step: 826...  Training loss: 1.5156...  1.4316 sec/batch\n",
      "Epoch: 9/20...  Training Step: 827...  Training loss: 1.4852...  1.4171 sec/batch\n",
      "Epoch: 9/20...  Training Step: 828...  Training loss: 1.5098...  1.4102 sec/batch\n",
      "Epoch: 9/20...  Training Step: 829...  Training loss: 1.4864...  1.4181 sec/batch\n",
      "Epoch: 9/20...  Training Step: 830...  Training loss: 1.5317...  1.4186 sec/batch\n",
      "Epoch: 9/20...  Training Step: 831...  Training loss: 1.4855...  1.4266 sec/batch\n",
      "Epoch: 9/20...  Training Step: 832...  Training loss: 1.4822...  1.4477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 833...  Training loss: 1.4739...  1.4145 sec/batch\n",
      "Epoch: 9/20...  Training Step: 834...  Training loss: 1.4727...  1.4283 sec/batch\n",
      "Epoch: 9/20...  Training Step: 835...  Training loss: 1.4416...  1.4163 sec/batch\n",
      "Epoch: 9/20...  Training Step: 836...  Training loss: 1.4531...  1.4280 sec/batch\n",
      "Epoch: 9/20...  Training Step: 837...  Training loss: 1.4873...  1.3924 sec/batch\n",
      "Epoch: 9/20...  Training Step: 838...  Training loss: 1.4449...  1.4017 sec/batch\n",
      "Epoch: 9/20...  Training Step: 839...  Training loss: 1.4845...  1.4004 sec/batch\n",
      "Epoch: 9/20...  Training Step: 840...  Training loss: 1.4654...  1.4024 sec/batch\n",
      "Epoch: 9/20...  Training Step: 841...  Training loss: 1.5143...  1.4007 sec/batch\n",
      "Epoch: 9/20...  Training Step: 842...  Training loss: 1.4990...  1.4084 sec/batch\n",
      "Epoch: 9/20...  Training Step: 843...  Training loss: 1.5087...  1.4065 sec/batch\n",
      "Epoch: 9/20...  Training Step: 844...  Training loss: 1.4984...  1.4665 sec/batch\n",
      "Epoch: 9/20...  Training Step: 845...  Training loss: 1.4753...  1.4630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 846...  Training loss: 1.5276...  1.4670 sec/batch\n",
      "Epoch: 9/20...  Training Step: 847...  Training loss: 1.5330...  1.4493 sec/batch\n",
      "Epoch: 9/20...  Training Step: 848...  Training loss: 1.4897...  1.4258 sec/batch\n",
      "Epoch: 9/20...  Training Step: 849...  Training loss: 1.4742...  1.4687 sec/batch\n",
      "Epoch: 9/20...  Training Step: 850...  Training loss: 1.4721...  1.4793 sec/batch\n",
      "Epoch: 9/20...  Training Step: 851...  Training loss: 1.5023...  1.4180 sec/batch\n",
      "Epoch: 9/20...  Training Step: 852...  Training loss: 1.4778...  1.4008 sec/batch\n",
      "Epoch: 9/20...  Training Step: 853...  Training loss: 1.4525...  1.4765 sec/batch\n",
      "Epoch: 9/20...  Training Step: 854...  Training loss: 1.4728...  1.4114 sec/batch\n",
      "Epoch: 9/20...  Training Step: 855...  Training loss: 1.4682...  1.4203 sec/batch\n",
      "Epoch: 9/20...  Training Step: 856...  Training loss: 1.4834...  1.4179 sec/batch\n",
      "Epoch: 9/20...  Training Step: 857...  Training loss: 1.5171...  1.4333 sec/batch\n",
      "Epoch: 9/20...  Training Step: 858...  Training loss: 1.5131...  1.3761 sec/batch\n",
      "Epoch: 9/20...  Training Step: 859...  Training loss: 1.4992...  1.3780 sec/batch\n",
      "Epoch: 9/20...  Training Step: 860...  Training loss: 1.4705...  1.3835 sec/batch\n",
      "Epoch: 9/20...  Training Step: 861...  Training loss: 1.5193...  1.3836 sec/batch\n",
      "Epoch: 9/20...  Training Step: 862...  Training loss: 1.5023...  1.3801 sec/batch\n",
      "Epoch: 9/20...  Training Step: 863...  Training loss: 1.5017...  1.3968 sec/batch\n",
      "Epoch: 9/20...  Training Step: 864...  Training loss: 1.4765...  1.3959 sec/batch\n",
      "Epoch: 9/20...  Training Step: 865...  Training loss: 1.4821...  1.4088 sec/batch\n",
      "Epoch: 9/20...  Training Step: 866...  Training loss: 1.4832...  1.4748 sec/batch\n",
      "Epoch: 9/20...  Training Step: 867...  Training loss: 1.4751...  1.4132 sec/batch\n",
      "Epoch: 9/20...  Training Step: 868...  Training loss: 1.4733...  1.4070 sec/batch\n",
      "Epoch: 9/20...  Training Step: 869...  Training loss: 1.4739...  1.3983 sec/batch\n",
      "Epoch: 9/20...  Training Step: 870...  Training loss: 1.4837...  1.4237 sec/batch\n",
      "Epoch: 9/20...  Training Step: 871...  Training loss: 1.5035...  1.4240 sec/batch\n",
      "Epoch: 9/20...  Training Step: 872...  Training loss: 1.5122...  1.4074 sec/batch\n",
      "Epoch: 9/20...  Training Step: 873...  Training loss: 1.4908...  1.4317 sec/batch\n",
      "Epoch: 9/20...  Training Step: 874...  Training loss: 1.5043...  1.4364 sec/batch\n",
      "Epoch: 9/20...  Training Step: 875...  Training loss: 1.5024...  1.4270 sec/batch\n",
      "Epoch: 9/20...  Training Step: 876...  Training loss: 1.4928...  1.4153 sec/batch\n",
      "Epoch: 9/20...  Training Step: 877...  Training loss: 1.4967...  1.4186 sec/batch\n",
      "Epoch: 9/20...  Training Step: 878...  Training loss: 1.5020...  1.4290 sec/batch\n",
      "Epoch: 9/20...  Training Step: 879...  Training loss: 1.4257...  1.4385 sec/batch\n",
      "Epoch: 9/20...  Training Step: 880...  Training loss: 1.4714...  1.4410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 881...  Training loss: 1.4760...  1.4202 sec/batch\n",
      "Epoch: 9/20...  Training Step: 882...  Training loss: 1.5050...  1.4222 sec/batch\n",
      "Epoch: 9/20...  Training Step: 883...  Training loss: 1.4567...  1.4115 sec/batch\n",
      "Epoch: 9/20...  Training Step: 884...  Training loss: 1.4699...  1.4207 sec/batch\n",
      "Epoch: 9/20...  Training Step: 885...  Training loss: 1.4811...  1.4199 sec/batch\n",
      "Epoch: 9/20...  Training Step: 886...  Training loss: 1.4803...  1.4569 sec/batch\n",
      "Epoch: 9/20...  Training Step: 887...  Training loss: 1.4450...  1.4164 sec/batch\n",
      "Epoch: 9/20...  Training Step: 888...  Training loss: 1.4730...  1.4311 sec/batch\n",
      "Epoch: 9/20...  Training Step: 889...  Training loss: 1.4600...  1.4178 sec/batch\n",
      "Epoch: 9/20...  Training Step: 890...  Training loss: 1.4673...  1.4261 sec/batch\n",
      "Epoch: 9/20...  Training Step: 891...  Training loss: 1.4644...  1.4172 sec/batch\n",
      "Epoch: 9/20...  Training Step: 892...  Training loss: 1.4522...  1.4332 sec/batch\n",
      "Epoch: 9/20...  Training Step: 893...  Training loss: 1.4947...  1.4481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 894...  Training loss: 1.4546...  1.4152 sec/batch\n",
      "Epoch: 9/20...  Training Step: 895...  Training loss: 1.4923...  1.4368 sec/batch\n",
      "Epoch: 9/20...  Training Step: 896...  Training loss: 1.4411...  1.4496 sec/batch\n",
      "Epoch: 9/20...  Training Step: 897...  Training loss: 1.4667...  1.4156 sec/batch\n",
      "Epoch: 9/20...  Training Step: 898...  Training loss: 1.4515...  1.4397 sec/batch\n",
      "Epoch: 9/20...  Training Step: 899...  Training loss: 1.4510...  1.4440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 900...  Training loss: 1.4704...  1.4340 sec/batch\n",
      "Epoch: 10/20...  Training Step: 901...  Training loss: 1.5483...  1.3767 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 902...  Training loss: 1.4738...  1.3765 sec/batch\n",
      "Epoch: 10/20...  Training Step: 903...  Training loss: 1.4895...  1.3915 sec/batch\n",
      "Epoch: 10/20...  Training Step: 904...  Training loss: 1.4688...  1.3973 sec/batch\n",
      "Epoch: 10/20...  Training Step: 905...  Training loss: 1.4946...  1.4010 sec/batch\n",
      "Epoch: 10/20...  Training Step: 906...  Training loss: 1.5063...  1.4037 sec/batch\n",
      "Epoch: 10/20...  Training Step: 907...  Training loss: 1.5332...  1.4129 sec/batch\n",
      "Epoch: 10/20...  Training Step: 908...  Training loss: 1.4907...  1.4170 sec/batch\n",
      "Epoch: 10/20...  Training Step: 909...  Training loss: 1.4868...  1.4133 sec/batch\n",
      "Epoch: 10/20...  Training Step: 910...  Training loss: 1.4676...  1.4148 sec/batch\n",
      "Epoch: 10/20...  Training Step: 911...  Training loss: 1.4772...  1.4595 sec/batch\n",
      "Epoch: 10/20...  Training Step: 912...  Training loss: 1.4788...  1.4300 sec/batch\n",
      "Epoch: 10/20...  Training Step: 913...  Training loss: 1.4412...  1.4504 sec/batch\n",
      "Epoch: 10/20...  Training Step: 914...  Training loss: 1.4368...  1.4299 sec/batch\n",
      "Epoch: 10/20...  Training Step: 915...  Training loss: 1.4628...  1.4083 sec/batch\n",
      "Epoch: 10/20...  Training Step: 916...  Training loss: 1.4787...  1.4244 sec/batch\n",
      "Epoch: 10/20...  Training Step: 917...  Training loss: 1.4616...  1.4203 sec/batch\n",
      "Epoch: 10/20...  Training Step: 918...  Training loss: 1.4723...  1.4299 sec/batch\n",
      "Epoch: 10/20...  Training Step: 919...  Training loss: 1.4746...  1.4272 sec/batch\n",
      "Epoch: 10/20...  Training Step: 920...  Training loss: 1.4710...  1.4107 sec/batch\n",
      "Epoch: 10/20...  Training Step: 921...  Training loss: 1.4653...  1.4416 sec/batch\n",
      "Epoch: 10/20...  Training Step: 922...  Training loss: 1.4643...  1.4171 sec/batch\n",
      "Epoch: 10/20...  Training Step: 923...  Training loss: 1.5258...  1.4208 sec/batch\n",
      "Epoch: 10/20...  Training Step: 924...  Training loss: 1.4617...  1.4201 sec/batch\n",
      "Epoch: 10/20...  Training Step: 925...  Training loss: 1.4616...  1.4246 sec/batch\n",
      "Epoch: 10/20...  Training Step: 926...  Training loss: 1.4927...  1.4236 sec/batch\n",
      "Epoch: 10/20...  Training Step: 927...  Training loss: 1.4693...  1.4197 sec/batch\n",
      "Epoch: 10/20...  Training Step: 928...  Training loss: 1.4929...  1.4106 sec/batch\n",
      "Epoch: 10/20...  Training Step: 929...  Training loss: 1.4732...  1.4189 sec/batch\n",
      "Epoch: 10/20...  Training Step: 930...  Training loss: 1.5134...  1.4169 sec/batch\n",
      "Epoch: 10/20...  Training Step: 931...  Training loss: 1.4656...  1.4178 sec/batch\n",
      "Epoch: 10/20...  Training Step: 932...  Training loss: 1.4571...  1.4405 sec/batch\n",
      "Epoch: 10/20...  Training Step: 933...  Training loss: 1.4598...  1.4448 sec/batch\n",
      "Epoch: 10/20...  Training Step: 934...  Training loss: 1.4505...  1.4413 sec/batch\n",
      "Epoch: 10/20...  Training Step: 935...  Training loss: 1.4239...  1.4509 sec/batch\n",
      "Epoch: 10/20...  Training Step: 936...  Training loss: 1.4286...  1.4104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 937...  Training loss: 1.4788...  1.4085 sec/batch\n",
      "Epoch: 10/20...  Training Step: 938...  Training loss: 1.4352...  1.4118 sec/batch\n",
      "Epoch: 10/20...  Training Step: 939...  Training loss: 1.4642...  1.4436 sec/batch\n",
      "Epoch: 10/20...  Training Step: 940...  Training loss: 1.4392...  1.4329 sec/batch\n",
      "Epoch: 10/20...  Training Step: 941...  Training loss: 1.4973...  1.4228 sec/batch\n",
      "Epoch: 10/20...  Training Step: 942...  Training loss: 1.4765...  1.4137 sec/batch\n",
      "Epoch: 10/20...  Training Step: 943...  Training loss: 1.4827...  1.4741 sec/batch\n",
      "Epoch: 10/20...  Training Step: 944...  Training loss: 1.4863...  1.4488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 945...  Training loss: 1.4547...  1.4332 sec/batch\n",
      "Epoch: 10/20...  Training Step: 946...  Training loss: 1.5179...  1.4428 sec/batch\n",
      "Epoch: 10/20...  Training Step: 947...  Training loss: 1.5128...  1.4585 sec/batch\n",
      "Epoch: 10/20...  Training Step: 948...  Training loss: 1.4681...  1.4416 sec/batch\n",
      "Epoch: 10/20...  Training Step: 949...  Training loss: 1.4645...  1.4317 sec/batch\n",
      "Epoch: 10/20...  Training Step: 950...  Training loss: 1.4526...  1.4486 sec/batch\n",
      "Epoch: 10/20...  Training Step: 951...  Training loss: 1.5018...  1.4573 sec/batch\n",
      "Epoch: 10/20...  Training Step: 952...  Training loss: 1.4656...  1.4509 sec/batch\n",
      "Epoch: 10/20...  Training Step: 953...  Training loss: 1.4427...  1.4353 sec/batch\n",
      "Epoch: 10/20...  Training Step: 954...  Training loss: 1.4582...  1.4103 sec/batch\n",
      "Epoch: 10/20...  Training Step: 955...  Training loss: 1.4508...  1.4128 sec/batch\n",
      "Epoch: 10/20...  Training Step: 956...  Training loss: 1.4757...  1.4223 sec/batch\n",
      "Epoch: 10/20...  Training Step: 957...  Training loss: 1.4984...  1.3847 sec/batch\n",
      "Epoch: 10/20...  Training Step: 958...  Training loss: 1.4909...  1.3898 sec/batch\n",
      "Epoch: 10/20...  Training Step: 959...  Training loss: 1.4825...  1.4029 sec/batch\n",
      "Epoch: 10/20...  Training Step: 960...  Training loss: 1.4576...  1.4124 sec/batch\n",
      "Epoch: 10/20...  Training Step: 961...  Training loss: 1.4979...  1.3872 sec/batch\n",
      "Epoch: 10/20...  Training Step: 962...  Training loss: 1.4810...  1.3998 sec/batch\n",
      "Epoch: 10/20...  Training Step: 963...  Training loss: 1.4924...  1.3989 sec/batch\n",
      "Epoch: 10/20...  Training Step: 964...  Training loss: 1.4625...  1.4021 sec/batch\n",
      "Epoch: 10/20...  Training Step: 965...  Training loss: 1.4579...  1.4017 sec/batch\n",
      "Epoch: 10/20...  Training Step: 966...  Training loss: 1.4595...  1.3970 sec/batch\n",
      "Epoch: 10/20...  Training Step: 967...  Training loss: 1.4703...  1.4113 sec/batch\n",
      "Epoch: 10/20...  Training Step: 968...  Training loss: 1.4547...  1.4031 sec/batch\n",
      "Epoch: 10/20...  Training Step: 969...  Training loss: 1.4583...  1.4057 sec/batch\n",
      "Epoch: 10/20...  Training Step: 970...  Training loss: 1.4774...  1.4090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 971...  Training loss: 1.4925...  1.4005 sec/batch\n",
      "Epoch: 10/20...  Training Step: 972...  Training loss: 1.4998...  1.3994 sec/batch\n",
      "Epoch: 10/20...  Training Step: 973...  Training loss: 1.4855...  1.3887 sec/batch\n",
      "Epoch: 10/20...  Training Step: 974...  Training loss: 1.4820...  1.3996 sec/batch\n",
      "Epoch: 10/20...  Training Step: 975...  Training loss: 1.4862...  1.4130 sec/batch\n",
      "Epoch: 10/20...  Training Step: 976...  Training loss: 1.4778...  1.4141 sec/batch\n",
      "Epoch: 10/20...  Training Step: 977...  Training loss: 1.4842...  1.4079 sec/batch\n",
      "Epoch: 10/20...  Training Step: 978...  Training loss: 1.4787...  1.4113 sec/batch\n",
      "Epoch: 10/20...  Training Step: 979...  Training loss: 1.4050...  1.4197 sec/batch\n",
      "Epoch: 10/20...  Training Step: 980...  Training loss: 1.4503...  1.4090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 981...  Training loss: 1.4503...  1.4089 sec/batch\n",
      "Epoch: 10/20...  Training Step: 982...  Training loss: 1.4843...  1.4315 sec/batch\n",
      "Epoch: 10/20...  Training Step: 983...  Training loss: 1.4526...  1.4195 sec/batch\n",
      "Epoch: 10/20...  Training Step: 984...  Training loss: 1.4528...  1.4348 sec/batch\n",
      "Epoch: 10/20...  Training Step: 985...  Training loss: 1.4559...  1.4397 sec/batch\n",
      "Epoch: 10/20...  Training Step: 986...  Training loss: 1.4602...  1.4308 sec/batch\n",
      "Epoch: 10/20...  Training Step: 987...  Training loss: 1.4445...  1.4291 sec/batch\n",
      "Epoch: 10/20...  Training Step: 988...  Training loss: 1.4631...  1.4467 sec/batch\n",
      "Epoch: 10/20...  Training Step: 989...  Training loss: 1.4304...  1.4400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 990...  Training loss: 1.4497...  1.4151 sec/batch\n",
      "Epoch: 10/20...  Training Step: 991...  Training loss: 1.4435...  1.4408 sec/batch\n",
      "Epoch: 10/20...  Training Step: 992...  Training loss: 1.4508...  1.4419 sec/batch\n",
      "Epoch: 10/20...  Training Step: 993...  Training loss: 1.4804...  1.4440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 994...  Training loss: 1.4445...  1.4752 sec/batch\n",
      "Epoch: 10/20...  Training Step: 995...  Training loss: 1.4879...  1.4442 sec/batch\n",
      "Epoch: 10/20...  Training Step: 996...  Training loss: 1.4387...  1.4347 sec/batch\n",
      "Epoch: 10/20...  Training Step: 997...  Training loss: 1.4475...  1.4420 sec/batch\n",
      "Epoch: 10/20...  Training Step: 998...  Training loss: 1.4512...  1.4338 sec/batch\n",
      "Epoch: 10/20...  Training Step: 999...  Training loss: 1.4475...  1.4347 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1000...  Training loss: 1.4600...  1.4423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1001...  Training loss: 1.5313...  1.3752 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1002...  Training loss: 1.4616...  1.3797 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1003...  Training loss: 1.4713...  1.3813 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1004...  Training loss: 1.4489...  1.3940 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1005...  Training loss: 1.4869...  1.3896 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1006...  Training loss: 1.4828...  1.3788 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1007...  Training loss: 1.5129...  1.4046 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1008...  Training loss: 1.4824...  1.4113 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1009...  Training loss: 1.4775...  1.4019 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1010...  Training loss: 1.4513...  1.3923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1011...  Training loss: 1.4661...  1.3939 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1012...  Training loss: 1.4535...  1.4084 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1013...  Training loss: 1.4189...  1.4137 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1014...  Training loss: 1.4121...  1.4178 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1015...  Training loss: 1.4583...  1.4192 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1016...  Training loss: 1.4599...  1.4535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1017...  Training loss: 1.4524...  1.4270 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1018...  Training loss: 1.4607...  1.4184 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1019...  Training loss: 1.4647...  1.4401 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1020...  Training loss: 1.4455...  1.4328 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1021...  Training loss: 1.4528...  1.4180 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1022...  Training loss: 1.4621...  1.4090 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1023...  Training loss: 1.5097...  1.4311 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1024...  Training loss: 1.4551...  1.4267 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1025...  Training loss: 1.4388...  1.4410 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1026...  Training loss: 1.4860...  1.4435 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1027...  Training loss: 1.4535...  1.4382 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1028...  Training loss: 1.4829...  1.4250 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1029...  Training loss: 1.4609...  1.3951 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1030...  Training loss: 1.4970...  1.4378 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1031...  Training loss: 1.4507...  1.4383 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1032...  Training loss: 1.4447...  1.4363 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1033...  Training loss: 1.4518...  1.4263 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1034...  Training loss: 1.4378...  1.4233 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1035...  Training loss: 1.4085...  1.3887 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1036...  Training loss: 1.4172...  1.3942 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1037...  Training loss: 1.4534...  1.3932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1038...  Training loss: 1.4306...  1.3877 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1039...  Training loss: 1.4490...  1.3912 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1040...  Training loss: 1.4395...  1.3742 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1041...  Training loss: 1.4863...  1.3787 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1042...  Training loss: 1.4714...  1.3752 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1043...  Training loss: 1.4645...  1.3757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1044...  Training loss: 1.4614...  1.3757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1045...  Training loss: 1.4472...  1.3767 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1046...  Training loss: 1.5026...  1.3811 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1047...  Training loss: 1.4940...  1.3787 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1048...  Training loss: 1.4559...  1.3882 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1049...  Training loss: 1.4496...  1.3751 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1050...  Training loss: 1.4417...  1.3942 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1051...  Training loss: 1.4909...  1.3978 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1052...  Training loss: 1.4465...  1.3986 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1053...  Training loss: 1.4118...  1.4189 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1054...  Training loss: 1.4454...  1.4081 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1055...  Training loss: 1.4412...  1.4155 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1056...  Training loss: 1.4574...  1.4306 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1057...  Training loss: 1.4981...  1.4252 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1058...  Training loss: 1.4879...  1.4160 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1059...  Training loss: 1.4646...  1.4079 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1060...  Training loss: 1.4378...  1.4359 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1061...  Training loss: 1.4900...  1.4354 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1062...  Training loss: 1.4689...  1.4456 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1063...  Training loss: 1.4785...  1.5812 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1064...  Training loss: 1.4537...  1.4494 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1065...  Training loss: 1.4618...  1.4255 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1066...  Training loss: 1.4548...  1.4212 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1067...  Training loss: 1.4486...  1.4399 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1068...  Training loss: 1.4389...  1.4150 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1069...  Training loss: 1.4492...  1.3968 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1070...  Training loss: 1.4593...  1.4028 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1071...  Training loss: 1.4778...  1.4058 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1072...  Training loss: 1.4829...  1.3983 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1073...  Training loss: 1.4662...  1.4023 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1074...  Training loss: 1.4741...  1.4127 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1075...  Training loss: 1.4737...  1.4184 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1076...  Training loss: 1.4594...  1.4184 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1077...  Training loss: 1.4841...  1.4140 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1078...  Training loss: 1.4706...  1.4223 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1079...  Training loss: 1.3955...  1.4077 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1080...  Training loss: 1.4406...  1.4208 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1081...  Training loss: 1.4512...  1.4303 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1082...  Training loss: 1.4753...  1.4627 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1083...  Training loss: 1.4511...  1.4157 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1084...  Training loss: 1.4350...  1.4450 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1085...  Training loss: 1.4471...  1.4317 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1086...  Training loss: 1.4497...  1.4085 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1087...  Training loss: 1.4284...  1.3939 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1088...  Training loss: 1.4411...  1.4218 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1089...  Training loss: 1.4327...  1.4115 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1090...  Training loss: 1.4217...  1.4361 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1091...  Training loss: 1.4271...  1.4441 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1092...  Training loss: 1.4281...  1.4461 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1093...  Training loss: 1.4651...  1.4085 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1094...  Training loss: 1.4239...  1.3996 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1095...  Training loss: 1.4637...  1.4225 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1096...  Training loss: 1.4161...  1.4080 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1097...  Training loss: 1.4358...  1.4201 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1098...  Training loss: 1.4349...  1.4433 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1099...  Training loss: 1.4351...  1.4248 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1100...  Training loss: 1.4361...  1.4642 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1101...  Training loss: 1.5080...  1.3742 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1102...  Training loss: 1.4536...  1.3799 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1103...  Training loss: 1.4573...  1.3776 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1104...  Training loss: 1.4289...  1.3767 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1105...  Training loss: 1.4630...  1.3799 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1106...  Training loss: 1.4794...  1.3793 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1107...  Training loss: 1.5017...  1.3807 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1108...  Training loss: 1.4552...  1.3795 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1109...  Training loss: 1.4649...  1.3792 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1110...  Training loss: 1.4442...  1.3817 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1111...  Training loss: 1.4460...  1.3903 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1112...  Training loss: 1.4469...  1.3922 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1113...  Training loss: 1.4126...  1.3855 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1114...  Training loss: 1.3993...  1.4019 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1115...  Training loss: 1.4507...  1.4066 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1116...  Training loss: 1.4433...  1.3992 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1117...  Training loss: 1.4389...  1.4130 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1118...  Training loss: 1.4456...  1.4103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1119...  Training loss: 1.4499...  1.4254 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1120...  Training loss: 1.4391...  1.4188 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1121...  Training loss: 1.4459...  1.4183 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1122...  Training loss: 1.4462...  1.4143 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1123...  Training loss: 1.4994...  1.4635 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1124...  Training loss: 1.4326...  1.3859 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1125...  Training loss: 1.4278...  1.3903 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1126...  Training loss: 1.4719...  1.3976 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1127...  Training loss: 1.4294...  1.4122 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1128...  Training loss: 1.4694...  1.4079 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1129...  Training loss: 1.4484...  1.4049 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1130...  Training loss: 1.4863...  1.4207 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1131...  Training loss: 1.4366...  1.4169 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1132...  Training loss: 1.4353...  1.3967 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1133...  Training loss: 1.4325...  1.4075 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1134...  Training loss: 1.4213...  1.4082 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1135...  Training loss: 1.4015...  1.4057 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1136...  Training loss: 1.4049...  1.4453 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1137...  Training loss: 1.4338...  1.4175 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1138...  Training loss: 1.4056...  1.4159 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1139...  Training loss: 1.4345...  1.3991 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1140...  Training loss: 1.4160...  1.4046 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1141...  Training loss: 1.4608...  1.4411 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1142...  Training loss: 1.4409...  1.4446 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1143...  Training loss: 1.4551...  1.4397 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1144...  Training loss: 1.4535...  1.4429 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1145...  Training loss: 1.4388...  1.4125 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1146...  Training loss: 1.4862...  1.4335 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1147...  Training loss: 1.4795...  1.4519 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1148...  Training loss: 1.4426...  1.5381 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1149...  Training loss: 1.4308...  1.4048 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1150...  Training loss: 1.4239...  1.3962 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1151...  Training loss: 1.4666...  1.4097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1152...  Training loss: 1.4399...  1.3965 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1153...  Training loss: 1.4005...  1.3887 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1154...  Training loss: 1.4331...  1.3881 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1155...  Training loss: 1.4264...  1.3919 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1156...  Training loss: 1.4474...  1.3997 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1157...  Training loss: 1.4813...  1.4264 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1158...  Training loss: 1.4673...  1.4764 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1159...  Training loss: 1.4634...  1.3935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1160...  Training loss: 1.4233...  1.3935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1161...  Training loss: 1.4792...  1.3927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1162...  Training loss: 1.4587...  1.3913 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1163...  Training loss: 1.4704...  1.3764 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1164...  Training loss: 1.4286...  1.3909 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1165...  Training loss: 1.4414...  1.3972 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1166...  Training loss: 1.4438...  1.3959 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1167...  Training loss: 1.4357...  1.3971 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1168...  Training loss: 1.4313...  1.4122 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1169...  Training loss: 1.4350...  1.4064 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1170...  Training loss: 1.4450...  1.4121 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1171...  Training loss: 1.4652...  1.4203 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1172...  Training loss: 1.4818...  1.4101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1173...  Training loss: 1.4459...  1.4061 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1174...  Training loss: 1.4606...  1.4188 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1175...  Training loss: 1.4663...  1.4064 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1176...  Training loss: 1.4513...  1.4046 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1177...  Training loss: 1.4594...  1.4357 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1178...  Training loss: 1.4558...  1.4218 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1179...  Training loss: 1.3922...  1.5674 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1180...  Training loss: 1.4234...  1.4115 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1181...  Training loss: 1.4256...  1.4244 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1182...  Training loss: 1.4646...  1.4950 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1183...  Training loss: 1.4309...  1.4175 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1184...  Training loss: 1.4241...  1.3961 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1185...  Training loss: 1.4407...  1.3967 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1186...  Training loss: 1.4387...  1.4010 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1187...  Training loss: 1.4115...  1.3894 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1188...  Training loss: 1.4315...  1.4213 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1189...  Training loss: 1.4219...  1.3971 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1190...  Training loss: 1.4178...  1.3883 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1191...  Training loss: 1.4125...  1.4063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1192...  Training loss: 1.4040...  1.4024 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1193...  Training loss: 1.4584...  1.4342 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1194...  Training loss: 1.4207...  1.4307 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1195...  Training loss: 1.4545...  1.4170 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1196...  Training loss: 1.4052...  1.4188 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 1197...  Training loss: 1.4242...  1.4195 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1198...  Training loss: 1.4126...  1.4319 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1199...  Training loss: 1.4239...  1.4565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1200...  Training loss: 1.4352...  1.5267 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1201...  Training loss: 1.5036...  1.3761 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1202...  Training loss: 1.4417...  1.3796 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1203...  Training loss: 1.4348...  1.3763 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1204...  Training loss: 1.4134...  1.3902 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1205...  Training loss: 1.4552...  1.3900 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1206...  Training loss: 1.4610...  1.4309 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1207...  Training loss: 1.4766...  1.4263 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1208...  Training loss: 1.4477...  1.4211 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1209...  Training loss: 1.4533...  1.4267 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1210...  Training loss: 1.4353...  1.4421 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1211...  Training loss: 1.4261...  1.4281 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1212...  Training loss: 1.4410...  1.4296 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1213...  Training loss: 1.4104...  1.4412 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1214...  Training loss: 1.3903...  1.4416 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1215...  Training loss: 1.4296...  1.4056 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1216...  Training loss: 1.4262...  1.4013 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1217...  Training loss: 1.4162...  1.4139 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1218...  Training loss: 1.4357...  1.4217 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1219...  Training loss: 1.4451...  1.4228 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1220...  Training loss: 1.4187...  1.4183 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1221...  Training loss: 1.4280...  1.4245 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1222...  Training loss: 1.4297...  1.4541 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1223...  Training loss: 1.4801...  1.4319 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1224...  Training loss: 1.4319...  1.4680 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1225...  Training loss: 1.4128...  1.4372 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1226...  Training loss: 1.4555...  1.4391 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1227...  Training loss: 1.4282...  1.4410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1228...  Training loss: 1.4514...  1.4444 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1229...  Training loss: 1.4385...  1.4112 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1230...  Training loss: 1.4736...  1.3904 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1231...  Training loss: 1.4229...  1.4203 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1232...  Training loss: 1.4129...  1.4341 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1233...  Training loss: 1.4266...  1.4345 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1234...  Training loss: 1.4109...  1.4402 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1235...  Training loss: 1.3856...  1.4584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1236...  Training loss: 1.3936...  1.3857 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1237...  Training loss: 1.4280...  1.3849 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1238...  Training loss: 1.3968...  1.3844 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1239...  Training loss: 1.4277...  1.3749 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1240...  Training loss: 1.4047...  1.4096 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1241...  Training loss: 1.4650...  1.3889 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1242...  Training loss: 1.4386...  1.3880 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1243...  Training loss: 1.4428...  1.4028 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1244...  Training loss: 1.4498...  1.4268 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1245...  Training loss: 1.4215...  1.4345 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1246...  Training loss: 1.4741...  1.3859 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1247...  Training loss: 1.4615...  1.3799 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1248...  Training loss: 1.4266...  1.4463 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1249...  Training loss: 1.4159...  1.4163 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1250...  Training loss: 1.4204...  1.4302 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1251...  Training loss: 1.4542...  1.4180 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1252...  Training loss: 1.4218...  1.4134 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1253...  Training loss: 1.3827...  1.4881 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1254...  Training loss: 1.4134...  1.4772 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1255...  Training loss: 1.4203...  1.4339 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1256...  Training loss: 1.4317...  1.4320 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1257...  Training loss: 1.4563...  1.4380 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1258...  Training loss: 1.4542...  1.4184 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1259...  Training loss: 1.4370...  1.4342 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1260...  Training loss: 1.4025...  1.4259 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1261...  Training loss: 1.4594...  1.4319 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1262...  Training loss: 1.4508...  1.4382 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1263...  Training loss: 1.4531...  1.4684 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1264...  Training loss: 1.4165...  1.4024 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1265...  Training loss: 1.4249...  1.4050 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1266...  Training loss: 1.4184...  1.4168 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1267...  Training loss: 1.4279...  1.3948 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1268...  Training loss: 1.4061...  1.4117 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1269...  Training loss: 1.4214...  1.3874 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1270...  Training loss: 1.4393...  1.3988 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1271...  Training loss: 1.4515...  1.3945 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1272...  Training loss: 1.4675...  1.3922 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1273...  Training loss: 1.4395...  1.4101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1274...  Training loss: 1.4419...  1.4156 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1275...  Training loss: 1.4484...  1.4195 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1276...  Training loss: 1.4331...  1.4238 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1277...  Training loss: 1.4365...  1.4284 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1278...  Training loss: 1.4387...  1.4402 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1279...  Training loss: 1.3781...  1.4129 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1280...  Training loss: 1.4205...  1.4012 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1281...  Training loss: 1.4288...  1.4237 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1282...  Training loss: 1.4482...  1.4146 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1283...  Training loss: 1.4257...  1.4258 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1284...  Training loss: 1.4111...  1.4007 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1285...  Training loss: 1.4157...  1.3979 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1286...  Training loss: 1.4255...  1.4076 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1287...  Training loss: 1.4037...  1.3980 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1288...  Training loss: 1.4140...  1.3995 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1289...  Training loss: 1.4118...  1.3997 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1290...  Training loss: 1.4048...  1.4397 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1291...  Training loss: 1.4000...  1.4117 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1292...  Training loss: 1.4013...  1.4153 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1293...  Training loss: 1.4387...  1.4259 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1294...  Training loss: 1.4160...  1.4301 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 1295...  Training loss: 1.4451...  1.4243 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1296...  Training loss: 1.3891...  1.4348 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1297...  Training loss: 1.4029...  1.4335 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1298...  Training loss: 1.4088...  1.4291 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1299...  Training loss: 1.4007...  1.4142 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1300...  Training loss: 1.4300...  1.4257 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1301...  Training loss: 1.4892...  1.3757 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1302...  Training loss: 1.4291...  1.3762 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1303...  Training loss: 1.4235...  1.3721 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1304...  Training loss: 1.4203...  1.3767 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1305...  Training loss: 1.4405...  1.3767 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1306...  Training loss: 1.4512...  1.3787 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1307...  Training loss: 1.4729...  1.3752 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1308...  Training loss: 1.4340...  1.3762 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1309...  Training loss: 1.4368...  1.3782 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1310...  Training loss: 1.4149...  1.3772 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1311...  Training loss: 1.4275...  1.3757 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1312...  Training loss: 1.4176...  1.3782 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1313...  Training loss: 1.3925...  1.3796 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1314...  Training loss: 1.3793...  1.3981 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1315...  Training loss: 1.4233...  1.4046 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1316...  Training loss: 1.4202...  1.3963 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1317...  Training loss: 1.4148...  1.4260 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1318...  Training loss: 1.4126...  1.4272 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1319...  Training loss: 1.4247...  1.4175 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1320...  Training loss: 1.4085...  1.4400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1321...  Training loss: 1.4187...  1.4329 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1322...  Training loss: 1.4161...  1.4224 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1323...  Training loss: 1.4662...  1.4360 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1324...  Training loss: 1.4107...  1.3975 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1325...  Training loss: 1.4061...  1.4163 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1326...  Training loss: 1.4385...  1.4435 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1327...  Training loss: 1.4151...  1.4278 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1328...  Training loss: 1.4497...  1.4104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1329...  Training loss: 1.4178...  1.4165 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1330...  Training loss: 1.4590...  1.4289 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1331...  Training loss: 1.4084...  1.4275 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1332...  Training loss: 1.4078...  1.4190 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1333...  Training loss: 1.4146...  1.4236 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1334...  Training loss: 1.3964...  1.4713 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1335...  Training loss: 1.3744...  1.4369 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1336...  Training loss: 1.3846...  1.4276 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1337...  Training loss: 1.4193...  1.4229 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1338...  Training loss: 1.3866...  1.4252 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1339...  Training loss: 1.4136...  1.4424 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1340...  Training loss: 1.4060...  1.4307 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1341...  Training loss: 1.4441...  1.4333 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1342...  Training loss: 1.4194...  1.4888 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1343...  Training loss: 1.4385...  1.4238 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1344...  Training loss: 1.4334...  1.4423 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1345...  Training loss: 1.4107...  1.3961 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1346...  Training loss: 1.4712...  1.4251 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1347...  Training loss: 1.4548...  1.4198 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1348...  Training loss: 1.4119...  1.3737 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1349...  Training loss: 1.4087...  1.3834 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1350...  Training loss: 1.4069...  1.3781 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1351...  Training loss: 1.4382...  1.4156 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1352...  Training loss: 1.4136...  1.4088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1353...  Training loss: 1.3832...  1.4128 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1354...  Training loss: 1.4050...  1.4037 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1355...  Training loss: 1.4018...  1.4123 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1356...  Training loss: 1.4164...  1.4111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1357...  Training loss: 1.4426...  1.4123 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1358...  Training loss: 1.4359...  1.4353 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1359...  Training loss: 1.4361...  1.4028 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1360...  Training loss: 1.3947...  1.4409 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1361...  Training loss: 1.4522...  1.4366 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1362...  Training loss: 1.4387...  1.4208 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1363...  Training loss: 1.4367...  1.4619 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1364...  Training loss: 1.4123...  1.4116 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1365...  Training loss: 1.4253...  1.3937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1366...  Training loss: 1.4163...  1.4063 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1367...  Training loss: 1.4106...  1.4000 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1368...  Training loss: 1.3989...  1.4127 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1369...  Training loss: 1.4053...  1.4150 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1370...  Training loss: 1.4081...  1.4198 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1371...  Training loss: 1.4331...  1.4227 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1372...  Training loss: 1.4501...  1.4198 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1373...  Training loss: 1.4262...  1.4207 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1374...  Training loss: 1.4382...  1.4302 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1375...  Training loss: 1.4440...  1.4502 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1376...  Training loss: 1.4239...  1.4209 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1377...  Training loss: 1.4350...  1.4243 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1378...  Training loss: 1.4231...  1.4253 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1379...  Training loss: 1.3670...  1.4238 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1380...  Training loss: 1.3931...  1.4213 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1381...  Training loss: 1.4087...  1.4178 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1382...  Training loss: 1.4310...  1.4144 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1383...  Training loss: 1.4066...  1.4280 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1384...  Training loss: 1.3989...  1.4288 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1385...  Training loss: 1.4008...  1.4271 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1386...  Training loss: 1.4185...  1.3775 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1387...  Training loss: 1.3865...  1.3944 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1388...  Training loss: 1.4134...  1.4085 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1389...  Training loss: 1.3934...  1.4010 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1390...  Training loss: 1.3970...  1.4136 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1391...  Training loss: 1.3967...  1.4114 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1392...  Training loss: 1.3819...  1.4192 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 1393...  Training loss: 1.4266...  1.4229 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1394...  Training loss: 1.3910...  1.4333 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1395...  Training loss: 1.4365...  1.3977 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1396...  Training loss: 1.3893...  1.4104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1397...  Training loss: 1.3883...  1.4163 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1398...  Training loss: 1.3911...  1.4071 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1399...  Training loss: 1.3856...  1.3764 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1400...  Training loss: 1.4136...  1.3789 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1401...  Training loss: 1.4744...  1.3754 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1402...  Training loss: 1.4207...  1.3790 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1403...  Training loss: 1.4106...  1.3905 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1404...  Training loss: 1.4043...  1.3887 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1405...  Training loss: 1.4312...  1.4236 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1406...  Training loss: 1.4376...  1.4580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1407...  Training loss: 1.4596...  1.4580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1408...  Training loss: 1.4167...  1.4279 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1409...  Training loss: 1.4202...  1.4163 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1410...  Training loss: 1.3985...  1.4125 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1411...  Training loss: 1.4140...  1.3996 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1412...  Training loss: 1.4089...  1.4112 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1413...  Training loss: 1.3806...  1.4945 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1414...  Training loss: 1.3734...  1.4366 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1415...  Training loss: 1.4064...  1.4500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1416...  Training loss: 1.4131...  1.4211 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1417...  Training loss: 1.4035...  1.4199 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1418...  Training loss: 1.4101...  1.4079 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1419...  Training loss: 1.4164...  1.4424 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1420...  Training loss: 1.4079...  1.4366 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1421...  Training loss: 1.4161...  1.4368 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1422...  Training loss: 1.4047...  1.4653 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1423...  Training loss: 1.4581...  1.4164 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1424...  Training loss: 1.4046...  1.4426 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1425...  Training loss: 1.3898...  1.4392 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1426...  Training loss: 1.4362...  1.4383 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1427...  Training loss: 1.4019...  1.4258 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1428...  Training loss: 1.4356...  1.4595 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1429...  Training loss: 1.4083...  1.5863 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1430...  Training loss: 1.4526...  1.4073 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1431...  Training loss: 1.3905...  1.4160 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1432...  Training loss: 1.3909...  1.4113 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1433...  Training loss: 1.3910...  1.4181 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1434...  Training loss: 1.3787...  1.4425 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1435...  Training loss: 1.3648...  1.3980 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1436...  Training loss: 1.3723...  1.4126 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1437...  Training loss: 1.4063...  1.4017 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1438...  Training loss: 1.3747...  1.4051 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1439...  Training loss: 1.4058...  1.4143 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1440...  Training loss: 1.3857...  1.4391 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1441...  Training loss: 1.4413...  1.4750 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1442...  Training loss: 1.4200...  1.4099 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1443...  Training loss: 1.4282...  1.4365 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1444...  Training loss: 1.4194...  1.4488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1445...  Training loss: 1.3962...  1.4187 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1446...  Training loss: 1.4465...  1.4217 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1447...  Training loss: 1.4473...  1.3972 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1448...  Training loss: 1.4065...  1.4047 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1449...  Training loss: 1.4024...  1.4127 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1450...  Training loss: 1.3887...  1.4190 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1451...  Training loss: 1.4266...  1.4206 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1452...  Training loss: 1.3967...  1.4215 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1453...  Training loss: 1.3796...  1.4261 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1454...  Training loss: 1.3979...  1.4300 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1455...  Training loss: 1.3909...  1.4463 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1456...  Training loss: 1.4043...  1.4361 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1457...  Training loss: 1.4402...  1.4588 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1458...  Training loss: 1.4312...  1.4388 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1459...  Training loss: 1.4274...  1.4313 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1460...  Training loss: 1.3865...  1.3936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1461...  Training loss: 1.4429...  1.3937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1462...  Training loss: 1.4257...  1.3917 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1463...  Training loss: 1.4253...  1.3812 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1464...  Training loss: 1.4071...  1.3733 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1465...  Training loss: 1.4088...  1.3769 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1466...  Training loss: 1.4103...  1.3753 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1467...  Training loss: 1.4063...  1.3785 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1468...  Training loss: 1.3809...  1.3772 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1469...  Training loss: 1.4025...  1.3906 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1470...  Training loss: 1.4092...  1.3972 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1471...  Training loss: 1.4264...  1.3891 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1472...  Training loss: 1.4410...  1.3886 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1473...  Training loss: 1.4163...  1.3905 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1474...  Training loss: 1.4292...  1.4259 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1475...  Training loss: 1.4212...  1.4416 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1476...  Training loss: 1.4178...  1.3983 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1477...  Training loss: 1.4266...  1.3938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1478...  Training loss: 1.4181...  1.3952 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1479...  Training loss: 1.3478...  1.4224 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1480...  Training loss: 1.3900...  1.4268 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1481...  Training loss: 1.3927...  1.4277 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1482...  Training loss: 1.4166...  1.4179 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1483...  Training loss: 1.3934...  1.4262 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1484...  Training loss: 1.3880...  1.4315 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1485...  Training loss: 1.4041...  1.4446 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1486...  Training loss: 1.4028...  1.4374 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1487...  Training loss: 1.3695...  1.4310 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1488...  Training loss: 1.3986...  1.4077 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1489...  Training loss: 1.3851...  1.4130 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1490...  Training loss: 1.3849...  1.4199 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 1491...  Training loss: 1.3784...  1.4264 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1492...  Training loss: 1.3800...  1.4199 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1493...  Training loss: 1.4212...  1.4436 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1494...  Training loss: 1.3856...  1.4347 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1495...  Training loss: 1.4246...  1.4326 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1496...  Training loss: 1.3695...  1.4164 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1497...  Training loss: 1.3795...  1.4244 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1498...  Training loss: 1.3743...  1.4515 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1499...  Training loss: 1.3819...  1.4159 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1500...  Training loss: 1.3979...  1.3735 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1501...  Training loss: 1.4693...  1.3763 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1502...  Training loss: 1.4128...  1.3751 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1503...  Training loss: 1.4076...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1504...  Training loss: 1.3770...  1.3783 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1505...  Training loss: 1.4146...  1.3749 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1506...  Training loss: 1.4199...  1.3737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1507...  Training loss: 1.4445...  1.3763 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1508...  Training loss: 1.4099...  1.3754 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1509...  Training loss: 1.4232...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1510...  Training loss: 1.3971...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1511...  Training loss: 1.3978...  1.3787 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1512...  Training loss: 1.4002...  1.3742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1513...  Training loss: 1.3552...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1514...  Training loss: 1.3595...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1515...  Training loss: 1.3867...  1.3737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1516...  Training loss: 1.3972...  1.3742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1517...  Training loss: 1.3883...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1518...  Training loss: 1.3942...  1.3832 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1519...  Training loss: 1.4003...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1520...  Training loss: 1.3865...  1.3772 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1521...  Training loss: 1.3966...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1522...  Training loss: 1.3937...  1.3737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1523...  Training loss: 1.4557...  1.3711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1524...  Training loss: 1.3900...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1525...  Training loss: 1.3755...  1.3737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1526...  Training loss: 1.4212...  1.3711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1527...  Training loss: 1.3935...  1.3737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1528...  Training loss: 1.4280...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1529...  Training loss: 1.3978...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1530...  Training loss: 1.4371...  1.3711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1531...  Training loss: 1.3907...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1532...  Training loss: 1.3836...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1533...  Training loss: 1.3946...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1534...  Training loss: 1.3724...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1535...  Training loss: 1.3510...  1.3782 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1536...  Training loss: 1.3553...  1.3752 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1537...  Training loss: 1.3886...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1538...  Training loss: 1.3615...  1.3742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1539...  Training loss: 1.3897...  1.3742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1540...  Training loss: 1.3786...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1541...  Training loss: 1.4245...  1.3711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1542...  Training loss: 1.4146...  1.3711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1543...  Training loss: 1.4029...  1.3752 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1544...  Training loss: 1.4078...  1.3722 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1545...  Training loss: 1.3883...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1546...  Training loss: 1.4403...  1.3712 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1547...  Training loss: 1.4333...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1548...  Training loss: 1.3996...  1.3757 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1549...  Training loss: 1.3857...  1.3766 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1550...  Training loss: 1.3767...  1.3742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1551...  Training loss: 1.4205...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1552...  Training loss: 1.3914...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1553...  Training loss: 1.3607...  1.3719 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1554...  Training loss: 1.3790...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1555...  Training loss: 1.3810...  1.3701 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1556...  Training loss: 1.3917...  1.3747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1557...  Training loss: 1.4263...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1558...  Training loss: 1.4198...  1.3762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1559...  Training loss: 1.4062...  1.3807 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1560...  Training loss: 1.3695...  1.3762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1561...  Training loss: 1.4290...  1.3747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1562...  Training loss: 1.4186...  1.3772 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1563...  Training loss: 1.4166...  1.3747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1564...  Training loss: 1.3857...  1.3736 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1565...  Training loss: 1.3973...  1.3752 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1566...  Training loss: 1.3895...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1567...  Training loss: 1.3950...  1.3747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1568...  Training loss: 1.3745...  1.3757 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1569...  Training loss: 1.3912...  1.3752 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1570...  Training loss: 1.3912...  1.3762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1571...  Training loss: 1.4170...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1572...  Training loss: 1.4271...  1.3686 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1573...  Training loss: 1.4101...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1574...  Training loss: 1.4172...  1.3711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1575...  Training loss: 1.4169...  1.3782 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1576...  Training loss: 1.4095...  1.3772 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1577...  Training loss: 1.4156...  1.3732 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1578...  Training loss: 1.4017...  1.3742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1579...  Training loss: 1.3393...  1.3757 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1580...  Training loss: 1.3701...  1.3762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1581...  Training loss: 1.3787...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1582...  Training loss: 1.4034...  1.3721 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1583...  Training loss: 1.3773...  1.3872 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1584...  Training loss: 1.3761...  1.3726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1585...  Training loss: 1.3851...  1.3757 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1586...  Training loss: 1.3938...  1.3747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1587...  Training loss: 1.3534...  1.3737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1588...  Training loss: 1.3797...  1.3727 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 1589...  Training loss: 1.3719...  1.3731 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1590...  Training loss: 1.3710...  1.3716 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1591...  Training loss: 1.3709...  1.3752 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1592...  Training loss: 1.3647...  1.3767 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1593...  Training loss: 1.4053...  1.3752 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1594...  Training loss: 1.3660...  1.3757 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1595...  Training loss: 1.4080...  1.3777 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1596...  Training loss: 1.3632...  1.3751 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1597...  Training loss: 1.3856...  1.3762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1598...  Training loss: 1.3612...  1.3777 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1599...  Training loss: 1.3695...  1.3767 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1600...  Training loss: 1.3830...  1.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1601...  Training loss: 1.4485...  1.3757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1602...  Training loss: 1.4030...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1603...  Training loss: 1.3959...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1604...  Training loss: 1.3789...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1605...  Training loss: 1.4096...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1606...  Training loss: 1.4237...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1607...  Training loss: 1.4374...  1.3747 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1608...  Training loss: 1.3990...  1.3782 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1609...  Training loss: 1.3990...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1610...  Training loss: 1.3789...  1.3727 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1611...  Training loss: 1.3995...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1612...  Training loss: 1.3906...  1.3736 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1613...  Training loss: 1.3514...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1614...  Training loss: 1.3461...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1615...  Training loss: 1.3916...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1616...  Training loss: 1.3861...  1.3722 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1617...  Training loss: 1.3753...  1.3747 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1618...  Training loss: 1.3793...  1.3757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1619...  Training loss: 1.3997...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1620...  Training loss: 1.3809...  1.3753 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1621...  Training loss: 1.3795...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1622...  Training loss: 1.3817...  1.3736 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1623...  Training loss: 1.4351...  1.3782 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1624...  Training loss: 1.3834...  1.3727 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1625...  Training loss: 1.3726...  1.3726 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1626...  Training loss: 1.4173...  1.3792 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1627...  Training loss: 1.3863...  1.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1628...  Training loss: 1.4101...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1629...  Training loss: 1.3932...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1630...  Training loss: 1.4246...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1631...  Training loss: 1.3768...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1632...  Training loss: 1.3700...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1633...  Training loss: 1.3721...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1634...  Training loss: 1.3659...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1635...  Training loss: 1.3403...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1636...  Training loss: 1.3481...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1637...  Training loss: 1.3810...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1638...  Training loss: 1.3502...  1.3782 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1639...  Training loss: 1.3771...  1.3721 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1640...  Training loss: 1.3648...  1.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1641...  Training loss: 1.4165...  1.3782 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1642...  Training loss: 1.3926...  1.3783 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1643...  Training loss: 1.4089...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1644...  Training loss: 1.3964...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1645...  Training loss: 1.3760...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1646...  Training loss: 1.4351...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1647...  Training loss: 1.4309...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1648...  Training loss: 1.3887...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1649...  Training loss: 1.3608...  1.3787 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1650...  Training loss: 1.3688...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1651...  Training loss: 1.4103...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1652...  Training loss: 1.3917...  1.3807 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1653...  Training loss: 1.3480...  1.3782 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1654...  Training loss: 1.3637...  1.3757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1655...  Training loss: 1.3657...  1.3731 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1656...  Training loss: 1.3964...  1.3721 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1657...  Training loss: 1.4168...  1.3787 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1658...  Training loss: 1.4169...  1.3757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1659...  Training loss: 1.3902...  1.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1660...  Training loss: 1.3582...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1661...  Training loss: 1.4145...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1662...  Training loss: 1.4041...  1.3773 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1663...  Training loss: 1.4048...  1.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1664...  Training loss: 1.3789...  1.3716 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1665...  Training loss: 1.3838...  1.3716 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1666...  Training loss: 1.3819...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1667...  Training loss: 1.3860...  1.3787 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1668...  Training loss: 1.3714...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1669...  Training loss: 1.3722...  1.3726 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1670...  Training loss: 1.3791...  1.3817 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1671...  Training loss: 1.4057...  1.3737 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1672...  Training loss: 1.4182...  1.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1673...  Training loss: 1.3899...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1674...  Training loss: 1.4045...  1.3757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1675...  Training loss: 1.4031...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1676...  Training loss: 1.3884...  1.3857 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1677...  Training loss: 1.3981...  1.3716 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1678...  Training loss: 1.4057...  1.3817 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1679...  Training loss: 1.3273...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1680...  Training loss: 1.3692...  1.3762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1681...  Training loss: 1.3713...  1.3817 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1682...  Training loss: 1.4007...  1.3747 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1683...  Training loss: 1.3699...  1.3787 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1684...  Training loss: 1.3633...  1.3802 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1685...  Training loss: 1.3765...  1.3777 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1686...  Training loss: 1.3801...  1.3837 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 1687...  Training loss: 1.3485...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1688...  Training loss: 1.3670...  1.3773 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1689...  Training loss: 1.3546...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1690...  Training loss: 1.3623...  1.3757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1691...  Training loss: 1.3473...  1.3722 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1692...  Training loss: 1.3497...  1.3772 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1693...  Training loss: 1.3889...  1.3717 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1694...  Training loss: 1.3631...  1.3792 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1695...  Training loss: 1.4040...  1.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1696...  Training loss: 1.3526...  1.3732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1697...  Training loss: 1.3638...  1.3716 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1698...  Training loss: 1.3605...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1699...  Training loss: 1.3607...  1.3767 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1700...  Training loss: 1.3745...  1.3726 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1701...  Training loss: 1.4350...  1.3752 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1702...  Training loss: 1.3912...  1.3747 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1703...  Training loss: 1.3848...  1.3742 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1704...  Training loss: 1.3602...  1.3742 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1705...  Training loss: 1.4014...  1.3767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1706...  Training loss: 1.4039...  1.3737 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1707...  Training loss: 1.4291...  1.3757 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1708...  Training loss: 1.3864...  1.3783 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1709...  Training loss: 1.3977...  1.3782 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1710...  Training loss: 1.3718...  1.3767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1711...  Training loss: 1.3839...  1.3757 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1712...  Training loss: 1.3857...  1.3787 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1713...  Training loss: 1.3414...  1.3767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1714...  Training loss: 1.3257...  1.3777 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1715...  Training loss: 1.3673...  1.3767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1716...  Training loss: 1.3762...  1.3757 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1717...  Training loss: 1.3726...  1.3787 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1718...  Training loss: 1.3758...  1.3762 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1719...  Training loss: 1.3819...  1.3752 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1720...  Training loss: 1.3786...  1.3726 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1721...  Training loss: 1.3760...  1.3726 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1722...  Training loss: 1.3771...  1.3794 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1723...  Training loss: 1.4196...  1.3801 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1724...  Training loss: 1.3713...  1.3799 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1725...  Training loss: 1.3573...  1.3743 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1726...  Training loss: 1.3982...  1.3743 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1727...  Training loss: 1.3709...  1.3743 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1728...  Training loss: 1.4111...  1.3874 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1729...  Training loss: 1.3826...  1.3822 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1730...  Training loss: 1.4229...  1.3899 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1731...  Training loss: 1.3725...  1.3856 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1732...  Training loss: 1.3611...  1.3857 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1733...  Training loss: 1.3663...  1.3850 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1734...  Training loss: 1.3467...  1.3919 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1735...  Training loss: 1.3378...  1.3992 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1736...  Training loss: 1.3344...  1.4442 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1737...  Training loss: 1.3750...  1.4025 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1738...  Training loss: 1.3484...  1.4332 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1739...  Training loss: 1.3696...  1.4083 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1740...  Training loss: 1.3504...  1.4011 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1741...  Training loss: 1.4007...  1.4067 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1742...  Training loss: 1.3776...  1.4403 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1743...  Training loss: 1.3882...  1.3987 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1744...  Training loss: 1.3853...  1.4041 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1745...  Training loss: 1.3667...  1.3923 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1746...  Training loss: 1.4206...  1.3912 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1747...  Training loss: 1.4062...  1.3922 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1748...  Training loss: 1.3724...  1.4328 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1749...  Training loss: 1.3622...  1.3843 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1750...  Training loss: 1.3580...  1.3816 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1751...  Training loss: 1.3858...  1.4046 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1752...  Training loss: 1.3698...  1.3737 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1753...  Training loss: 1.3365...  1.3748 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1754...  Training loss: 1.3547...  1.3738 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1755...  Training loss: 1.3616...  1.3775 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1756...  Training loss: 1.3733...  1.3833 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1757...  Training loss: 1.4051...  1.3740 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1758...  Training loss: 1.4040...  1.3802 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1759...  Training loss: 1.3796...  1.3780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1760...  Training loss: 1.3551...  1.3741 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1761...  Training loss: 1.4071...  1.3815 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1762...  Training loss: 1.3866...  1.3762 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1763...  Training loss: 1.3984...  1.3761 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1764...  Training loss: 1.3609...  1.3732 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1765...  Training loss: 1.3662...  1.3760 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1766...  Training loss: 1.3689...  1.3731 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1767...  Training loss: 1.3846...  1.3712 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1768...  Training loss: 1.3622...  1.3776 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1769...  Training loss: 1.3784...  1.3762 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1770...  Training loss: 1.3711...  1.3757 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1771...  Training loss: 1.3863...  1.3762 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1772...  Training loss: 1.4122...  1.3711 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1773...  Training loss: 1.3965...  1.3742 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1774...  Training loss: 1.3993...  1.3752 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1775...  Training loss: 1.3964...  1.3812 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1776...  Training loss: 1.3848...  1.3767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1777...  Training loss: 1.3964...  1.3737 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1778...  Training loss: 1.3878...  1.3762 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1779...  Training loss: 1.3214...  1.3752 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1780...  Training loss: 1.3583...  1.3752 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1781...  Training loss: 1.3596...  1.3731 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1782...  Training loss: 1.3886...  1.3782 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1783...  Training loss: 1.3637...  1.3807 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1784...  Training loss: 1.3603...  1.3757 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 1785...  Training loss: 1.3653...  1.3737 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1786...  Training loss: 1.3708...  1.3732 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1787...  Training loss: 1.3311...  1.3732 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1788...  Training loss: 1.3610...  1.3818 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1789...  Training loss: 1.3506...  1.3747 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1790...  Training loss: 1.3612...  1.3793 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1791...  Training loss: 1.3482...  1.3854 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1792...  Training loss: 1.3524...  1.3980 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1793...  Training loss: 1.3931...  1.3918 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1794...  Training loss: 1.3554...  1.4065 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1795...  Training loss: 1.3862...  1.3909 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1796...  Training loss: 1.3488...  1.4092 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1797...  Training loss: 1.3628...  1.4018 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1798...  Training loss: 1.3505...  1.3772 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1799...  Training loss: 1.3542...  1.3738 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1800...  Training loss: 1.3628...  1.3772 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1801...  Training loss: 1.4283...  1.3772 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1802...  Training loss: 1.3751...  1.3752 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1803...  Training loss: 1.3662...  1.3747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1804...  Training loss: 1.3590...  1.3726 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1805...  Training loss: 1.3892...  1.3762 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1806...  Training loss: 1.3970...  1.3822 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1807...  Training loss: 1.4105...  1.3752 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1808...  Training loss: 1.3762...  1.3782 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1809...  Training loss: 1.3912...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1810...  Training loss: 1.3643...  1.3782 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1811...  Training loss: 1.3715...  1.3787 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1812...  Training loss: 1.3635...  1.3767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1813...  Training loss: 1.3375...  1.3762 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1814...  Training loss: 1.3277...  1.3767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1815...  Training loss: 1.3701...  1.3777 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1816...  Training loss: 1.3658...  1.3792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1817...  Training loss: 1.3615...  1.3797 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1818...  Training loss: 1.3734...  1.3776 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1819...  Training loss: 1.3725...  1.3772 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1820...  Training loss: 1.3740...  1.3767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1821...  Training loss: 1.3577...  1.3757 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1822...  Training loss: 1.3641...  1.3777 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1823...  Training loss: 1.4075...  1.3782 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1824...  Training loss: 1.3624...  1.3772 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1825...  Training loss: 1.3479...  1.3782 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1826...  Training loss: 1.3898...  1.3767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1827...  Training loss: 1.3694...  1.3737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1828...  Training loss: 1.3893...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1829...  Training loss: 1.3673...  1.3742 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1830...  Training loss: 1.4134...  1.3726 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1831...  Training loss: 1.3509...  1.3727 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1832...  Training loss: 1.3516...  1.3792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1833...  Training loss: 1.3526...  1.3711 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1834...  Training loss: 1.3436...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1835...  Training loss: 1.3255...  1.3737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1836...  Training loss: 1.3200...  1.3727 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1837...  Training loss: 1.3635...  1.3742 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1838...  Training loss: 1.3337...  1.3747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1839...  Training loss: 1.3628...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1840...  Training loss: 1.3470...  1.3711 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1841...  Training loss: 1.4019...  1.3720 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1842...  Training loss: 1.3751...  1.3711 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1843...  Training loss: 1.3759...  1.3742 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1844...  Training loss: 1.3691...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1845...  Training loss: 1.3511...  1.3710 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1846...  Training loss: 1.4019...  1.3756 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1847...  Training loss: 1.3930...  1.3784 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1848...  Training loss: 1.3719...  1.3796 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1849...  Training loss: 1.3526...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1850...  Training loss: 1.3463...  1.3752 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1851...  Training loss: 1.3845...  1.3749 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1852...  Training loss: 1.3564...  1.3750 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1853...  Training loss: 1.3248...  1.3726 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1854...  Training loss: 1.3558...  1.3757 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1855...  Training loss: 1.3509...  1.3717 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1856...  Training loss: 1.3733...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1857...  Training loss: 1.3878...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1858...  Training loss: 1.3815...  1.3711 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1859...  Training loss: 1.3857...  1.3701 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1860...  Training loss: 1.3346...  1.3706 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1861...  Training loss: 1.4017...  1.3706 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1862...  Training loss: 1.3851...  1.3736 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1863...  Training loss: 1.3861...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1864...  Training loss: 1.3606...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1865...  Training loss: 1.3647...  1.3722 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1866...  Training loss: 1.3614...  1.3731 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1867...  Training loss: 1.3643...  1.3737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1868...  Training loss: 1.3524...  1.3721 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1869...  Training loss: 1.3610...  1.3721 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1870...  Training loss: 1.3644...  1.3752 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1871...  Training loss: 1.3846...  1.3737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1872...  Training loss: 1.3920...  1.3721 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1873...  Training loss: 1.3800...  1.3721 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1874...  Training loss: 1.3826...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1875...  Training loss: 1.3809...  1.3761 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1876...  Training loss: 1.3748...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1877...  Training loss: 1.3769...  1.3701 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1878...  Training loss: 1.3816...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1879...  Training loss: 1.3027...  1.3742 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1880...  Training loss: 1.3531...  1.3717 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1881...  Training loss: 1.3467...  1.3742 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1882...  Training loss: 1.3785...  1.3747 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 1883...  Training loss: 1.3586...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1884...  Training loss: 1.3400...  1.3747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1885...  Training loss: 1.3618...  1.3762 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1886...  Training loss: 1.3615...  1.3721 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1887...  Training loss: 1.3293...  1.3721 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1888...  Training loss: 1.3555...  1.3732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1889...  Training loss: 1.3460...  1.3716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1890...  Training loss: 1.3493...  1.3762 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1891...  Training loss: 1.3310...  1.3731 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1892...  Training loss: 1.3410...  1.3747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1893...  Training loss: 1.3742...  1.3787 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1894...  Training loss: 1.3386...  1.3777 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1895...  Training loss: 1.3832...  1.3747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1896...  Training loss: 1.3322...  1.3772 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1897...  Training loss: 1.3406...  1.3752 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1898...  Training loss: 1.3402...  1.3742 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1899...  Training loss: 1.3420...  1.3747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1900...  Training loss: 1.3512...  1.3773 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1901...  Training loss: 1.4155...  1.3805 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1902...  Training loss: 1.3724...  1.3752 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1903...  Training loss: 1.3642...  1.3796 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1904...  Training loss: 1.3401...  1.3796 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1905...  Training loss: 1.3718...  1.3788 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1906...  Training loss: 1.3924...  1.3810 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1907...  Training loss: 1.4122...  1.3780 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1908...  Training loss: 1.3741...  1.3814 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1909...  Training loss: 1.3773...  1.3810 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1910...  Training loss: 1.3385...  1.3945 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1911...  Training loss: 1.3605...  1.3877 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1912...  Training loss: 1.3595...  1.4165 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1913...  Training loss: 1.3276...  1.4096 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1914...  Training loss: 1.3168...  1.4032 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1915...  Training loss: 1.3560...  1.4267 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1916...  Training loss: 1.3528...  1.4263 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1917...  Training loss: 1.3510...  1.4187 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1918...  Training loss: 1.3597...  1.4154 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1919...  Training loss: 1.3662...  1.4471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1920...  Training loss: 1.3507...  1.4362 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1921...  Training loss: 1.3542...  1.4755 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1922...  Training loss: 1.3592...  1.4904 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1923...  Training loss: 1.4128...  1.4302 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1924...  Training loss: 1.3547...  1.4354 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1925...  Training loss: 1.3429...  1.4286 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1926...  Training loss: 1.3724...  1.4263 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1927...  Training loss: 1.3544...  1.4438 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1928...  Training loss: 1.3847...  1.4304 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1929...  Training loss: 1.3559...  1.4167 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1930...  Training loss: 1.4019...  1.4092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1931...  Training loss: 1.3376...  1.4228 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1932...  Training loss: 1.3407...  1.4109 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1933...  Training loss: 1.3526...  1.3959 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1934...  Training loss: 1.3334...  1.4073 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1935...  Training loss: 1.3125...  1.3982 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1936...  Training loss: 1.3149...  1.4153 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1937...  Training loss: 1.3600...  1.3972 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1938...  Training loss: 1.3214...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1939...  Training loss: 1.3477...  1.3727 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1940...  Training loss: 1.3330...  1.3731 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1941...  Training loss: 1.3815...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1942...  Training loss: 1.3639...  1.3732 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1943...  Training loss: 1.3761...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1944...  Training loss: 1.3665...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1945...  Training loss: 1.3448...  1.3742 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1946...  Training loss: 1.4150...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1947...  Training loss: 1.3970...  1.3752 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1948...  Training loss: 1.3600...  1.3802 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1949...  Training loss: 1.3484...  1.3769 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1950...  Training loss: 1.3416...  1.3762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1951...  Training loss: 1.3869...  1.3777 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1952...  Training loss: 1.3615...  1.3767 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1953...  Training loss: 1.3192...  1.3792 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1954...  Training loss: 1.3386...  1.3797 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1955...  Training loss: 1.3386...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1956...  Training loss: 1.3562...  1.3742 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1957...  Training loss: 1.3877...  1.3797 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1958...  Training loss: 1.3753...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1959...  Training loss: 1.3656...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1960...  Training loss: 1.3293...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1961...  Training loss: 1.3854...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1962...  Training loss: 1.3849...  1.3777 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1963...  Training loss: 1.3842...  1.3767 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1964...  Training loss: 1.3435...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1965...  Training loss: 1.3538...  1.3762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1966...  Training loss: 1.3561...  1.3762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1967...  Training loss: 1.3597...  1.3782 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1968...  Training loss: 1.3376...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1969...  Training loss: 1.3414...  1.3701 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1970...  Training loss: 1.3564...  1.3748 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1971...  Training loss: 1.3714...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1972...  Training loss: 1.3828...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1973...  Training loss: 1.3710...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1974...  Training loss: 1.3692...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1975...  Training loss: 1.3821...  1.3777 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1976...  Training loss: 1.3555...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1977...  Training loss: 1.3791...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1978...  Training loss: 1.3750...  1.3788 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1979...  Training loss: 1.2938...  1.3772 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1980...  Training loss: 1.3339...  1.3772 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 1981...  Training loss: 1.3518...  1.3729 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1982...  Training loss: 1.3737...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1983...  Training loss: 1.3470...  1.3762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1984...  Training loss: 1.3404...  1.3731 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1985...  Training loss: 1.3455...  1.3762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1986...  Training loss: 1.3566...  1.3792 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1987...  Training loss: 1.3231...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1988...  Training loss: 1.3380...  1.3732 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1989...  Training loss: 1.3286...  1.3787 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1990...  Training loss: 1.3263...  1.3797 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1991...  Training loss: 1.3307...  1.3757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1992...  Training loss: 1.3274...  1.3747 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1993...  Training loss: 1.3683...  1.3752 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1994...  Training loss: 1.3332...  1.3762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1995...  Training loss: 1.3679...  1.3742 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1996...  Training loss: 1.3268...  1.3787 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1997...  Training loss: 1.3247...  1.3777 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1998...  Training loss: 1.3333...  1.3787 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1999...  Training loss: 1.3381...  1.3732 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2000...  Training loss: 1.3417...  1.3747 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    saver.restore(sess, 'checkpoints\\\\i1000_l512.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora só falta Gerar o texto a partir da rede\n",
    "\n",
    "Geraremos caractere a caractere, usando uma função que escolhe probabilisticamente entre os _top N_ caracteres mais prováveis de aparecerem de acordo com a RNN treinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está o caminho do último checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i2000_l512.ckpt'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns exemplos de texto gerado a partir deste checkpoint é este:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i2000_l512.ckpt\n",
      "Eram sombras. Mas não estava pronto. As pravessas e em seus olhos, ele passavam de novo. Entregando-se correndo a partiram. O portão se encontrava as passagres do Anel e estava claro, em suas\n",
      "sementes a cabeça e a certa, em pess antes do que era uma margina, mos quando os outros permarecem esperando as coisas até o que procurava em direção ao leste, e estavam entre ela, e se algumas portas atrás dele, com um cavale repontou e ali, atantou a chama do campo, e estava em suas coiras atrás deles a lenta e enquanto a mesm em seus anos sobre o senhor entes de uma coisa que, sinuos e sobres e olhos contra um consolho prescapar. Olhou ao lado de Bri, e o meu caminho davam por algum\n",
      "senhor. E estava\n",
      "subindo no lago cobriu o carado e suriam e colocadas a espada de ali, e a espura se apagar pou a chiva que era o sim dos anos, a lesbe das montanhas se abriumos e com embarco a maiar e descia sobre o mesmo tampé. O sol entre seus cabelos colocaram a encura e chegaram o anel de um perigo caminho. A coisa\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Era\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i2000_l512.ckpt\n",
      "Eram na floresta no tempo, conduzindo-o pela margem. E se enquanto alguma for a cordento e os perto da perigo, e a centeria a comida e a mesmo asi estivessem cheias de carados. As coisas acha que seus portas, além das montanhas do mal estava sentado até que as passagem solteres, e agora se esgrevam pronunciar por uma vez a noite a coração por trazia de novo. A mara do modo tinha através do convidado para ele. Em sua mão de volta de caminhos saber que as pontos profundos dos outros arasmos pra ele, e a luz de seu anos para\n",
      "o próprio Cavaleiro da Entrad. A perdo em passou em cira na esprada. A passagem subindo de um lar e outros ele se apreximava por esta longa, cominha de como a espada, em que ele supou se tentar engrando a mais frate, e o menos sentiu o senhor;\n",
      "e assim e chegaram o que tem sabedos. Mesmo as palavras do Anel estava melhor. Ele se pode chamar de uma crescura. O caminho e a porta sinto como um cavilho encontrado com os pés e para o campo, embaixo assim, além das colinas\n",
      "parec\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Era\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note como o texto gerado vai melhorando com o passar do treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i100_l512.ckpt\n",
      "Eraa  e sa aoo  eas s a o a  aeesee  esa  soss oess ee ae s  sasa  esase   ooo eee  ss  es s  eeo e  es e a aoe ao  es esaesa  ao ose a a o ao e  seeoe ase ose o  aaa  a eeaa sas  seooa eoea so ssees ose oosss e  ee aao a aoao eeao eeo   aa oses e a eses aeasssea  o o a aaeee  oesa  aae oo  oa  e o ooe  oooeo  s o oaoa  ae  sa aaas   aoa o  oses  osaeao  oaa  o ea  eae s ooe  oaoe e oea oe a  aae  oe sse eeoae  a asseo o ae   esa e  aaaa o  ae ose   eoesa e saeos ssaeos a eees  so e aseae   aeoaa  aaaes esaaso  eaee   s raea o s eao  e   osaa  s ea a es aoa esas o aaee s oeooos    oaae  oaeeoe   eosoa  a ees  ao o  eeo  aa asee see sooos  ee aee  o ssaoaa  sae a se  oa s ao soeo   aoaseo o e oe os essosaee ess  o a sea o oeo o soeeoa   eaoao  oeo seoeo aeos  ae e aa e ae ssaa aoee  a  saaa   eaeoo a  eo aaoeea  eoe  so oa  e ae eea   aeoae  osso a a a ee  e  ra soo sa o a a saa ae   sa saae  oaaa e a ee  oaos oo oe aoea eeaa aes aea  aosseo  ee  oaae seas  see aeeo aoeeaea  s  rsas ee aaoe\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints\\\\i100_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Era\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l512.ckpt\n",
      "Era, \n",
      "ente sue sanara aranam e palas dos ue se te tirade eres amarartina donso ses re tiadas ponasa dor am ramam uido de cola dendo enante core a contondos ostiridam dam desdo esestos as ertasane to conam a palana semas ar e pantena a e paremo dontam ementos ues elorone do pemtamas ene tentera so pestoras or os ane as a es o tereno. — E tas elo telia, aramo cas erar aro pianom o eria poma sentamto emo ta tos enentemes o doro pirona pamtes do pomas destens ue dos aras dele de endam ere asemtam a pelo dessa de parte e e pasrrane anside eltona dore do esenantoras o poros uass os e taro su ro cas ase pos eresemas elo pore endo ses eses uos pirtor eror ua poreras ento tarem doses e prro des oss ui a essiito, am osser a as e casdira. — Esso esta ciado dos o es as rres e penes onta se eltoras a te tono esase as ararta ame cortordo se tanosem arena cimas antinode. — Ertas urares delamtos ui dertone de pirantu sa amtos antino pas os re ara dam duisos\n",
      "e tes alam emestira ana asa as e tes aras a teli\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Era\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i600_l512.ckpt\n",
      "Erar ma no sua conco tero do pestos, em a nom sembia, e astresso de suar estinha conte para está dila.\n",
      "Ertovam a misto do fonha e contente a condo ele came estados algore dos osses derros. A alguma empava e encontado, e ancurta de penterios de pasa cariginta.\n",
      "— Então por cortou, ela num cominho de paria camanta.\n",
      "— Mas não tressa, alés pora essa entenda de padina. Alave es alguntos. — Não está pera o dia estenda a destra e enteram por esto destando, coma e silha e perecas essor camo e a mosso astia pale contando, e cama cintira disse pode es raveles., Mesta ne cunaram a dis com de porte dere ela em ancostar de cadente a asserado. Nove e ascula espentido, e sobbide o que ente para a contuda.\n",
      "— Vente a manter, alhos se serrando, e pranasser pos enseras precidessas e ancira de sou e escirindo a semes, e esculos de cirando da\n",
      "caleça estrava do cregindo.\n",
      "Osso podo sua angontaram não com diveita am olente a pinaceste. Es uma vezida. A ele aria em sou priter para estária disecu.\n",
      "— Eu dasciram de m\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Era\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1000_l512.ckpt\n",
      "Eran, das alguéis altave de aragora e o pessoria protunda, e as pessagassas procimeras\n",
      "em solhar, em podo o caminho com os harigos, e o que assia\n",
      "acerder que não senhorem seu corrente dos próximos do cantina, e enquinto e estejamado palavras angosa com a sobre dos cobras de modo. A encurta e a chegar a menher atravia de argum. Não sendo então estantemante, as mais distestavam de pedidass das coisas de se arrasteres.\n",
      "— Acho delho que possíamos duintar de modem a com deles. Encandou-se sera de carragado a coisa. E estava algués do que elfos. As muitas antes da conter o mai do que não podemos se astia e procurando-e paseram a cencação do passo das árvores, como acho de sua própria choia a com desejo.\n",
      "— Não passo da munta de sous, enquentro de\n",
      "seu creste distante, por escitou a mais entrada do modo, e não tinha acondecuro da melo, ma malgum do com o desejarem a mais de\n",
      "alhos estadas antes aligasse coisas, e o pispou a pistar de alguma contina deseja do selhor quando astestaram desapareciumos, \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Era\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
